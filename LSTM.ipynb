{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829d9da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "df = pd.read_csv(r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\IMDB Dataset.csv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5580a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df89861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_42\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"CEBaB/lstm.CEBaB.sa.5-class.exclusive.seed_42\")\n",
    "\n",
    "# # Prepare input text\n",
    "# sampled = df.loc[:10]\n",
    "# for row in sampled:\n",
    "#     text = row[\"review\"]\n",
    "\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "#     # Perform inference\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**inputs).logits\n",
    "\n",
    "#     # Get predicted class\n",
    "#     predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "#     print(f\"Predicted label: {predicted_class} ground-truth: {row['sentiment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030066a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# # from torchtext.models import RobertaClassificationHead, LSTMClassifier\n",
    "# # from torchtext.functional import to_tensor\n",
    "\n",
    "# # from torchtext.models import LSTMClassificationHead, LSTMEncoder\n",
    "# from torchtext.models import RNNModel\n",
    "\n",
    "# # Load pretrained RNN model\n",
    "# bundle = torchtext.models.RNNModel.from_pretrained(\"lstm-imdb\")\n",
    "# print(bundle.model)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = bundle.to(device)\n",
    "\n",
    "# # Evaluate sentiment of a sentence\n",
    "# sampled = df.loc[:10]\n",
    "# for row in sampled:\n",
    "#     text = row[\"review\"]\n",
    "#     input_batch = bundle.transform(text)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         prediction = model(input_batch)\n",
    "#         label = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "\n",
    "#     print(f\"Predicted label: {label} ground-truth: {row['sentiment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5710ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500\n",
      "Validation size: 500\n",
      "Test size: 49000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split off test set (15%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.98, random_state=42, stratify=df[\"sentiment\"])\n",
    "\n",
    "# Then split train/val (17.65% of remaining data -> 15% of total for val)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.5, random_state=42, stratify=train_val_df[\"sentiment\"])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec936ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 13014\n",
      "Index of 'movie': 18, unknown token -> 1\n",
      "Train size: 500\n",
      "Val size: 500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "# from torchtext.vocab import vocab as build_vocab\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Dataset preparation\n",
    "# ---------------------------\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = torch.tensor([self.vocab[token] for token in tokens], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return indices, label\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Count tokens\n",
    "counter = Counter()\n",
    "for text in train_df[\"review\"]:\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Define special tokens\n",
    "specials = [\"<pad>\", \"<unk>\"]\n",
    "\n",
    "# Build vocab\n",
    "vocab = Vocab(counter, specials=specials)\n",
    "\n",
    "# --- Older torchtext doesn't support .set_default_index(), so define manually ---\n",
    "UNK_IDX = vocab.stoi[\"<unk>\"]\n",
    "\n",
    "# Create a safe lookup wrapper\n",
    "def safe_lookup(token):\n",
    "    return vocab.stoi.get(token, UNK_IDX)\n",
    "\n",
    "# Example usage:\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Index of 'movie': {safe_lookup('movie')}, unknown token -> {safe_lookup('xyzqwe')}\")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = SentimentDataset(train_df[\"review\"], train_df[\"sentiment\"], vocab, tokenizer)\n",
    "val_dataset = SentimentDataset(val_df[\"review\"], val_df[\"sentiment\"], vocab, tokenizer)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a827248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60a5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.7233, Val Loss: 0.7089\n",
      "Initial val loss: 0.7089\n",
      "Epoch [2/100], Train Loss: 0.6836, Val Loss: 0.7388\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [3/100], Train Loss: 0.7015, Val Loss: 0.7048\n",
      "Validation loss improved (0.7089 -> 0.7048)\n",
      "Epoch [4/100], Train Loss: 0.6948, Val Loss: 0.7057\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [5/100], Train Loss: 0.6993, Val Loss: 0.7095\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [6/100], Train Loss: 0.7044, Val Loss: 0.6833\n",
      "Validation loss improved (0.7048 -> 0.6833)\n",
      "Epoch [7/100], Train Loss: 0.6869, Val Loss: 0.7026\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [8/100], Train Loss: 0.6964, Val Loss: 0.6827\n",
      "Validation loss improved (0.6833 -> 0.6827)\n",
      "Epoch [9/100], Train Loss: 0.7061, Val Loss: 0.7062\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [10/100], Train Loss: 0.7035, Val Loss: 0.6935\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [11/100], Train Loss: 0.7058, Val Loss: 0.6909\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch [12/100], Train Loss: 0.7076, Val Loss: 0.6857\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch [13/100], Train Loss: 0.7045, Val Loss: 0.6895\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered at epoch 13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return sequences_padded, torch.tensor(labels)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :]) # last hidden state\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"Initial val loss: {val_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss:.4f} -> {val_loss:.4f})\")\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "model = SimpleRNN(vocab_size=vocab_size, embed_dim=64, hidden_size=128, output_size=1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ace918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 45.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to eval mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "testdf = test_df.sample(200, random_state=42)\n",
    "# testdf = test_df\n",
    "\n",
    "for _, row in testdf.iterrows():\n",
    "    test_text = row[\"review\"]  # or appropriate text column name\n",
    "    true_label = row[\"sentiment\"]\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    test_tokens = tokenizer(test_text)\n",
    "    test_indices = torch.tensor([vocab[token] for token in test_tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(test_indices)\n",
    "        # Assuming output is raw logits for binary class, apply sigmoid then threshold\n",
    "        pred_prob = torch.sigmoid(output).item()  \n",
    "        pred_label = 1 if pred_prob > 0.5 else 0\n",
    "\n",
    "    total += 1\n",
    "    correct += (pred_label == true_label)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81631c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to RNN_model.pth\n"
     ]
    }
   ],
   "source": [
    "# RNN_model.save\n",
    "RNN_save_path = \"RNN_model.pth\"\n",
    "\n",
    "# Save model state_dict (recommended)\n",
    "torch.save(RNN_model.state_dict(), RNN_save_path)\n",
    "print(f\"Model saved to {RNN_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = SimpleRNN(vocab_size=len(vocab), embed_dim=64, hidden_size=128, output_size=1)\n",
    "\n",
    "RNN_model.load_state_dict(torch.load(RNN_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629c0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # last time step hidden state\n",
    "        return out\n",
    "\n",
    "# Example instantiation\n",
    "# model = LSTMModel(input_size=10, hidden_size=20, num_layers=2, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "857fab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_46936\\2815759266.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.6961 | Val Loss: 0.6935\n",
      "Epoch [2/10] - Train Loss: 0.6874 | Val Loss: 0.6946\n",
      "Epoch [3/10] - Train Loss: 0.7004 | Val Loss: 0.7019\n",
      "Epoch [4/10] - Train Loss: 0.6366 | Val Loss: 0.7331\n",
      "Epoch [5/10] - Train Loss: 0.5762 | Val Loss: 0.8025\n",
      "Epoch [6/10] - Train Loss: 0.7024 | Val Loss: 0.6931\n",
      "Epoch [7/10] - Train Loss: 0.6908 | Val Loss: 0.6971\n",
      "Epoch [8/10] - Train Loss: 0.6509 | Val Loss: 0.7606\n",
      "Epoch [9/10] - Train Loss: 0.5813 | Val Loss: 0.7867\n",
      "Epoch [10/10] - Train Loss: 0.5576 | Val Loss: 0.7850\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# ----------------------------------\n",
    "# 2. LSTM model (with embedding)\n",
    "# ----------------------------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        # self.sigmoid = nn.Sigmoid()  # optional if using BCELoss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size, device=x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # last timestep output\n",
    "        return out  # logits (use BCEWithLogitsLoss)\n",
    "\n",
    "# ----------------------------------\n",
    "# 3. Early stopping (same as yours)\n",
    "# ----------------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved: {self.best_loss:.4f} â†’ {val_loss:.4f}\")\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Setup\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "LSTM_model = LSTMModel(vocab_size=vocab_size, embed_dim=64, hidden_size=128, num_layers=2, output_size=1)\n",
    "LSTM_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # stable version\n",
    "optimizer = optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# 5. Training loop\n",
    "# ----------------------------------\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    LSTM_model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = LSTM_model(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    LSTM_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = LSTM_model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # early_stopping(avg_val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d708cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 55.20%\n"
     ]
    }
   ],
   "source": [
    "LSTM_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# for _, row in test_df.sample(400).iterrows():\n",
    "for _, row in test_df.iterrows():\n",
    "    text = row[\"review\"]\n",
    "    true_label = int(row[\"sentiment\"])  # or map if it's a string\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    indices = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = LSTM_model(indices)\n",
    "        pred_prob = torch.sigmoid(output).item()\n",
    "        pred_label = 1 if pred_prob > 0.5 else 0\n",
    "\n",
    "    correct += (pred_label == true_label)\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279dde00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to LSTM_model.pth\n"
     ]
    }
   ],
   "source": [
    "LSTM_save_path = \"LSTM_model.pth\"\n",
    "\n",
    "# Save model state_dict (recommended)\n",
    "torch.save(LSTM_model.state_dict(), LSTM_save_path)\n",
    "print(f\"Model saved to {LSTM_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = LSTMModel(vocab_size=vocab_size, embed_dim=64, hidden_size=128, num_layers=2, output_size=1)\n",
    "LSTM_model.to(device)\n",
    "\n",
    "LSTM_model.load_state_dict(torch.load(LSTM_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb5c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d205e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 500\n",
      "val size: 500\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "corpus_tokens = [tokenizer(text) for text in train_df[\"review\"]]\n",
    "\n",
    "# Train Word2Vec model (or load pretrained)\n",
    "w2v_model = Word2Vec(sentences=corpus_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "w2v_vocab = w2v_model.wv.key_to_index  \n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "orig_vocab_size = len(w2v_vocab)\n",
    "unk_index = orig_vocab_size  \n",
    "\n",
    "new_vocab = dict(w2v_vocab)\n",
    "new_vocab[UNK_TOKEN] = unk_index\n",
    "\n",
    "embedding_dim = w2v_model.vector_size\n",
    "vocab_size = orig_vocab_size + 1  # one extra for <unk>\n",
    "\n",
    "# Initialize embedding matrix with extra row\n",
    "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Fill embeddings for known words\n",
    "for word, idx in w2v_vocab.items():\n",
    "    embedding_matrix[idx] = torch.tensor(w2v_model.wv[word])\n",
    "\n",
    "# Initialize <unk> vector as zeros (or random)\n",
    "embedding_matrix[unk_index] = torch.zeros(embedding_dim)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab  # word2vec vocab: word -> idx\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                indices.append(self.vocab[token])\n",
    "            else:\n",
    "                # UNK token\n",
    "                indices.append(self.vocab[UNK_TOKEN])\n",
    "\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return indices, label\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_dataset = SentimentDataset(train_df[\"review\"], train_df[\"sentiment\"], new_vocab, tokenizer)\n",
    "print(f\"train size: {len(train_dataset)}\")\n",
    "\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_dataset = SentimentDataset(val_df[\"review\"], val_df[\"sentiment\"], new_vocab, tokenizer)\n",
    "print(f\"val size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f2172f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMWord2Vec(\n",
       "  (embedding): Embedding(13013, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 2. Define LSTM model using pretrained embeddings\n",
    "class LSTMWord2Vec(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMWord2Vec, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.size()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  # freeze=True to keep fixed\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # shape: (batch, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # last timestep\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# 3. Example instantiation\n",
    "embedding_matrix = embedding_matrix.to(device)\n",
    "LSTM_w2v_model = LSTMWord2Vec(embedding_matrix=embedding_matrix, hidden_size=128, output_size=1)\n",
    "LSTM_w2v_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2d9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 4GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)           # Should show CUDA version string, e.g., '12.1'\n",
    "print(torch.cuda.is_available())    # Should be True\n",
    "print(torch.cuda.device_count())    # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a69f29cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss: 0.6933\n",
      "Epoch [1/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 2 Batch 0 Loss: 0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 3 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 4 Batch 0 Loss: 0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 5 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 6 Batch 0 Loss: 0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 7 Batch 0 Loss: 0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 8 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 9 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 10 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 11 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 12 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 13 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 14 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 15 Batch 0 Loss: 0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 16 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 17 Batch 0 Loss: 0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 18 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 19 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 20 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 21 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 22 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 23 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 24 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 25 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 26 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 27 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 28 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 29 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 30 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50] - Train Loss: 0.6932 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 31 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 32 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 33 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 34 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 35 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 36 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 37 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 38 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 39 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 40 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 41 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 42 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 43 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 44 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 45 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 46 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 47 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 48 Batch 0 Loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 49 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n",
      "Epoch 50 Batch 0 Loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1100\\2361058783.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50] - Train Loss: 0.6931 | Val Loss: 0.6931 | Val Acc: 0.5000 | Val F1: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return torch.tensor(sequences_padded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved: {self.best_loss:.4f} â†’ {val_loss:.4f}\")\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Setup\n",
    "# ----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # stable version\n",
    "optimizer = optim.Adam(LSTM_w2v_model.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# ----------------------------------\n",
    "# 5. Training loop\n",
    "# ----------------------------------\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    LSTM_w2v_model.train()\n",
    "    total_loss = 0\n",
    "    # for inputs, labels in train_loader:\n",
    "    #     inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = LSTM_w2v_model(inputs).squeeze(1)\n",
    "    #     loss = criterion(outputs, labels)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        # total_loss += loss.item()\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = LSTM_w2v_model(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch_idx} Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    LSTM_w2v_model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = LSTM_w2v_model(inputs).squeeze(1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs)  # convert logits to probabilities\n",
    "            preds = (probs >= 0.5).long()  # threshold probabilities to get predicted labels\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "    val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # early_stopping(avg_val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 50.00%\n"
     ]
    }
   ],
   "source": [
    "LSTM_w2v_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    text = row[\"review\"]\n",
    "    true_label = int(row[\"sentiment\"])  # or map if it's a string\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    indices = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = LSTM_w2v_model(indices)\n",
    "        pred_prob = torch.sigmoid(output).item()\n",
    "        pred_label = 1 if pred_prob > 0.5 else 0\n",
    "\n",
    "    correct += (pred_label == true_label)\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa535b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
