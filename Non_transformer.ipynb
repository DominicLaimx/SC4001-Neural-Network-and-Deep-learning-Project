{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d605dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8100\\558747981.py:3: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  from pynvml import *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "Index(['Unnamed: 0', 'review_id', 'user_id', 'business_id', 'stars', 'useful',\n",
      "       'funny', 'cool', 'text', 'date'],\n",
      "      dtype='object')\n",
      "stars\n",
      "5.0    3185\n",
      "1.0    2886\n",
      "2.0    2114\n",
      "4.0    1815\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "1    5000\n",
      "0    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pynvml import *\n",
    "\n",
    "pos_df = pd.read_csv(r\"C:\\Users\\ASUS\\Downloads\\yelp_4_and_above\")\n",
    "print(len(pos_df))\n",
    "# print(pos_df.head(1))\n",
    "# print(pos_df.columns)\n",
    "\n",
    "neg_df = pd.read_csv(r\"C:\\Users\\ASUS\\Downloads\\yelp_2_and_below\")\n",
    "print(len(neg_df))\n",
    "# print(neg_df.head(1))\n",
    "print(neg_df.columns)\n",
    "\n",
    "\n",
    "\n",
    "yelp_df = pd.concat([pos_df,neg_df])\n",
    "yelp_df['sentiment'] = yelp_df['stars'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "print(yelp_df[\"stars\"].value_counts())\n",
    "print(yelp_df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65861304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8000\n",
      "Validation size: 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(yelp_df, test_size=0.2, random_state=42, stratify=yelp_df[\"sentiment\"])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65840598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e884bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterlen: 26450\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import copy\n",
    "\n",
    "train_texts = train_df[\"text\"]\n",
    "train_labels = train_df[\"sentiment\"]\n",
    "val_texts = val_df[\"text\"]\n",
    "val_labels = val_df[\"sentiment\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(tokenize(text))\n",
    "print(f\"counterlen: {len(counter)}\")\n",
    "vocab_size = 20000\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(vocab_size))}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "\n",
    "def numericalize(tokens):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [torch.tensor(numericalize(tokenize(t))) for t in texts]\n",
    "        self.labels = torch.tensor(labels.tolist())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return texts_padded, torch.tensor(labels)\n",
    "\n",
    "train_data = IMDBDataset(train_texts, train_labels)\n",
    "val_data = IMDBDataset(val_texts, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c874d2",
   "metadata": {},
   "source": [
    "# Test dataset IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2334d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "IMDB_df = pd.read_csv(r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\IMDB Dataset.csv\")\n",
    "print(len(IMDB_df))\n",
    "print(IMDB_df.head())\n",
    "IMDB_df[\"sentiment\"] = IMDB_df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "test_texts = IMDB_df[\"review\"]\n",
    "test_labels = IMDB_df[\"sentiment\"]\n",
    "IMDB_data = IMDBDataset(test_texts, test_labels)\n",
    "test_loader = DataLoader(IMDB_data, batch_size=32, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff60ab",
   "metadata": {},
   "source": [
    "# GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469fa991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\torchtext\\vocab.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name=\"6B\", dim=100)\n",
    "embedding_dim = glove.dim\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove.stoi:\n",
    "        embedding_matrix[idx] = glove[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embedding_dim) * 0.05  # random init for OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6e147",
   "metadata": {},
   "source": [
    "# carbon tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c5e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "from carbontracker import parser as ct_parser\n",
    "\n",
    "def find_ct_output_log(ct_dir, prefix:str | None = None, timeout_s: float = 3.0):\n",
    "    search_roots = [os.path.abspath(ct_dir), os.path.abspath(os.getcwd())]\n",
    "    deadline = time.time() + timeout_s\n",
    "    best = None\n",
    "    while time.time() <= deadline:\n",
    "        candidates = []\n",
    "        for root in search_roots:\n",
    "            if os.path.isdir(root):\n",
    "                candidates.extend(glob.glob(os.path.join(root, \"**\", \"*carbontracker_output.log\"), recursive=True))\n",
    "        candidates = sorted(set(candidates), key=os.path.getmtime, reverse=True)\n",
    "        if prefix:\n",
    "            pref = [p for p in candidates if prefix in os.path.basename(p)]\n",
    "            if pref:\n",
    "                return pref[0]\n",
    "        if candidates:\n",
    "            return candidates[0]\n",
    "        time.sleep(0.25)\n",
    "    return best\n",
    "\n",
    "def parse_carbontracker(ct_dir, prefix: str | None = None):\n",
    "    output_log = None\n",
    "    # Try official helper\n",
    "    try:\n",
    "        from carbontracker import parser as ct_parser\n",
    "        output_log, _ = ct_parser.get_most_recent_logs(log_dir=ct_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        output_log = find_ct_output_log(ct_dir, prefix=prefix)\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        return None, None, None, None\n",
    "\n",
    "    with open(output_log, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        out_txt = f.read()\n",
    "    try:\n",
    "        from carbontracker import parser as ct_parser\n",
    "        actual, pred = ct_parser.get_consumption(out_txt)\n",
    "        cons = actual or pred\n",
    "        if cons:\n",
    "            e = cons.get(\"energy (kWh)\")\n",
    "            c = cons.get(\"co2eq (g)\")\n",
    "            d = cons.get(\"duration (s)\")\n",
    "            return (\n",
    "                float(e) if e is not None else None,\n",
    "                float(c) if c is not None else None,\n",
    "                float(d) if d is not None else None,\n",
    "                output_log,\n",
    "            )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #regex fallback\n",
    "    mE = re.search(r\"Energy:\\s*([0-9.eE+-]+)\\s*kWh\", out_txt)\n",
    "    mC = re.search(r\"CO2eq:\\s*([0-9.eE+-]+)\\s*g\", out_txt)\n",
    "    mT = re.search(r\"Time:\\s*([0-9:]+)\", out_txt)\n",
    "\n",
    "    energy_kwh = float(mE.group(1)) if mE else None\n",
    "    co2_g      = float(mC.group(1)) if mC else None\n",
    "    duration_s = None\n",
    "    if mT:\n",
    "        hh, mm, ss = (int(x) for x in mT.group(1).split(\":\"))\n",
    "        duration_s = 3600*hh + 60*mm + ss\n",
    "\n",
    "    return energy_kwh, co2_g, duration_s, output_log\n",
    "\n",
    "\n",
    "import re, os, time\n",
    "from carbontracker import parser as ct_parser\n",
    "\n",
    "def parse_carbontracker_totals(ct_dir, prefix: str | None = None):\n",
    "    output_log = None\n",
    "    # Try official helper\n",
    "    try:\n",
    "        output_log, _ = ct_parser.get_most_recent_logs(log_dir=ct_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        # fallback to searching manually\n",
    "        output_log = find_ct_output_log(ct_dir, prefix=prefix)\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        return None, None, None, None\n",
    "\n",
    "    with open(output_log, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        out_txt = f.read()\n",
    "\n",
    "    # Try CarbonTracker parser first\n",
    "    try:\n",
    "        actual, pred = ct_parser.get_consumption(out_txt)\n",
    "        cons = actual or pred\n",
    "        if cons:\n",
    "            energy = float(cons.get(\"energy (kWh)\", 0))\n",
    "            co2 = float(cons.get(\"co2eq (g)\", 0))\n",
    "            duration = float(cons.get(\"duration (s)\", 0))\n",
    "            return energy, co2, duration, output_log\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Regex fallback for summary totals at the end of log\n",
    "    mE = re.findall(r\"Energy:\\s*([0-9.eE+-]+)\\s*kWh\", out_txt)\n",
    "    mC = re.findall(r\"CO2eq:\\s*([0-9.eE+-]+)\\s*g\", out_txt)\n",
    "    mT = re.findall(r\"Time:\\s*([0-9:]+)\", out_txt)\n",
    "\n",
    "    # take last occurrence (final totals)\n",
    "    energy = float(mE[-1]) if mE else None\n",
    "    co2 = float(mC[-1]) if mC else None\n",
    "    duration_s = None\n",
    "    if mT:\n",
    "        hh, mm, ss = (int(x) for x in mT[-1].split(\":\"))\n",
    "        duration_s = hh*3600 + mm*60 + ss\n",
    "\n",
    "    return energy, co2, duration_s, output_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94baa545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_latest_carbontracker_output(log_dir=\"carbon_logs\"):\n",
    "    log_dir = Path(log_dir)\n",
    "\n",
    "    # Match .log or any extension\n",
    "    files = list(log_dir.glob(\"*carbontracker_output*\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No files containing 'carbontracker_output' found.\")\n",
    "\n",
    "    latest_file = max(files, key=lambda f: f.stat().st_mtime)\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "def parse_carbontracker_log(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    energy_pattern = r\"Energy:\\s*([0-9.]+)\\s*kWh\"\n",
    "    co2_pattern = r\"CO2eq:\\s*([0-9.]+)\\s*g\"\n",
    "    car_pattern = r\"equivalent to:\\s*([0-9.]+)\\s*km travelled by car\"\n",
    "\n",
    "    energy = re.search(energy_pattern, text)\n",
    "    co2 = re.search(co2_pattern, text)\n",
    "    car_distance = re.search(car_pattern, text)\n",
    "\n",
    "    return {\n",
    "        \"energy_kWh\": float(energy.group(1)) if energy else None,\n",
    "        \"co2_grams\": float(co2.group(1)) if co2 else None,\n",
    "        \"car_km\": float(car_distance.group(1)) if car_distance else None\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09061948",
   "metadata": {},
   "source": [
    "# Preparing training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69f67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pynvml import *\n",
    "import time\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_power_usage_watts():\n",
    "    try:\n",
    "        \n",
    "        result = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\"]\n",
    "        )\n",
    "        return float(result.strip())\n",
    "    except Exception as e:\n",
    "        print(\"Error reading GPU power:\", e)\n",
    "        return 0.0\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_power = get_power_usage_watts()\n",
    "\n",
    "    for texts, labels in dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_power = get_power_usage_watts()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    avg_power = (start_power + end_power) / 2  # rough average\n",
    "    energy_joules = avg_power * elapsed_time\n",
    "\n",
    "    return avg_loss, accuracy, energy_joules\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            batch_size = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu()\n",
    "    all_labels = torch.cat(all_labels).cpu()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    acc = (all_preds == all_labels).float().mean().item()\n",
    "    f1 = f1_score(all_labels.numpy(), all_preds.numpy())\n",
    "\n",
    "    return avg_loss, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861d171",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07ffc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, method=\"last\", dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,1))\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.method = method  \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  \n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "\n",
    "        if self.method == \"last\":\n",
    "            hidden_fw = hidden[-2, :, :]  \n",
    "            hidden_bw = hidden[-1, :, :]  \n",
    "            hidden_cat = torch.cat((hidden_fw, hidden_bw), dim=1)  \n",
    "            \n",
    "        elif self.method == \"avg\":\n",
    "            hidden_cat = torch.mean(output, dim=1)  \n",
    "        \n",
    "        elif self.method == \"max\":\n",
    "            hidden_cat, _ = torch.max(output, dim=1)\n",
    "\n",
    "        elif self.method == \"2dmaxpool\":\n",
    "            output_4d = output.unsqueeze(1)\n",
    "            pooled = self.maxpool(output_4d) \n",
    "            hidden_cat = pooled.squeeze(1).max(dim=1)[0]\n",
    "\n",
    "        hidden_cat = self.dropout(hidden_cat)\n",
    "        return self.fc(hidden_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c4474",
   "metadata": {},
   "source": [
    "# Test code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def test_worst(model, dataloader, criterion, return_preds=False, text_dataset=None, label_names= [\"Negative\", \"Positive\"]):\n",
    "    model.eval()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "    all_preds, all_labels, all_probs, all_texts = [], [], [], []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (texts, labels) in enumerate(dataloader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            \n",
    "            if text_dataset is not None:\n",
    "                all_texts.extend(text_dataset[i * dataloader.batch_size : i * dataloader.batch_size + len(labels)])\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Processed {total_samples:,} samples...\")\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    acc = (all_preds == all_labels).float().mean().item()\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds) if return_preds else None\n",
    "\n",
    "    wrong_mask = all_preds != all_labels\n",
    "    wrong_indices = torch.where(wrong_mask)[0]\n",
    "\n",
    "    rows = []\n",
    "    for idx in wrong_indices:\n",
    "        true_label = int(all_labels[idx])\n",
    "        pred_label = int(all_preds[idx])\n",
    "        pred_conf = float(all_probs[idx, pred_label])\n",
    "        true_conf = float(all_probs[idx, true_label])\n",
    "        text = all_texts[idx] if text_dataset is not None else None\n",
    "\n",
    "        rows.append({\n",
    "            \"index\": int(idx),\n",
    "            \"true_label\": label_names[true_label] if label_names else true_label,\n",
    "            \"pred_label\": label_names[pred_label] if label_names else pred_label,\n",
    "            \"true_conf\": true_conf,\n",
    "            \"pred_conf\": pred_conf,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    df_wrong = pd.DataFrame(rows)\n",
    "\n",
    "    df_sorted = df_wrong.sort_values([\"true_label\", \"pred_conf\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"labels\": all_labels.numpy(),\n",
    "        \"preds\": all_preds.numpy(),\n",
    "        \"misclassified\": df_sorted\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e76f7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tuning(lr, optimiser, weight_decay, method):\n",
    "    NUM_RUNS = 3\n",
    "    all_results = []\n",
    "\n",
    "    for run in range(1, NUM_RUNS + 1):\n",
    "        print(f\"\\n{'='*25}\\nStarting training run {run}\\n{'='*25}\")\n",
    "\n",
    "        model = biLSTM(\n",
    "            vocab_size=embedding_matrix.shape[0],\n",
    "            embed_dim=embedding_matrix.shape[1],\n",
    "            hidden_dim=128,\n",
    "            output_dim=2,\n",
    "            method=method\n",
    "        ).to(device)\n",
    "\n",
    "        # Load GloVe weights (frozen)\n",
    "        model.embedding.weight.data.copy_(embedding_matrix)\n",
    "        model.embedding.weight.requires_grad = False\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        if optimiser.lower() == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimiser.lower() == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimiser.lower() == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "        \n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_model_state = None\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies, val_f1s = [], [], []\n",
    "\n",
    "        patience = 5\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        total_energy_usage = 0.0\n",
    "\n",
    "        for epoch in range(50):\n",
    "            train_loss, train_acc, energy_used = train_epoch(model, train_loader, optimizer, criterion)\n",
    "            val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            total_energy_usage += energy_used\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d}: \"\n",
    "                f\"train loss={train_loss:.4f}, val loss={val_loss:.4f}, \"\n",
    "                f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"No improvement in val loss for {epochs_no_improve} epochs.\")\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval() \n",
    "\n",
    "        test_results = test_worst(\n",
    "            model,\n",
    "            test_loader,\n",
    "            criterion,\n",
    "            return_preds=True,\n",
    "            text_dataset=test_texts,\n",
    "        )\n",
    "\n",
    "        all_results.append({\n",
    "            \"run\": run,\n",
    "            \"num_epochs\": epoch+1,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"total_energy\": total_energy_usage,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\":train_accuracies,\n",
    "            \"val_accuracies\":val_accuracies,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_f1s\": val_f1s,\n",
    "            \"test_acc\": test_results[\"acc\"],\n",
    "            \"test_f1\": test_results[\"f1\"]\n",
    "        })\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c158ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31f0e7",
   "metadata": {},
   "source": [
    "# Tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing learning rate: 0.0001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6625, val loss=0.5303, train_acc=0.6114, val_acc=0.7585, val_f1=0.7512\n",
      "New best accuracy: 0.7585 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5405, val loss=0.5121, train_acc=0.7495, val_acc=0.7780, val_f1=0.7494\n",
      "New best accuracy: 0.7780 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4860, val loss=0.4518, train_acc=0.7869, val_acc=0.8035, val_f1=0.7952\n",
      "New best accuracy: 0.8035 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4410, val loss=0.4162, train_acc=0.8105, val_acc=0.8220, val_f1=0.8207\n",
      "New best accuracy: 0.8220 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4266, val loss=0.4002, train_acc=0.8206, val_acc=0.8340, val_f1=0.8283\n",
      "New best accuracy: 0.8340 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3937, val loss=0.3889, train_acc=0.8425, val_acc=0.8470, val_f1=0.8439\n",
      "New best accuracy: 0.8470 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4849, val loss=0.3943, train_acc=0.7746, val_acc=0.8375, val_f1=0.8466\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.3748, val loss=0.3555, train_acc=0.8480, val_acc=0.8540, val_f1=0.8510\n",
      "New best accuracy: 0.8540 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3635, val loss=0.3577, train_acc=0.8542, val_acc=0.8600, val_f1=0.8631\n",
      "New best accuracy: 0.8600 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.4129, val loss=0.3630, train_acc=0.8416, val_acc=0.8570, val_f1=0.8576\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.3468, val loss=0.3571, train_acc=0.8598, val_acc=0.8465, val_f1=0.8340\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.3472, val loss=0.3491, train_acc=0.8602, val_acc=0.8575, val_f1=0.8634\n",
      "Epoch 13: train loss=0.3378, val loss=0.3519, train_acc=0.8645, val_acc=0.8590, val_f1=0.8516\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.3401, val loss=0.3265, train_acc=0.8655, val_acc=0.8685, val_f1=0.8668\n",
      "New best accuracy: 0.8685 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.3296, val loss=0.3512, train_acc=0.8680, val_acc=0.8560, val_f1=0.8639\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.3285, val loss=0.4288, train_acc=0.8661, val_acc=0.8290, val_f1=0.8457\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.3357, val loss=0.3385, train_acc=0.8710, val_acc=0.8700, val_f1=0.8688\n",
      "New best accuracy: 0.8700 at epoch 17, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.3928, val loss=0.5545, train_acc=0.8536, val_acc=0.7590, val_f1=0.7985\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.3749, val loss=0.3297, train_acc=0.8425, val_acc=0.8755, val_f1=0.8726\n",
      "New best accuracy: 0.8755 at epoch 19, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6714, val loss=0.7031, train_acc=0.6029, val_acc=0.5860, val_f1=0.3191\n",
      "New best accuracy: 0.5860 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5326, val loss=0.4780, train_acc=0.7522, val_acc=0.7865, val_f1=0.7619\n",
      "New best accuracy: 0.7865 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4586, val loss=0.4679, train_acc=0.8037, val_acc=0.7980, val_f1=0.7715\n",
      "New best accuracy: 0.7980 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4250, val loss=0.4096, train_acc=0.8245, val_acc=0.8315, val_f1=0.8390\n",
      "New best accuracy: 0.8315 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3958, val loss=0.3712, train_acc=0.8370, val_acc=0.8565, val_f1=0.8621\n",
      "New best accuracy: 0.8565 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3723, val loss=0.3744, train_acc=0.8510, val_acc=0.8435, val_f1=0.8304\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3575, val loss=0.4500, train_acc=0.8549, val_acc=0.8115, val_f1=0.7781\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.3527, val loss=0.3575, train_acc=0.8572, val_acc=0.8615, val_f1=0.8653\n",
      "New best accuracy: 0.8615 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3575, val loss=0.3653, train_acc=0.8548, val_acc=0.8620, val_f1=0.8579\n",
      "New best accuracy: 0.8620 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.3512, val loss=0.3290, train_acc=0.8579, val_acc=0.8690, val_f1=0.8675\n",
      "New best accuracy: 0.8690 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.3364, val loss=0.3636, train_acc=0.8655, val_acc=0.8555, val_f1=0.8437\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.3381, val loss=0.3445, train_acc=0.8661, val_acc=0.8630, val_f1=0.8678\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.3345, val loss=0.3347, train_acc=0.8632, val_acc=0.8695, val_f1=0.8693\n",
      "New best accuracy: 0.8695 at epoch 13, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.3226, val loss=0.3955, train_acc=0.8744, val_acc=0.8340, val_f1=0.8103\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.3143, val loss=0.3113, train_acc=0.8742, val_acc=0.8755, val_f1=0.8739\n",
      "New best accuracy: 0.8755 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.3200, val loss=0.3207, train_acc=0.8712, val_acc=0.8725, val_f1=0.8667\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.3164, val loss=0.3245, train_acc=0.8726, val_acc=0.8680, val_f1=0.8597\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.3027, val loss=0.3111, train_acc=0.8784, val_acc=0.8770, val_f1=0.8733\n",
      "New best accuracy: 0.8770 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.3091, val loss=0.3059, train_acc=0.8788, val_acc=0.8750, val_f1=0.8734\n",
      "Epoch 20: train loss=0.2975, val loss=0.3003, train_acc=0.8835, val_acc=0.8785, val_f1=0.8777\n",
      "New best accuracy: 0.8785 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.2958, val loss=0.3153, train_acc=0.8852, val_acc=0.8725, val_f1=0.8729\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.2982, val loss=0.3589, train_acc=0.8844, val_acc=0.8660, val_f1=0.8739\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 23: train loss=0.2980, val loss=0.3112, train_acc=0.8852, val_acc=0.8745, val_f1=0.8702\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 24: train loss=0.2813, val loss=0.2945, train_acc=0.8888, val_acc=0.8800, val_f1=0.8777\n",
      "New best accuracy: 0.8800 at epoch 24, saving model.\n",
      "Epoch 25: train loss=0.2768, val loss=0.3517, train_acc=0.8912, val_acc=0.8685, val_f1=0.8578\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 26: train loss=0.2768, val loss=0.2957, train_acc=0.8931, val_acc=0.8835, val_f1=0.8823\n",
      "New best accuracy: 0.8835 at epoch 26, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 27: train loss=0.2916, val loss=0.3401, train_acc=0.8838, val_acc=0.8655, val_f1=0.8740\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 28: train loss=0.2738, val loss=0.2917, train_acc=0.8926, val_acc=0.8835, val_f1=0.8837\n",
      "Epoch 29: train loss=0.2878, val loss=0.3250, train_acc=0.8889, val_acc=0.8635, val_f1=0.8719\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 30: train loss=0.2719, val loss=0.2882, train_acc=0.8935, val_acc=0.8880, val_f1=0.8894\n",
      "New best accuracy: 0.8880 at epoch 30, saving model.\n",
      "Epoch 31: train loss=0.2623, val loss=0.2958, train_acc=0.8991, val_acc=0.8830, val_f1=0.8829\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 32: train loss=0.2645, val loss=0.2942, train_acc=0.8931, val_acc=0.8845, val_f1=0.8882\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 33: train loss=0.2530, val loss=0.2885, train_acc=0.9034, val_acc=0.8855, val_f1=0.8888\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 34: train loss=0.2538, val loss=0.3070, train_acc=0.9050, val_acc=0.8815, val_f1=0.8853\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 35: train loss=0.2463, val loss=0.3273, train_acc=0.9077, val_acc=0.8740, val_f1=0.8809\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 35.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6811, val loss=0.6615, train_acc=0.5714, val_acc=0.5940, val_f1=0.3666\n",
      "New best accuracy: 0.5940 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5757, val loss=0.5107, train_acc=0.7155, val_acc=0.7690, val_f1=0.7529\n",
      "New best accuracy: 0.7690 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5190, val loss=0.4716, train_acc=0.7568, val_acc=0.7970, val_f1=0.7998\n",
      "New best accuracy: 0.7970 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.5084, val loss=0.4493, train_acc=0.7658, val_acc=0.7960, val_f1=0.7873\n",
      "Epoch 05: train loss=0.4385, val loss=0.4329, train_acc=0.8107, val_acc=0.8125, val_f1=0.8172\n",
      "New best accuracy: 0.8125 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.4071, val loss=0.3960, train_acc=0.8301, val_acc=0.8345, val_f1=0.8400\n",
      "New best accuracy: 0.8345 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4009, val loss=0.3823, train_acc=0.8360, val_acc=0.8385, val_f1=0.8410\n",
      "New best accuracy: 0.8385 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3789, val loss=0.3659, train_acc=0.8465, val_acc=0.8500, val_f1=0.8525\n",
      "New best accuracy: 0.8500 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3890, val loss=0.3824, train_acc=0.8459, val_acc=0.8460, val_f1=0.8553\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.3577, val loss=0.3812, train_acc=0.8576, val_acc=0.8480, val_f1=0.8569\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.3551, val loss=0.3821, train_acc=0.8575, val_acc=0.8560, val_f1=0.8585\n",
      "New best accuracy: 0.8560 at epoch 11, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.3462, val loss=0.3697, train_acc=0.8622, val_acc=0.8430, val_f1=0.8254\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.3322, val loss=0.3502, train_acc=0.8680, val_acc=0.8570, val_f1=0.8464\n",
      "New best accuracy: 0.8570 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.3291, val loss=0.3409, train_acc=0.8695, val_acc=0.8635, val_f1=0.8652\n",
      "New best accuracy: 0.8635 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.3350, val loss=0.3409, train_acc=0.8680, val_acc=0.8655, val_f1=0.8695\n",
      "New best accuracy: 0.8655 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.3220, val loss=0.3253, train_acc=0.8726, val_acc=0.8720, val_f1=0.8723\n",
      "New best accuracy: 0.8720 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.3320, val loss=0.3508, train_acc=0.8640, val_acc=0.8675, val_f1=0.8724\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.3231, val loss=0.3269, train_acc=0.8708, val_acc=0.8650, val_f1=0.8588\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.3071, val loss=0.3279, train_acc=0.8779, val_acc=0.8755, val_f1=0.8758\n",
      "New best accuracy: 0.8755 at epoch 19, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 20: train loss=0.3510, val loss=0.3235, train_acc=0.8586, val_acc=0.8740, val_f1=0.8734\n",
      "Epoch 21: train loss=0.3179, val loss=0.3323, train_acc=0.8788, val_acc=0.8680, val_f1=0.8725\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.3040, val loss=0.3177, train_acc=0.8792, val_acc=0.8780, val_f1=0.8791\n",
      "New best accuracy: 0.8780 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.3591, val loss=0.3403, train_acc=0.8465, val_acc=0.8665, val_f1=0.8693\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.2956, val loss=0.3188, train_acc=0.8814, val_acc=0.8690, val_f1=0.8681\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.2915, val loss=0.3183, train_acc=0.8850, val_acc=0.8750, val_f1=0.8777\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.2880, val loss=0.3395, train_acc=0.8868, val_acc=0.8715, val_f1=0.8737\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.2875, val loss=0.3160, train_acc=0.8845, val_acc=0.8775, val_f1=0.8764\n",
      "Epoch 28: train loss=0.2884, val loss=0.3122, train_acc=0.8870, val_acc=0.8770, val_f1=0.8756\n",
      "Epoch 29: train loss=0.2992, val loss=0.3089, train_acc=0.8819, val_acc=0.8755, val_f1=0.8715\n",
      "Epoch 30: train loss=0.2800, val loss=0.3156, train_acc=0.8889, val_acc=0.8760, val_f1=0.8794\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 31: train loss=0.3336, val loss=0.3152, train_acc=0.8724, val_acc=0.8705, val_f1=0.8678\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 32: train loss=0.2836, val loss=0.3001, train_acc=0.8872, val_acc=0.8820, val_f1=0.8797\n",
      "New best accuracy: 0.8820 at epoch 32, saving model.\n",
      "Epoch 33: train loss=0.2682, val loss=0.3026, train_acc=0.8942, val_acc=0.8830, val_f1=0.8813\n",
      "New best accuracy: 0.8830 at epoch 33, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 34: train loss=0.2719, val loss=0.3075, train_acc=0.8930, val_acc=0.8745, val_f1=0.8721\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 35: train loss=0.2616, val loss=0.3014, train_acc=0.8976, val_acc=0.8805, val_f1=0.8752\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 36: train loss=0.2638, val loss=0.3040, train_acc=0.8974, val_acc=0.8810, val_f1=0.8809\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 37: train loss=0.2565, val loss=0.2959, train_acc=0.8996, val_acc=0.8795, val_f1=0.8782\n",
      "Epoch 38: train loss=0.2593, val loss=0.2963, train_acc=0.8995, val_acc=0.8875, val_f1=0.8873\n",
      "New best accuracy: 0.8875 at epoch 38, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 39: train loss=0.2606, val loss=0.3173, train_acc=0.8982, val_acc=0.8775, val_f1=0.8676\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 40: train loss=0.2566, val loss=0.3248, train_acc=0.9038, val_acc=0.8655, val_f1=0.8531\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 41: train loss=0.2569, val loss=0.3274, train_acc=0.9021, val_acc=0.8670, val_f1=0.8540\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 42: train loss=0.2394, val loss=0.2859, train_acc=0.9110, val_acc=0.8905, val_f1=0.8882\n",
      "New best accuracy: 0.8905 at epoch 42, saving model.\n",
      "Epoch 43: train loss=0.2400, val loss=0.2893, train_acc=0.9097, val_acc=0.8900, val_f1=0.8864\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 44: train loss=0.2490, val loss=0.3274, train_acc=0.9042, val_acc=0.8725, val_f1=0.8805\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 45: train loss=0.2401, val loss=0.3004, train_acc=0.9099, val_acc=0.8855, val_f1=0.8797\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 46: train loss=0.2414, val loss=0.2991, train_acc=0.9036, val_acc=0.8925, val_f1=0.8969\n",
      "New best accuracy: 0.8925 at epoch 46, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 47: train loss=0.2328, val loss=0.2825, train_acc=0.9107, val_acc=0.8895, val_f1=0.8889\n",
      "Epoch 48: train loss=0.2666, val loss=0.2826, train_acc=0.8928, val_acc=0.8900, val_f1=0.8866\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 49: train loss=0.2268, val loss=0.3114, train_acc=0.9153, val_acc=0.8885, val_f1=0.8833\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 50: train loss=0.2241, val loss=0.2855, train_acc=0.9134, val_acc=0.8925, val_f1=0.8946\n",
      "No improvement in val loss for 3 epochs.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.0001: [0.7300800085067749, 0.7541400194168091, 0.7644799947738647]\n",
      "Test F1 Scores for lr=0.0001: [0.7288716085227331, 0.7541373225554426, 0.7633949295253714]\n",
      "Average Test Accuracy: 0.7496, Average Test F1: 0.7488\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing learning rate: 0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5869, val loss=0.4413, train_acc=0.6899, val_acc=0.7975, val_f1=0.8089\n",
      "New best accuracy: 0.7975 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5485, val loss=0.6111, train_acc=0.7238, val_acc=0.6870, val_f1=0.6681\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4695, val loss=0.4195, train_acc=0.7840, val_acc=0.8155, val_f1=0.7937\n",
      "New best accuracy: 0.8155 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.5739, val loss=0.6361, train_acc=0.7056, val_acc=0.6665, val_f1=0.6557\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.6071, val loss=0.6207, train_acc=0.6593, val_acc=0.6570, val_f1=0.5755\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.5695, val loss=0.4660, train_acc=0.6957, val_acc=0.7975, val_f1=0.8008\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 07: train loss=0.5521, val loss=0.4985, train_acc=0.7164, val_acc=0.7610, val_f1=0.7178\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 08: train loss=0.3884, val loss=0.3599, train_acc=0.8384, val_acc=0.8540, val_f1=0.8447\n",
      "New best accuracy: 0.8540 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3218, val loss=0.3253, train_acc=0.8686, val_acc=0.8775, val_f1=0.8805\n",
      "New best accuracy: 0.8775 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.3145, val loss=0.3100, train_acc=0.8725, val_acc=0.8815, val_f1=0.8831\n",
      "New best accuracy: 0.8815 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2880, val loss=0.2867, train_acc=0.8842, val_acc=0.8860, val_f1=0.8850\n",
      "New best accuracy: 0.8860 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2652, val loss=0.2755, train_acc=0.8924, val_acc=0.8940, val_f1=0.8916\n",
      "New best accuracy: 0.8940 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2637, val loss=0.2676, train_acc=0.8929, val_acc=0.8965, val_f1=0.8939\n",
      "New best accuracy: 0.8965 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2414, val loss=0.2622, train_acc=0.9059, val_acc=0.9035, val_f1=0.9044\n",
      "New best accuracy: 0.9035 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2304, val loss=0.2997, train_acc=0.9065, val_acc=0.8990, val_f1=0.8953\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.2298, val loss=0.2968, train_acc=0.9084, val_acc=0.8995, val_f1=0.8943\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.2146, val loss=0.2751, train_acc=0.9156, val_acc=0.8895, val_f1=0.8953\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.2071, val loss=0.2557, train_acc=0.9204, val_acc=0.9030, val_f1=0.9038\n",
      "Epoch 19: train loss=0.1981, val loss=0.2731, train_acc=0.9235, val_acc=0.9010, val_f1=0.9049\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1962, val loss=0.2398, train_acc=0.9263, val_acc=0.9085, val_f1=0.9097\n",
      "New best accuracy: 0.9085 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.1832, val loss=0.2316, train_acc=0.9309, val_acc=0.9215, val_f1=0.9207\n",
      "New best accuracy: 0.9215 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.1669, val loss=0.2610, train_acc=0.9371, val_acc=0.9030, val_f1=0.9066\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.1642, val loss=0.2359, train_acc=0.9393, val_acc=0.9190, val_f1=0.9179\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.1499, val loss=0.2440, train_acc=0.9449, val_acc=0.9130, val_f1=0.9137\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 25: train loss=0.1452, val loss=0.2403, train_acc=0.9451, val_acc=0.9180, val_f1=0.9188\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 26: train loss=0.1445, val loss=0.2352, train_acc=0.9465, val_acc=0.9155, val_f1=0.9128\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 26.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6113, val loss=0.6222, train_acc=0.6717, val_acc=0.6975, val_f1=0.6663\n",
      "New best accuracy: 0.6975 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5131, val loss=0.4425, train_acc=0.7585, val_acc=0.8140, val_f1=0.8180\n",
      "New best accuracy: 0.8140 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4355, val loss=0.3716, train_acc=0.8140, val_acc=0.8510, val_f1=0.8456\n",
      "New best accuracy: 0.8510 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4091, val loss=0.4973, train_acc=0.8280, val_acc=0.7595, val_f1=0.7302\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3864, val loss=0.3838, train_acc=0.8374, val_acc=0.8360, val_f1=0.8204\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.4305, val loss=0.3544, train_acc=0.8136, val_acc=0.8545, val_f1=0.8522\n",
      "New best accuracy: 0.8545 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3697, val loss=0.3432, train_acc=0.8451, val_acc=0.8645, val_f1=0.8680\n",
      "New best accuracy: 0.8645 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.5188, val loss=0.5484, train_acc=0.7394, val_acc=0.7285, val_f1=0.6958\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.5264, val loss=0.3712, train_acc=0.7382, val_acc=0.8500, val_f1=0.8384\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.3414, val loss=0.2942, train_acc=0.8638, val_acc=0.8840, val_f1=0.8805\n",
      "New best accuracy: 0.8840 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2905, val loss=0.3051, train_acc=0.8865, val_acc=0.8820, val_f1=0.8841\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2922, val loss=0.2937, train_acc=0.8858, val_acc=0.8900, val_f1=0.8917\n",
      "New best accuracy: 0.8900 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2748, val loss=0.2713, train_acc=0.8946, val_acc=0.8980, val_f1=0.8970\n",
      "New best accuracy: 0.8980 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2558, val loss=0.2752, train_acc=0.9035, val_acc=0.8980, val_f1=0.8993\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2473, val loss=0.2665, train_acc=0.9046, val_acc=0.9005, val_f1=0.8977\n",
      "New best accuracy: 0.9005 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2413, val loss=0.2580, train_acc=0.9044, val_acc=0.9065, val_f1=0.9055\n",
      "New best accuracy: 0.9065 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2225, val loss=0.2787, train_acc=0.9151, val_acc=0.8940, val_f1=0.8967\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.2131, val loss=0.2614, train_acc=0.9211, val_acc=0.9050, val_f1=0.9011\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.2061, val loss=0.2497, train_acc=0.9200, val_acc=0.9160, val_f1=0.9145\n",
      "New best accuracy: 0.9160 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.1938, val loss=0.2441, train_acc=0.9285, val_acc=0.9120, val_f1=0.9108\n",
      "Epoch 21: train loss=0.1885, val loss=0.2424, train_acc=0.9270, val_acc=0.9175, val_f1=0.9170\n",
      "New best accuracy: 0.9175 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.1792, val loss=0.2464, train_acc=0.9339, val_acc=0.9155, val_f1=0.9147\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.1709, val loss=0.2440, train_acc=0.9360, val_acc=0.9200, val_f1=0.9183\n",
      "New best accuracy: 0.9200 at epoch 23, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.1569, val loss=0.2474, train_acc=0.9409, val_acc=0.9120, val_f1=0.9132\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 25: train loss=0.1574, val loss=0.2510, train_acc=0.9411, val_acc=0.9170, val_f1=0.9150\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 26: train loss=0.1434, val loss=0.2421, train_acc=0.9461, val_acc=0.9165, val_f1=0.9170\n",
      "Epoch 27: train loss=0.1394, val loss=0.2560, train_acc=0.9489, val_acc=0.9170, val_f1=0.9171\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 28: train loss=0.1283, val loss=0.2867, train_acc=0.9559, val_acc=0.9025, val_f1=0.8967\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 29: train loss=0.1200, val loss=0.2914, train_acc=0.9583, val_acc=0.9025, val_f1=0.9056\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 30: train loss=0.1117, val loss=0.2867, train_acc=0.9601, val_acc=0.9175, val_f1=0.9168\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 31: train loss=0.1046, val loss=0.2706, train_acc=0.9636, val_acc=0.9195, val_f1=0.9186\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 31.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5982, val loss=0.4951, train_acc=0.6789, val_acc=0.7790, val_f1=0.7566\n",
      "New best accuracy: 0.7790 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4632, val loss=0.3728, train_acc=0.7900, val_acc=0.8485, val_f1=0.8474\n",
      "New best accuracy: 0.8485 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4056, val loss=0.4793, train_acc=0.8326, val_acc=0.7840, val_f1=0.8117\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3809, val loss=0.3877, train_acc=0.8433, val_acc=0.8320, val_f1=0.8268\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.3542, val loss=0.3179, train_acc=0.8619, val_acc=0.8775, val_f1=0.8815\n",
      "New best accuracy: 0.8775 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3970, val loss=0.4338, train_acc=0.8170, val_acc=0.8125, val_f1=0.8242\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3387, val loss=0.3193, train_acc=0.8656, val_acc=0.8740, val_f1=0.8709\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.2808, val loss=0.3317, train_acc=0.8924, val_acc=0.8710, val_f1=0.8601\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 09: train loss=0.2654, val loss=0.2949, train_acc=0.9005, val_acc=0.8960, val_f1=0.8946\n",
      "New best accuracy: 0.8960 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2532, val loss=0.2800, train_acc=0.9024, val_acc=0.8990, val_f1=0.8961\n",
      "New best accuracy: 0.8990 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2488, val loss=0.2589, train_acc=0.9062, val_acc=0.9085, val_f1=0.9085\n",
      "New best accuracy: 0.9085 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2376, val loss=0.2636, train_acc=0.9086, val_acc=0.9055, val_f1=0.9074\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2235, val loss=0.2620, train_acc=0.9154, val_acc=0.9070, val_f1=0.9070\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.2293, val loss=0.2687, train_acc=0.9086, val_acc=0.9090, val_f1=0.9059\n",
      "New best accuracy: 0.9090 at epoch 14, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.2069, val loss=0.2451, train_acc=0.9221, val_acc=0.9130, val_f1=0.9127\n",
      "New best accuracy: 0.9130 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.1947, val loss=0.2510, train_acc=0.9275, val_acc=0.9095, val_f1=0.9104\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1905, val loss=0.2434, train_acc=0.9296, val_acc=0.9110, val_f1=0.9118\n",
      "Epoch 18: train loss=0.1840, val loss=0.2489, train_acc=0.9310, val_acc=0.9110, val_f1=0.9115\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.1686, val loss=0.2588, train_acc=0.9401, val_acc=0.9100, val_f1=0.9105\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 20: train loss=0.1695, val loss=0.2418, train_acc=0.9384, val_acc=0.9125, val_f1=0.9109\n",
      "Epoch 21: train loss=0.1630, val loss=0.2521, train_acc=0.9414, val_acc=0.9130, val_f1=0.9108\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.1482, val loss=0.2603, train_acc=0.9473, val_acc=0.9100, val_f1=0.9071\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 23: train loss=0.1363, val loss=0.2519, train_acc=0.9514, val_acc=0.9155, val_f1=0.9161\n",
      "New best accuracy: 0.9155 at epoch 23, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 24: train loss=0.1322, val loss=0.2689, train_acc=0.9519, val_acc=0.9085, val_f1=0.9044\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 25: train loss=0.1201, val loss=0.2682, train_acc=0.9595, val_acc=0.9170, val_f1=0.9157\n",
      "New best accuracy: 0.9170 at epoch 25, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 25.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.0005: [0.7929400205612183, 0.7879800200462341, 0.7863199710845947]\n",
      "Test F1 Scores for lr=0.0005: [0.7927909519623543, 0.7878745121161302, 0.7862965288622836]\n",
      "Average Test Accuracy: 0.7891, Average Test F1: 0.7890\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing learning rate: 0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5864, val loss=0.6307, train_acc=0.6971, val_acc=0.6415, val_f1=0.4653\n",
      "New best accuracy: 0.6415 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4595, val loss=0.4105, train_acc=0.7975, val_acc=0.8290, val_f1=0.8151\n",
      "New best accuracy: 0.8290 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3932, val loss=0.3465, train_acc=0.8390, val_acc=0.8640, val_f1=0.8691\n",
      "New best accuracy: 0.8640 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3441, val loss=0.3468, train_acc=0.8619, val_acc=0.8625, val_f1=0.8491\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2975, val loss=0.2935, train_acc=0.8805, val_acc=0.8830, val_f1=0.8888\n",
      "New best accuracy: 0.8830 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2514, val loss=0.2589, train_acc=0.9031, val_acc=0.9030, val_f1=0.9034\n",
      "New best accuracy: 0.9030 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3367, val loss=0.3695, train_acc=0.8590, val_acc=0.8605, val_f1=0.8730\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2332, val loss=0.2562, train_acc=0.9090, val_acc=0.9115, val_f1=0.9122\n",
      "New best accuracy: 0.9115 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2093, val loss=0.2485, train_acc=0.9190, val_acc=0.9140, val_f1=0.9122\n",
      "New best accuracy: 0.9140 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2024, val loss=0.2305, train_acc=0.9224, val_acc=0.9180, val_f1=0.9175\n",
      "New best accuracy: 0.9180 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1851, val loss=0.2328, train_acc=0.9317, val_acc=0.9175, val_f1=0.9166\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2060, val loss=0.2776, train_acc=0.9207, val_acc=0.8935, val_f1=0.8876\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1886, val loss=0.2501, train_acc=0.9255, val_acc=0.9150, val_f1=0.9158\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1566, val loss=0.2424, train_acc=0.9417, val_acc=0.9145, val_f1=0.9163\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1322, val loss=0.2471, train_acc=0.9511, val_acc=0.9150, val_f1=0.9169\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5805, val loss=0.4938, train_acc=0.6999, val_acc=0.7665, val_f1=0.7890\n",
      "New best accuracy: 0.7665 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5371, val loss=0.4752, train_acc=0.7471, val_acc=0.7785, val_f1=0.8044\n",
      "New best accuracy: 0.7785 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4959, val loss=1.0139, train_acc=0.7626, val_acc=0.5475, val_f1=0.1765\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.5357, val loss=0.6201, train_acc=0.7264, val_acc=0.7295, val_f1=0.6352\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.3678, val loss=0.2966, train_acc=0.8500, val_acc=0.8905, val_f1=0.8902\n",
      "New best accuracy: 0.8905 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3667, val loss=0.2933, train_acc=0.8559, val_acc=0.8950, val_f1=0.8957\n",
      "New best accuracy: 0.8950 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2905, val loss=0.2846, train_acc=0.8841, val_acc=0.8905, val_f1=0.8846\n",
      "Epoch 08: train loss=0.2592, val loss=0.2580, train_acc=0.8971, val_acc=0.9030, val_f1=0.9038\n",
      "New best accuracy: 0.9030 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2443, val loss=0.2545, train_acc=0.9062, val_acc=0.9065, val_f1=0.9078\n",
      "New best accuracy: 0.9065 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2405, val loss=0.2531, train_acc=0.9044, val_acc=0.9025, val_f1=0.8994\n",
      "Epoch 11: train loss=0.2093, val loss=0.2425, train_acc=0.9180, val_acc=0.9090, val_f1=0.9102\n",
      "New best accuracy: 0.9090 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1976, val loss=0.2303, train_acc=0.9236, val_acc=0.9170, val_f1=0.9162\n",
      "New best accuracy: 0.9170 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1799, val loss=0.2321, train_acc=0.9299, val_acc=0.9180, val_f1=0.9194\n",
      "New best accuracy: 0.9180 at epoch 13, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1628, val loss=0.2407, train_acc=0.9383, val_acc=0.9100, val_f1=0.9097\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1500, val loss=0.2537, train_acc=0.9433, val_acc=0.9140, val_f1=0.9116\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.1378, val loss=0.2387, train_acc=0.9467, val_acc=0.9195, val_f1=0.9202\n",
      "New best accuracy: 0.9195 at epoch 16, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.1448, val loss=0.2959, train_acc=0.9470, val_acc=0.9075, val_f1=0.9034\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5976, val loss=0.6308, train_acc=0.6899, val_acc=0.6875, val_f1=0.5944\n",
      "New best accuracy: 0.6875 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5426, val loss=0.5953, train_acc=0.7270, val_acc=0.6940, val_f1=0.7041\n",
      "New best accuracy: 0.6940 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4703, val loss=0.4894, train_acc=0.7881, val_acc=0.7655, val_f1=0.7635\n",
      "New best accuracy: 0.7655 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3567, val loss=0.3227, train_acc=0.8541, val_acc=0.8855, val_f1=0.8895\n",
      "New best accuracy: 0.8855 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2865, val loss=0.3221, train_acc=0.8898, val_acc=0.8765, val_f1=0.8841\n",
      "Epoch 06: train loss=0.2689, val loss=0.3357, train_acc=0.8935, val_acc=0.8585, val_f1=0.8442\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2496, val loss=0.2595, train_acc=0.9030, val_acc=0.9035, val_f1=0.9001\n",
      "New best accuracy: 0.9035 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2306, val loss=0.3544, train_acc=0.9100, val_acc=0.8960, val_f1=0.9011\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2064, val loss=0.2315, train_acc=0.9219, val_acc=0.9220, val_f1=0.9228\n",
      "New best accuracy: 0.9220 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1862, val loss=0.2521, train_acc=0.9283, val_acc=0.9080, val_f1=0.9046\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1794, val loss=0.2350, train_acc=0.9346, val_acc=0.9170, val_f1=0.9157\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1683, val loss=0.2273, train_acc=0.9369, val_acc=0.9235, val_f1=0.9232\n",
      "New best accuracy: 0.9235 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1536, val loss=0.2344, train_acc=0.9415, val_acc=0.9260, val_f1=0.9267\n",
      "New best accuracy: 0.9260 at epoch 13, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1350, val loss=0.2308, train_acc=0.9499, val_acc=0.9145, val_f1=0.9130\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1186, val loss=0.2370, train_acc=0.9557, val_acc=0.9255, val_f1=0.9266\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.1082, val loss=0.2378, train_acc=0.9615, val_acc=0.9220, val_f1=0.9240\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.0913, val loss=0.2425, train_acc=0.9688, val_acc=0.9235, val_f1=0.9229\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.001: [0.7813799977302551, 0.7851600050926208, 0.7944999933242798]\n",
      "Test F1 Scores for lr=0.001: [0.7812902267242668, 0.7848307104275722, 0.7939475386518913]\n",
      "Average Test Accuracy: 0.7870, Average Test F1: 0.7867\n",
      "\n",
      "\n",
      "========================================\n",
      "Best learning rate: 0.0005 with average test accuracy: 0.7891\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.0005, 0.001]\n",
    "\n",
    "best_avg_acc = 0.0\n",
    "best_lr = None\n",
    "all_lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n{'='*40}\\nTesting learning rate: {lr}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(lr=lr, optimiser=\"adam\", weight_decay=0.0, method=\"last\")\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for lr={lr}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for lr={lr}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_lr_results[lr] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    # Update best configuration\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest learning rate: {best_lr} with average test accuracy: {best_avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd1229",
   "metadata": {},
   "source": [
    "# Tuning optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0445c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing optimiser: adam with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6086, val loss=0.5562, train_acc=0.6670, val_acc=0.7255, val_f1=0.7424\n",
      "New best accuracy: 0.7255 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5232, val loss=0.4327, train_acc=0.7461, val_acc=0.8020, val_f1=0.8185\n",
      "New best accuracy: 0.8020 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4394, val loss=0.3853, train_acc=0.8087, val_acc=0.8510, val_f1=0.8475\n",
      "New best accuracy: 0.8510 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4583, val loss=0.6335, train_acc=0.7915, val_acc=0.6400, val_f1=0.6137\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.5421, val loss=0.4002, train_acc=0.7258, val_acc=0.8230, val_f1=0.8007\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.3362, val loss=0.3320, train_acc=0.8675, val_acc=0.8720, val_f1=0.8769\n",
      "New best accuracy: 0.8720 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3278, val loss=0.2930, train_acc=0.8788, val_acc=0.8875, val_f1=0.8836\n",
      "New best accuracy: 0.8875 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2817, val loss=0.2960, train_acc=0.8888, val_acc=0.8835, val_f1=0.8871\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2553, val loss=0.3096, train_acc=0.9019, val_acc=0.8805, val_f1=0.8710\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.2524, val loss=0.2586, train_acc=0.8989, val_acc=0.9050, val_f1=0.9044\n",
      "New best accuracy: 0.9050 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2956, val loss=0.2827, train_acc=0.8786, val_acc=0.8975, val_f1=0.8931\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2414, val loss=0.2828, train_acc=0.9067, val_acc=0.8875, val_f1=0.8799\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.2250, val loss=0.3201, train_acc=0.9145, val_acc=0.8660, val_f1=0.8773\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.2209, val loss=0.2569, train_acc=0.9153, val_acc=0.9035, val_f1=0.8997\n",
      "Epoch 15: train loss=0.2043, val loss=0.2582, train_acc=0.9221, val_acc=0.9075, val_f1=0.9107\n",
      "New best accuracy: 0.9075 at epoch 15, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1912, val loss=0.2329, train_acc=0.9260, val_acc=0.9200, val_f1=0.9208\n",
      "New best accuracy: 0.9200 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.1821, val loss=0.2297, train_acc=0.9311, val_acc=0.9185, val_f1=0.9162\n",
      "Epoch 18: train loss=0.1780, val loss=0.2303, train_acc=0.9334, val_acc=0.9150, val_f1=0.9145\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.1615, val loss=0.2495, train_acc=0.9390, val_acc=0.9110, val_f1=0.9138\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 20: train loss=0.1540, val loss=0.2303, train_acc=0.9436, val_acc=0.9180, val_f1=0.9184\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 21: train loss=0.1407, val loss=0.2365, train_acc=0.9495, val_acc=0.9205, val_f1=0.9212\n",
      "New best accuracy: 0.9205 at epoch 21, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 22: train loss=0.1365, val loss=0.2488, train_acc=0.9506, val_acc=0.9205, val_f1=0.9201\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 22.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5935, val loss=0.4811, train_acc=0.6885, val_acc=0.7740, val_f1=0.7804\n",
      "New best accuracy: 0.7740 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5369, val loss=0.4560, train_acc=0.7416, val_acc=0.7995, val_f1=0.7947\n",
      "New best accuracy: 0.7995 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4700, val loss=0.4176, train_acc=0.7923, val_acc=0.8255, val_f1=0.8224\n",
      "New best accuracy: 0.8255 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4084, val loss=0.4355, train_acc=0.8333, val_acc=0.7970, val_f1=0.7669\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3980, val loss=0.3553, train_acc=0.8345, val_acc=0.8550, val_f1=0.8486\n",
      "New best accuracy: 0.8550 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3542, val loss=0.3805, train_acc=0.8589, val_acc=0.8440, val_f1=0.8241\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3101, val loss=0.2933, train_acc=0.8799, val_acc=0.8860, val_f1=0.8889\n",
      "New best accuracy: 0.8860 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3223, val loss=0.3623, train_acc=0.8662, val_acc=0.8640, val_f1=0.8538\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.3124, val loss=0.2711, train_acc=0.8730, val_acc=0.8970, val_f1=0.8965\n",
      "New best accuracy: 0.8970 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2586, val loss=0.2965, train_acc=0.9002, val_acc=0.8915, val_f1=0.8866\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2899, val loss=0.2865, train_acc=0.8882, val_acc=0.8925, val_f1=0.8943\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.2392, val loss=0.2770, train_acc=0.9090, val_acc=0.8895, val_f1=0.8950\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.2351, val loss=0.3187, train_acc=0.9123, val_acc=0.8775, val_f1=0.8662\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.2404, val loss=0.2552, train_acc=0.9093, val_acc=0.9090, val_f1=0.9071\n",
      "New best accuracy: 0.9090 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2103, val loss=0.2409, train_acc=0.9225, val_acc=0.9115, val_f1=0.9112\n",
      "New best accuracy: 0.9115 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2019, val loss=0.2699, train_acc=0.9213, val_acc=0.8970, val_f1=0.8903\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1955, val loss=0.2687, train_acc=0.9259, val_acc=0.9035, val_f1=0.9071\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.2112, val loss=0.2453, train_acc=0.9151, val_acc=0.9095, val_f1=0.9121\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.1835, val loss=0.2483, train_acc=0.9296, val_acc=0.9090, val_f1=0.9059\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 20: train loss=0.1666, val loss=0.2386, train_acc=0.9386, val_acc=0.9200, val_f1=0.9187\n",
      "New best accuracy: 0.9200 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.1594, val loss=0.2365, train_acc=0.9393, val_acc=0.9125, val_f1=0.9128\n",
      "Epoch 22: train loss=0.1618, val loss=0.2499, train_acc=0.9413, val_acc=0.9110, val_f1=0.9130\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.1379, val loss=0.2781, train_acc=0.9493, val_acc=0.9150, val_f1=0.9166\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.1587, val loss=0.2780, train_acc=0.9401, val_acc=0.9040, val_f1=0.8987\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 25: train loss=0.1188, val loss=0.2653, train_acc=0.9559, val_acc=0.9120, val_f1=0.9095\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 26: train loss=0.1139, val loss=0.2861, train_acc=0.9605, val_acc=0.9140, val_f1=0.9115\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 26.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6043, val loss=0.5227, train_acc=0.6809, val_acc=0.7890, val_f1=0.7898\n",
      "New best accuracy: 0.7890 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6580, val loss=0.6164, train_acc=0.6144, val_acc=0.6680, val_f1=0.5514\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5971, val loss=0.6317, train_acc=0.6933, val_acc=0.6740, val_f1=0.6374\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.5215, val loss=0.4661, train_acc=0.7590, val_acc=0.7990, val_f1=0.7747\n",
      "New best accuracy: 0.7990 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4158, val loss=0.4349, train_acc=0.8273, val_acc=0.8135, val_f1=0.8315\n",
      "New best accuracy: 0.8135 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3705, val loss=0.3439, train_acc=0.8526, val_acc=0.8610, val_f1=0.8540\n",
      "New best accuracy: 0.8610 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4021, val loss=0.3287, train_acc=0.8216, val_acc=0.8715, val_f1=0.8722\n",
      "New best accuracy: 0.8715 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3490, val loss=0.3193, train_acc=0.8619, val_acc=0.8685, val_f1=0.8725\n",
      "Epoch 09: train loss=0.2848, val loss=0.2858, train_acc=0.8878, val_acc=0.8885, val_f1=0.8884\n",
      "New best accuracy: 0.8885 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2749, val loss=0.2768, train_acc=0.8889, val_acc=0.8955, val_f1=0.8937\n",
      "New best accuracy: 0.8955 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2603, val loss=0.2732, train_acc=0.8974, val_acc=0.9015, val_f1=0.9007\n",
      "New best accuracy: 0.9015 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2531, val loss=0.2628, train_acc=0.8978, val_acc=0.8990, val_f1=0.8965\n",
      "Epoch 13: train loss=0.2351, val loss=0.2602, train_acc=0.9089, val_acc=0.9055, val_f1=0.9052\n",
      "New best accuracy: 0.9055 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2378, val loss=0.2630, train_acc=0.9070, val_acc=0.8995, val_f1=0.9012\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2226, val loss=0.2598, train_acc=0.9145, val_acc=0.9045, val_f1=0.9061\n",
      "Epoch 16: train loss=0.2112, val loss=0.2492, train_acc=0.9161, val_acc=0.9080, val_f1=0.9051\n",
      "New best accuracy: 0.9080 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2042, val loss=0.2468, train_acc=0.9200, val_acc=0.9085, val_f1=0.9052\n",
      "New best accuracy: 0.9085 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2012, val loss=0.2377, train_acc=0.9217, val_acc=0.9130, val_f1=0.9113\n",
      "New best accuracy: 0.9130 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1836, val loss=0.2428, train_acc=0.9293, val_acc=0.9105, val_f1=0.9069\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1696, val loss=0.2467, train_acc=0.9357, val_acc=0.9075, val_f1=0.9093\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1702, val loss=0.2916, train_acc=0.9345, val_acc=0.9045, val_f1=0.8992\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1539, val loss=0.2343, train_acc=0.9441, val_acc=0.9205, val_f1=0.9203\n",
      "New best accuracy: 0.9205 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.1480, val loss=0.3222, train_acc=0.9431, val_acc=0.9010, val_f1=0.9047\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1351, val loss=0.2546, train_acc=0.9479, val_acc=0.9110, val_f1=0.9127\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1301, val loss=0.2480, train_acc=0.9510, val_acc=0.9210, val_f1=0.9204\n",
      "New best accuracy: 0.9210 at epoch 25, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.1166, val loss=0.2677, train_acc=0.9547, val_acc=0.9120, val_f1=0.9117\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.1042, val loss=0.3231, train_acc=0.9601, val_acc=0.9085, val_f1=0.9052\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=adam: [0.7613000273704529, 0.7900999784469604, 0.7703800201416016]\n",
      "Test F1 Scores for optimiser=adam: [0.7557016831254214, 0.789677497342212, 0.7692251686817182]\n",
      "Average Test Accuracy: 0.7739, Average Test F1: 0.7715\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing optimiser: adamw with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6114, val loss=0.6464, train_acc=0.6665, val_acc=0.6610, val_f1=0.6439\n",
      "New best accuracy: 0.6610 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6497, val loss=0.6100, train_acc=0.6476, val_acc=0.6875, val_f1=0.6935\n",
      "New best accuracy: 0.6875 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5000, val loss=0.6627, train_acc=0.7744, val_acc=0.7255, val_f1=0.7762\n",
      "New best accuracy: 0.7255 at epoch 3, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.4720, val loss=0.4169, train_acc=0.7961, val_acc=0.8305, val_f1=0.8417\n",
      "New best accuracy: 0.8305 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3609, val loss=0.3347, train_acc=0.8569, val_acc=0.8675, val_f1=0.8687\n",
      "New best accuracy: 0.8675 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3297, val loss=0.3147, train_acc=0.8680, val_acc=0.8745, val_f1=0.8704\n",
      "New best accuracy: 0.8745 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3106, val loss=0.3235, train_acc=0.8776, val_acc=0.8745, val_f1=0.8719\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2972, val loss=0.2880, train_acc=0.8800, val_acc=0.8840, val_f1=0.8810\n",
      "New best accuracy: 0.8840 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2609, val loss=0.2673, train_acc=0.8952, val_acc=0.8945, val_f1=0.8943\n",
      "New best accuracy: 0.8945 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2488, val loss=0.2562, train_acc=0.9000, val_acc=0.9060, val_f1=0.9055\n",
      "New best accuracy: 0.9060 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2481, val loss=0.2462, train_acc=0.9025, val_acc=0.9095, val_f1=0.9086\n",
      "New best accuracy: 0.9095 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2256, val loss=0.2570, train_acc=0.9173, val_acc=0.9050, val_f1=0.9074\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2151, val loss=0.2396, train_acc=0.9175, val_acc=0.9115, val_f1=0.9116\n",
      "New best accuracy: 0.9115 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1989, val loss=0.2430, train_acc=0.9236, val_acc=0.9110, val_f1=0.9097\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1905, val loss=0.2510, train_acc=0.9255, val_acc=0.9145, val_f1=0.9128\n",
      "New best accuracy: 0.9145 at epoch 15, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1818, val loss=0.2309, train_acc=0.9321, val_acc=0.9160, val_f1=0.9160\n",
      "New best accuracy: 0.9160 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.1757, val loss=0.2843, train_acc=0.9353, val_acc=0.8990, val_f1=0.9045\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.1667, val loss=0.2370, train_acc=0.9361, val_acc=0.9150, val_f1=0.9146\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.1564, val loss=0.2762, train_acc=0.9401, val_acc=0.9135, val_f1=0.9121\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 20: train loss=0.1454, val loss=0.2590, train_acc=0.9453, val_acc=0.9160, val_f1=0.9170\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 21: train loss=0.1331, val loss=0.2564, train_acc=0.9504, val_acc=0.9180, val_f1=0.9169\n",
      "New best accuracy: 0.9180 at epoch 21, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 21.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5835, val loss=0.5052, train_acc=0.6870, val_acc=0.7675, val_f1=0.7455\n",
      "New best accuracy: 0.7675 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5223, val loss=0.4382, train_acc=0.7491, val_acc=0.8165, val_f1=0.8179\n",
      "New best accuracy: 0.8165 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4394, val loss=0.5458, train_acc=0.8121, val_acc=0.7620, val_f1=0.7010\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.4565, val loss=0.4339, train_acc=0.7996, val_acc=0.8150, val_f1=0.8307\n",
      "Epoch 05: train loss=0.3671, val loss=0.3693, train_acc=0.8480, val_acc=0.8505, val_f1=0.8546\n",
      "New best accuracy: 0.8505 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3897, val loss=0.3754, train_acc=0.8351, val_acc=0.8485, val_f1=0.8495\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3388, val loss=0.3914, train_acc=0.8595, val_acc=0.8605, val_f1=0.8619\n",
      "New best accuracy: 0.8605 at epoch 7, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.3137, val loss=0.3432, train_acc=0.8730, val_acc=0.8635, val_f1=0.8657\n",
      "New best accuracy: 0.8635 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2931, val loss=0.3325, train_acc=0.8835, val_acc=0.8675, val_f1=0.8703\n",
      "New best accuracy: 0.8675 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2933, val loss=0.2850, train_acc=0.8870, val_acc=0.8910, val_f1=0.8892\n",
      "New best accuracy: 0.8910 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2633, val loss=0.2867, train_acc=0.8971, val_acc=0.8915, val_f1=0.8943\n",
      "New best accuracy: 0.8915 at epoch 11, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.3423, val loss=0.3609, train_acc=0.8589, val_acc=0.8570, val_f1=0.8416\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.2980, val loss=0.2810, train_acc=0.8818, val_acc=0.8855, val_f1=0.8805\n",
      "Epoch 14: train loss=0.2462, val loss=0.3037, train_acc=0.9058, val_acc=0.8890, val_f1=0.8910\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2389, val loss=0.2545, train_acc=0.9124, val_acc=0.9005, val_f1=0.8992\n",
      "New best accuracy: 0.9005 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2238, val loss=0.2447, train_acc=0.9141, val_acc=0.9040, val_f1=0.9046\n",
      "New best accuracy: 0.9040 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2244, val loss=0.2561, train_acc=0.9153, val_acc=0.8990, val_f1=0.9008\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.2012, val loss=0.2597, train_acc=0.9251, val_acc=0.9050, val_f1=0.9013\n",
      "New best accuracy: 0.9050 at epoch 18, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.1947, val loss=0.2389, train_acc=0.9299, val_acc=0.9080, val_f1=0.9072\n",
      "New best accuracy: 0.9080 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.1794, val loss=0.2413, train_acc=0.9317, val_acc=0.9175, val_f1=0.9156\n",
      "New best accuracy: 0.9175 at epoch 20, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 21: train loss=0.1722, val loss=0.2462, train_acc=0.9380, val_acc=0.9095, val_f1=0.9124\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 22: train loss=0.1568, val loss=0.2330, train_acc=0.9420, val_acc=0.9185, val_f1=0.9170\n",
      "New best accuracy: 0.9185 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.1491, val loss=0.2356, train_acc=0.9465, val_acc=0.9110, val_f1=0.9121\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1444, val loss=0.2418, train_acc=0.9455, val_acc=0.9130, val_f1=0.9106\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1412, val loss=0.2589, train_acc=0.9500, val_acc=0.8965, val_f1=0.8940\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.1572, val loss=0.3021, train_acc=0.9410, val_acc=0.8840, val_f1=0.8884\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.1372, val loss=0.2323, train_acc=0.9471, val_acc=0.9165, val_f1=0.9151\n",
      "Epoch 28: train loss=0.1111, val loss=0.2486, train_acc=0.9609, val_acc=0.9155, val_f1=0.9153\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 29: train loss=0.1035, val loss=0.2495, train_acc=0.9631, val_acc=0.9190, val_f1=0.9190\n",
      "New best accuracy: 0.9190 at epoch 29, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 30: train loss=0.0906, val loss=0.2849, train_acc=0.9700, val_acc=0.9130, val_f1=0.9159\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 31: train loss=0.0947, val loss=0.2783, train_acc=0.9673, val_acc=0.9120, val_f1=0.9151\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 32: train loss=0.0831, val loss=0.2939, train_acc=0.9732, val_acc=0.9055, val_f1=0.9018\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 32.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6408, val loss=0.5579, train_acc=0.6320, val_acc=0.7200, val_f1=0.7371\n",
      "New best accuracy: 0.7200 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5801, val loss=0.4631, train_acc=0.7119, val_acc=0.7885, val_f1=0.7965\n",
      "New best accuracy: 0.7885 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5598, val loss=1.2260, train_acc=0.7209, val_acc=0.5040, val_f1=0.0159\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.5951, val loss=0.4932, train_acc=0.6879, val_acc=0.7545, val_f1=0.6990\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.5583, val loss=0.6330, train_acc=0.7200, val_acc=0.6745, val_f1=0.6632\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 06: train loss=0.5495, val loss=0.4433, train_acc=0.7160, val_acc=0.8045, val_f1=0.8260\n",
      "New best accuracy: 0.8045 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3683, val loss=0.3285, train_acc=0.8492, val_acc=0.8775, val_f1=0.8785\n",
      "New best accuracy: 0.8775 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3194, val loss=0.2984, train_acc=0.8681, val_acc=0.8815, val_f1=0.8796\n",
      "New best accuracy: 0.8815 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2941, val loss=0.2996, train_acc=0.8814, val_acc=0.8860, val_f1=0.8913\n",
      "New best accuracy: 0.8860 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2866, val loss=0.2929, train_acc=0.8868, val_acc=0.8885, val_f1=0.8853\n",
      "New best accuracy: 0.8885 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2737, val loss=0.2842, train_acc=0.8931, val_acc=0.8975, val_f1=0.9008\n",
      "New best accuracy: 0.8975 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2639, val loss=0.2692, train_acc=0.8951, val_acc=0.8985, val_f1=0.9005\n",
      "New best accuracy: 0.8985 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2450, val loss=0.2732, train_acc=0.9022, val_acc=0.8935, val_f1=0.8891\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2377, val loss=0.2652, train_acc=0.9084, val_acc=0.9025, val_f1=0.9007\n",
      "New best accuracy: 0.9025 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2264, val loss=0.2522, train_acc=0.9113, val_acc=0.9100, val_f1=0.9111\n",
      "New best accuracy: 0.9100 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2243, val loss=0.3235, train_acc=0.9149, val_acc=0.8560, val_f1=0.8400\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2133, val loss=0.2463, train_acc=0.9189, val_acc=0.9135, val_f1=0.9125\n",
      "New best accuracy: 0.9135 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2036, val loss=0.2434, train_acc=0.9213, val_acc=0.9160, val_f1=0.9149\n",
      "New best accuracy: 0.9160 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.2035, val loss=0.2680, train_acc=0.9225, val_acc=0.8960, val_f1=0.8908\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1897, val loss=0.2683, train_acc=0.9263, val_acc=0.9000, val_f1=0.8956\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1795, val loss=0.2457, train_acc=0.9349, val_acc=0.9125, val_f1=0.9141\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1690, val loss=0.2583, train_acc=0.9379, val_acc=0.9050, val_f1=0.9019\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1621, val loss=0.2448, train_acc=0.9410, val_acc=0.9085, val_f1=0.9070\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 23.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=adamw: [0.7839000225067139, 0.7922400236129761, 0.7873200178146362]\n",
      "Test F1 Scores for optimiser=adamw: [0.783605354887268, 0.791915006531417, 0.7871765736960278]\n",
      "Average Test Accuracy: 0.7878, Average Test F1: 0.7876\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing optimiser: sgd with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6934, val loss=0.6914, train_acc=0.5109, val_acc=0.5505, val_f1=0.4941\n",
      "New best accuracy: 0.5505 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6910, val loss=0.6895, train_acc=0.5270, val_acc=0.5760, val_f1=0.5023\n",
      "New best accuracy: 0.5760 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6886, val loss=0.6875, train_acc=0.5501, val_acc=0.6000, val_f1=0.5210\n",
      "New best accuracy: 0.6000 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.6862, val loss=0.6854, train_acc=0.5683, val_acc=0.6070, val_f1=0.5783\n",
      "New best accuracy: 0.6070 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.6835, val loss=0.6833, train_acc=0.5866, val_acc=0.6120, val_f1=0.6081\n",
      "New best accuracy: 0.6120 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.6807, val loss=0.6810, train_acc=0.5968, val_acc=0.6205, val_f1=0.6094\n",
      "New best accuracy: 0.6205 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.6779, val loss=0.6786, train_acc=0.6058, val_acc=0.6270, val_f1=0.6143\n",
      "New best accuracy: 0.6270 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.6746, val loss=0.6759, train_acc=0.6122, val_acc=0.6255, val_f1=0.6181\n",
      "Epoch 09: train loss=0.6721, val loss=0.6730, train_acc=0.6181, val_acc=0.6345, val_f1=0.6093\n",
      "New best accuracy: 0.6345 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.6692, val loss=0.6699, train_acc=0.6240, val_acc=0.6345, val_f1=0.6142\n",
      "Epoch 11: train loss=0.6646, val loss=0.6665, train_acc=0.6242, val_acc=0.6345, val_f1=0.6029\n",
      "Epoch 12: train loss=0.6609, val loss=0.6627, train_acc=0.6418, val_acc=0.6350, val_f1=0.6287\n",
      "New best accuracy: 0.6350 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.6561, val loss=0.6587, train_acc=0.6398, val_acc=0.6390, val_f1=0.6212\n",
      "New best accuracy: 0.6390 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.6516, val loss=0.6546, train_acc=0.6406, val_acc=0.6475, val_f1=0.6103\n",
      "New best accuracy: 0.6475 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.6453, val loss=0.6495, train_acc=0.6505, val_acc=0.6500, val_f1=0.6224\n",
      "New best accuracy: 0.6500 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.6402, val loss=0.6444, train_acc=0.6511, val_acc=0.6530, val_f1=0.6257\n",
      "New best accuracy: 0.6530 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.6327, val loss=0.6388, train_acc=0.6561, val_acc=0.6555, val_f1=0.6266\n",
      "New best accuracy: 0.6555 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.6245, val loss=0.6320, train_acc=0.6647, val_acc=0.6525, val_f1=0.6382\n",
      "Epoch 19: train loss=0.6188, val loss=0.6246, train_acc=0.6655, val_acc=0.6585, val_f1=0.6448\n",
      "New best accuracy: 0.6585 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.6066, val loss=0.6160, train_acc=0.6774, val_acc=0.6630, val_f1=0.6533\n",
      "New best accuracy: 0.6630 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.5984, val loss=0.6045, train_acc=0.6854, val_acc=0.6715, val_f1=0.6687\n",
      "New best accuracy: 0.6715 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.5831, val loss=0.5881, train_acc=0.6943, val_acc=0.6875, val_f1=0.6932\n",
      "New best accuracy: 0.6875 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.5605, val loss=0.5484, train_acc=0.7115, val_acc=0.7220, val_f1=0.7110\n",
      "New best accuracy: 0.7220 at epoch 23, saving model.\n",
      "Epoch 24: train loss=0.5307, val loss=0.5836, train_acc=0.7464, val_acc=0.7090, val_f1=0.6231\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 25: train loss=0.5445, val loss=0.4660, train_acc=0.7401, val_acc=0.8010, val_f1=0.8008\n",
      "New best accuracy: 0.8010 at epoch 25, saving model.\n",
      "Epoch 26: train loss=0.5448, val loss=0.5138, train_acc=0.7416, val_acc=0.7595, val_f1=0.7166\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 27: train loss=0.5674, val loss=0.6039, train_acc=0.7344, val_acc=0.6955, val_f1=0.6923\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 28: train loss=0.5715, val loss=0.5314, train_acc=0.7071, val_acc=0.7435, val_f1=0.7426\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 29: train loss=0.4739, val loss=0.4964, train_acc=0.7865, val_acc=0.7755, val_f1=0.8060\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 30: train loss=0.4910, val loss=0.4278, train_acc=0.7802, val_acc=0.8145, val_f1=0.8073\n",
      "New best accuracy: 0.8145 at epoch 30, saving model.\n",
      "Epoch 31: train loss=0.6797, val loss=0.6324, train_acc=0.6395, val_acc=0.6605, val_f1=0.6660\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 32: train loss=0.6184, val loss=0.6160, train_acc=0.6659, val_acc=0.6790, val_f1=0.6241\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 33: train loss=0.5899, val loss=0.5839, train_acc=0.6885, val_acc=0.6985, val_f1=0.7037\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 34: train loss=0.5288, val loss=0.4527, train_acc=0.7439, val_acc=0.7965, val_f1=0.8070\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 35: train loss=0.4608, val loss=0.4485, train_acc=0.7969, val_acc=0.8155, val_f1=0.7942\n",
      "New best accuracy: 0.8155 at epoch 35, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 35.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6920, val loss=0.6900, train_acc=0.5190, val_acc=0.5745, val_f1=0.5461\n",
      "New best accuracy: 0.5745 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6884, val loss=0.6876, train_acc=0.5515, val_acc=0.5955, val_f1=0.5210\n",
      "New best accuracy: 0.5955 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6867, val loss=0.6853, train_acc=0.5705, val_acc=0.6075, val_f1=0.5622\n",
      "New best accuracy: 0.6075 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.6833, val loss=0.6829, train_acc=0.5849, val_acc=0.6200, val_f1=0.5979\n",
      "New best accuracy: 0.6200 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.6802, val loss=0.6806, train_acc=0.5945, val_acc=0.6145, val_f1=0.5238\n",
      "Epoch 06: train loss=0.6778, val loss=0.6779, train_acc=0.6079, val_acc=0.6300, val_f1=0.6193\n",
      "New best accuracy: 0.6300 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.6743, val loss=0.6751, train_acc=0.6150, val_acc=0.6360, val_f1=0.5816\n",
      "New best accuracy: 0.6360 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.6714, val loss=0.6720, train_acc=0.6162, val_acc=0.6415, val_f1=0.6240\n",
      "New best accuracy: 0.6415 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.6670, val loss=0.6686, train_acc=0.6319, val_acc=0.6435, val_f1=0.6261\n",
      "New best accuracy: 0.6435 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.6640, val loss=0.6651, train_acc=0.6268, val_acc=0.6430, val_f1=0.5998\n",
      "Epoch 11: train loss=0.6587, val loss=0.6610, train_acc=0.6405, val_acc=0.6485, val_f1=0.6135\n",
      "New best accuracy: 0.6485 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.6541, val loss=0.6566, train_acc=0.6446, val_acc=0.6470, val_f1=0.6350\n",
      "Epoch 13: train loss=0.6484, val loss=0.6518, train_acc=0.6469, val_acc=0.6505, val_f1=0.6338\n",
      "New best accuracy: 0.6505 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.6432, val loss=0.6467, train_acc=0.6505, val_acc=0.6565, val_f1=0.6260\n",
      "New best accuracy: 0.6565 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.6370, val loss=0.6409, train_acc=0.6591, val_acc=0.6520, val_f1=0.6282\n",
      "Epoch 16: train loss=0.6313, val loss=0.6349, train_acc=0.6581, val_acc=0.6590, val_f1=0.6203\n",
      "New best accuracy: 0.6590 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.6225, val loss=0.6278, train_acc=0.6693, val_acc=0.6640, val_f1=0.6295\n",
      "New best accuracy: 0.6640 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.6122, val loss=0.6195, train_acc=0.6757, val_acc=0.6620, val_f1=0.6537\n",
      "Epoch 19: train loss=0.6033, val loss=0.6092, train_acc=0.6755, val_acc=0.6770, val_f1=0.6527\n",
      "New best accuracy: 0.6770 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.5889, val loss=0.6022, train_acc=0.6954, val_acc=0.6780, val_f1=0.6185\n",
      "New best accuracy: 0.6780 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.5724, val loss=0.5654, train_acc=0.7045, val_acc=0.7020, val_f1=0.6937\n",
      "New best accuracy: 0.7020 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.5244, val loss=0.5000, train_acc=0.7448, val_acc=0.7585, val_f1=0.7223\n",
      "New best accuracy: 0.7585 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.5946, val loss=0.6000, train_acc=0.7066, val_acc=0.6810, val_f1=0.6885\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.5712, val loss=0.5555, train_acc=0.7091, val_acc=0.7120, val_f1=0.6632\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.5129, val loss=0.5405, train_acc=0.7586, val_acc=0.7305, val_f1=0.6612\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.5170, val loss=0.8548, train_acc=0.7699, val_acc=0.5230, val_f1=0.0914\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.5646, val loss=0.6508, train_acc=0.7107, val_acc=0.6280, val_f1=0.4294\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6927, val loss=0.6905, train_acc=0.5134, val_acc=0.5705, val_f1=0.4816\n",
      "New best accuracy: 0.5705 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6893, val loss=0.6884, train_acc=0.5509, val_acc=0.6015, val_f1=0.5495\n",
      "New best accuracy: 0.6015 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6864, val loss=0.6863, train_acc=0.5739, val_acc=0.5735, val_f1=0.3543\n",
      "Epoch 04: train loss=0.6839, val loss=0.6839, train_acc=0.5857, val_acc=0.6215, val_f1=0.5295\n",
      "New best accuracy: 0.6215 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.6813, val loss=0.6815, train_acc=0.6090, val_acc=0.6165, val_f1=0.6011\n",
      "Epoch 06: train loss=0.6789, val loss=0.6790, train_acc=0.6052, val_acc=0.6330, val_f1=0.5682\n",
      "New best accuracy: 0.6330 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.6760, val loss=0.6764, train_acc=0.6182, val_acc=0.6065, val_f1=0.6272\n",
      "Epoch 08: train loss=0.6725, val loss=0.6732, train_acc=0.6179, val_acc=0.6285, val_f1=0.5960\n",
      "Epoch 09: train loss=0.6688, val loss=0.6698, train_acc=0.6279, val_acc=0.6320, val_f1=0.6039\n",
      "Epoch 10: train loss=0.6644, val loss=0.6664, train_acc=0.6331, val_acc=0.6460, val_f1=0.5791\n",
      "New best accuracy: 0.6460 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.6604, val loss=0.6618, train_acc=0.6338, val_acc=0.6365, val_f1=0.6188\n",
      "Epoch 12: train loss=0.6545, val loss=0.6572, train_acc=0.6449, val_acc=0.6425, val_f1=0.6086\n",
      "Epoch 13: train loss=0.6483, val loss=0.6519, train_acc=0.6466, val_acc=0.6430, val_f1=0.6198\n",
      "Epoch 14: train loss=0.6417, val loss=0.6460, train_acc=0.6579, val_acc=0.6445, val_f1=0.6179\n",
      "Epoch 15: train loss=0.6355, val loss=0.6394, train_acc=0.6584, val_acc=0.6435, val_f1=0.6233\n",
      "Epoch 16: train loss=0.6266, val loss=0.6320, train_acc=0.6670, val_acc=0.6615, val_f1=0.6270\n",
      "New best accuracy: 0.6615 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.6169, val loss=0.6228, train_acc=0.6739, val_acc=0.6560, val_f1=0.6486\n",
      "Epoch 18: train loss=0.6075, val loss=0.6117, train_acc=0.6805, val_acc=0.6680, val_f1=0.6445\n",
      "New best accuracy: 0.6680 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.5905, val loss=0.5952, train_acc=0.6954, val_acc=0.6765, val_f1=0.6474\n",
      "New best accuracy: 0.6765 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.5683, val loss=0.5555, train_acc=0.7127, val_acc=0.7180, val_f1=0.7063\n",
      "New best accuracy: 0.7180 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.5283, val loss=0.7114, train_acc=0.7495, val_acc=0.6225, val_f1=0.4106\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.5404, val loss=0.4611, train_acc=0.7476, val_acc=0.7990, val_f1=0.7938\n",
      "New best accuracy: 0.7990 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.5575, val loss=0.6100, train_acc=0.7370, val_acc=0.6510, val_f1=0.7212\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.5733, val loss=0.9243, train_acc=0.7361, val_acc=0.5045, val_f1=0.0198\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.6327, val loss=0.6112, train_acc=0.6559, val_acc=0.6835, val_f1=0.6765\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.5876, val loss=0.5867, train_acc=0.6966, val_acc=0.7005, val_f1=0.6736\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.5533, val loss=0.5283, train_acc=0.7222, val_acc=0.7440, val_f1=0.7555\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=sgd: [0.6907200217247009, 0.6450999975204468, 0.6536399722099304]\n",
      "Test F1 Scores for optimiser=sgd: [0.6895187979060909, 0.6422057292973729, 0.6516107222019041]\n",
      "Average Test Accuracy: 0.6632, Average Test F1: 0.6611\n",
      "\n",
      "\n",
      "========================================\n",
      "Best optimiser: adamw with average test accuracy: 0.7878\n"
     ]
    }
   ],
   "source": [
    "optimisers = [\"adam\", \"adamw\", \"sgd\"]  \n",
    "\n",
    "best_avg_acc_opt = 0.0\n",
    "best_optimiser = None\n",
    "all_optimiser_results = {}\n",
    "\n",
    "for opt in optimisers:\n",
    "    print(f\"\\n{'='*40}\\nTesting optimiser: {opt} with lr={best_lr}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(lr=best_lr, optimiser=opt, weight_decay=0.0, method=\"last\")\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for optimiser={opt}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for optimiser={opt}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_optimiser_results[opt] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_opt:\n",
    "        best_avg_acc_opt = avg_acc\n",
    "        best_optimiser = opt\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest optimiser: {best_optimiser} with average test accuracy: {best_avg_acc_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84e0ad",
   "metadata": {},
   "source": [
    "# Tuning weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea1022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing weight decay: 0.0 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6156, val loss=0.4832, train_acc=0.6679, val_acc=0.7870, val_f1=0.7878\n",
      "New best accuracy: 0.7870 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6395, val loss=0.5269, train_acc=0.6430, val_acc=0.7495, val_f1=0.7744\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5630, val loss=0.4683, train_acc=0.7181, val_acc=0.7840, val_f1=0.7483\n",
      "Epoch 04: train loss=0.4891, val loss=0.4701, train_acc=0.7857, val_acc=0.8010, val_f1=0.8239\n",
      "New best accuracy: 0.8010 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.4249, val loss=0.3817, train_acc=0.8279, val_acc=0.8475, val_f1=0.8480\n",
      "New best accuracy: 0.8475 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3733, val loss=0.3535, train_acc=0.8509, val_acc=0.8635, val_f1=0.8702\n",
      "New best accuracy: 0.8635 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3614, val loss=0.3195, train_acc=0.8536, val_acc=0.8725, val_f1=0.8705\n",
      "New best accuracy: 0.8725 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3158, val loss=0.3129, train_acc=0.8712, val_acc=0.8770, val_f1=0.8782\n",
      "New best accuracy: 0.8770 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3046, val loss=0.3483, train_acc=0.8798, val_acc=0.8725, val_f1=0.8636\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2795, val loss=0.3214, train_acc=0.8876, val_acc=0.8855, val_f1=0.8812\n",
      "New best accuracy: 0.8855 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.2769, val loss=0.2837, train_acc=0.8908, val_acc=0.8880, val_f1=0.8824\n",
      "New best accuracy: 0.8880 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2528, val loss=0.3000, train_acc=0.9002, val_acc=0.8855, val_f1=0.8913\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2368, val loss=0.2512, train_acc=0.9079, val_acc=0.9035, val_f1=0.9020\n",
      "New best accuracy: 0.9035 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2285, val loss=0.2585, train_acc=0.9119, val_acc=0.9055, val_f1=0.9082\n",
      "New best accuracy: 0.9055 at epoch 14, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2171, val loss=0.2651, train_acc=0.9159, val_acc=0.9030, val_f1=0.9042\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.2116, val loss=0.2569, train_acc=0.9175, val_acc=0.9105, val_f1=0.9133\n",
      "New best accuracy: 0.9105 at epoch 16, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.2034, val loss=0.2331, train_acc=0.9206, val_acc=0.9110, val_f1=0.9091\n",
      "New best accuracy: 0.9110 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2032, val loss=0.2301, train_acc=0.9185, val_acc=0.9145, val_f1=0.9144\n",
      "New best accuracy: 0.9145 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1806, val loss=0.2364, train_acc=0.9306, val_acc=0.9140, val_f1=0.9124\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1722, val loss=0.2869, train_acc=0.9339, val_acc=0.9000, val_f1=0.8932\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1651, val loss=0.2809, train_acc=0.9380, val_acc=0.8885, val_f1=0.8793\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1606, val loss=0.2353, train_acc=0.9367, val_acc=0.9190, val_f1=0.9179\n",
      "New best accuracy: 0.9190 at epoch 22, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1500, val loss=0.2296, train_acc=0.9434, val_acc=0.9200, val_f1=0.9202\n",
      "New best accuracy: 0.9200 at epoch 23, saving model.\n",
      "Epoch 24: train loss=0.1418, val loss=0.2624, train_acc=0.9469, val_acc=0.9095, val_f1=0.9052\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 25: train loss=0.1392, val loss=0.2704, train_acc=0.9493, val_acc=0.9150, val_f1=0.9148\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 26: train loss=0.1268, val loss=0.2532, train_acc=0.9529, val_acc=0.9175, val_f1=0.9171\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 27: train loss=0.1120, val loss=0.2605, train_acc=0.9597, val_acc=0.9130, val_f1=0.9119\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 28: train loss=0.1072, val loss=0.2559, train_acc=0.9615, val_acc=0.9185, val_f1=0.9181\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 28.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5732, val loss=0.4342, train_acc=0.7023, val_acc=0.8105, val_f1=0.8010\n",
      "New best accuracy: 0.8105 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5645, val loss=0.5575, train_acc=0.7241, val_acc=0.6930, val_f1=0.7454\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4558, val loss=0.4037, train_acc=0.7994, val_acc=0.8320, val_f1=0.8266\n",
      "New best accuracy: 0.8320 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3979, val loss=0.3651, train_acc=0.8327, val_acc=0.8575, val_f1=0.8632\n",
      "New best accuracy: 0.8575 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3809, val loss=0.4047, train_acc=0.8444, val_acc=0.8335, val_f1=0.8462\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.4202, val loss=0.3361, train_acc=0.8414, val_acc=0.8685, val_f1=0.8653\n",
      "New best accuracy: 0.8685 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3581, val loss=0.3240, train_acc=0.8542, val_acc=0.8710, val_f1=0.8648\n",
      "New best accuracy: 0.8710 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2972, val loss=0.3954, train_acc=0.8864, val_acc=0.8380, val_f1=0.8543\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2786, val loss=0.3117, train_acc=0.8935, val_acc=0.8850, val_f1=0.8777\n",
      "New best accuracy: 0.8850 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2871, val loss=0.2815, train_acc=0.8855, val_acc=0.8985, val_f1=0.8969\n",
      "New best accuracy: 0.8985 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.3086, val loss=0.3320, train_acc=0.8834, val_acc=0.8790, val_f1=0.8839\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2573, val loss=0.2813, train_acc=0.9062, val_acc=0.8920, val_f1=0.8946\n",
      "Epoch 13: train loss=0.2335, val loss=0.2651, train_acc=0.9159, val_acc=0.9025, val_f1=0.8982\n",
      "New best accuracy: 0.9025 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2261, val loss=0.3025, train_acc=0.9156, val_acc=0.8800, val_f1=0.8868\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2230, val loss=0.2618, train_acc=0.9169, val_acc=0.9050, val_f1=0.9061\n",
      "New best accuracy: 0.9050 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2009, val loss=0.2535, train_acc=0.9271, val_acc=0.9065, val_f1=0.9033\n",
      "New best accuracy: 0.9065 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.1976, val loss=0.2572, train_acc=0.9269, val_acc=0.9065, val_f1=0.9078\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.1858, val loss=0.2368, train_acc=0.9319, val_acc=0.9135, val_f1=0.9124\n",
      "New best accuracy: 0.9135 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1942, val loss=0.2408, train_acc=0.9277, val_acc=0.9170, val_f1=0.9181\n",
      "New best accuracy: 0.9170 at epoch 19, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1707, val loss=0.2884, train_acc=0.9377, val_acc=0.9135, val_f1=0.9120\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1643, val loss=0.3176, train_acc=0.9407, val_acc=0.8705, val_f1=0.8819\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1506, val loss=0.2576, train_acc=0.9461, val_acc=0.9140, val_f1=0.9112\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1465, val loss=0.2484, train_acc=0.9476, val_acc=0.9145, val_f1=0.9117\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 23.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6437, val loss=0.6146, train_acc=0.6420, val_acc=0.6875, val_f1=0.6427\n",
      "New best accuracy: 0.6875 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.7080, val loss=0.6803, train_acc=0.5717, val_acc=0.5495, val_f1=0.2019\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.6693, val loss=0.6673, train_acc=0.5940, val_acc=0.6165, val_f1=0.6681\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.6157, val loss=0.5373, train_acc=0.6579, val_acc=0.7345, val_f1=0.7751\n",
      "New best accuracy: 0.7345 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.5909, val loss=0.6317, train_acc=0.6886, val_acc=0.7105, val_f1=0.6875\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.5705, val loss=0.4493, train_acc=0.7115, val_acc=0.8245, val_f1=0.8124\n",
      "New best accuracy: 0.8245 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4228, val loss=0.3518, train_acc=0.8267, val_acc=0.8620, val_f1=0.8612\n",
      "New best accuracy: 0.8620 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3653, val loss=0.3197, train_acc=0.8536, val_acc=0.8725, val_f1=0.8771\n",
      "New best accuracy: 0.8725 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3156, val loss=0.2901, train_acc=0.8740, val_acc=0.8885, val_f1=0.8862\n",
      "New best accuracy: 0.8885 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.3046, val loss=0.3015, train_acc=0.8796, val_acc=0.8820, val_f1=0.8878\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2828, val loss=0.2702, train_acc=0.8915, val_acc=0.8950, val_f1=0.8942\n",
      "New best accuracy: 0.8950 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2680, val loss=0.2879, train_acc=0.8935, val_acc=0.8870, val_f1=0.8918\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2630, val loss=0.2966, train_acc=0.8989, val_acc=0.8910, val_f1=0.8849\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.2522, val loss=0.2767, train_acc=0.9021, val_acc=0.8905, val_f1=0.8948\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.2362, val loss=0.2564, train_acc=0.9087, val_acc=0.9040, val_f1=0.9013\n",
      "New best accuracy: 0.9040 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2335, val loss=0.2427, train_acc=0.9091, val_acc=0.9110, val_f1=0.9101\n",
      "New best accuracy: 0.9110 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2200, val loss=0.2560, train_acc=0.9173, val_acc=0.9070, val_f1=0.9044\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.2197, val loss=0.2543, train_acc=0.9164, val_acc=0.9025, val_f1=0.8979\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.2054, val loss=0.2422, train_acc=0.9216, val_acc=0.9150, val_f1=0.9159\n",
      "New best accuracy: 0.9150 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.2069, val loss=0.2552, train_acc=0.9204, val_acc=0.9005, val_f1=0.9045\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 21: train loss=0.1933, val loss=0.2509, train_acc=0.9267, val_acc=0.9070, val_f1=0.9103\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 22: train loss=0.1936, val loss=0.2372, train_acc=0.9233, val_acc=0.9125, val_f1=0.9132\n",
      "Epoch 23: train loss=0.1823, val loss=0.2482, train_acc=0.9306, val_acc=0.9015, val_f1=0.9030\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1795, val loss=0.2806, train_acc=0.9341, val_acc=0.9055, val_f1=0.9019\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1667, val loss=0.2380, train_acc=0.9376, val_acc=0.9105, val_f1=0.9111\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.1560, val loss=0.2407, train_acc=0.9440, val_acc=0.9120, val_f1=0.9124\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.1573, val loss=0.2668, train_acc=0.9445, val_acc=0.9085, val_f1=0.9056\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.0: [0.775879979133606, 0.7971000075340271, 0.7851200103759766]\n",
      "Test F1 Scores for weight_decay=0.0: [0.7746832312445292, 0.7968245525264847, 0.7841264927837875]\n",
      "Average Test Accuracy: 0.7860, Average Test F1: 0.7852\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 1e-05 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6451, val loss=0.6498, train_acc=0.6350, val_acc=0.6735, val_f1=0.6097\n",
      "New best accuracy: 0.6735 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5568, val loss=0.5914, train_acc=0.7268, val_acc=0.7110, val_f1=0.6986\n",
      "New best accuracy: 0.7110 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5725, val loss=0.5993, train_acc=0.7091, val_acc=0.6820, val_f1=0.6035\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.5578, val loss=0.4700, train_acc=0.7157, val_acc=0.7805, val_f1=0.7686\n",
      "New best accuracy: 0.7805 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4405, val loss=0.3665, train_acc=0.8026, val_acc=0.8535, val_f1=0.8522\n",
      "New best accuracy: 0.8535 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3946, val loss=0.4160, train_acc=0.8273, val_acc=0.8075, val_f1=0.8277\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3629, val loss=0.3231, train_acc=0.8489, val_acc=0.8660, val_f1=0.8621\n",
      "New best accuracy: 0.8660 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3255, val loss=0.3226, train_acc=0.8688, val_acc=0.8730, val_f1=0.8727\n",
      "New best accuracy: 0.8730 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3066, val loss=0.3036, train_acc=0.8765, val_acc=0.8785, val_f1=0.8742\n",
      "New best accuracy: 0.8785 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.3281, val loss=0.2984, train_acc=0.8764, val_acc=0.8835, val_f1=0.8787\n",
      "New best accuracy: 0.8835 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2783, val loss=0.2742, train_acc=0.8894, val_acc=0.8940, val_f1=0.8942\n",
      "New best accuracy: 0.8940 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2723, val loss=0.2756, train_acc=0.8900, val_acc=0.8950, val_f1=0.8943\n",
      "New best accuracy: 0.8950 at epoch 12, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2450, val loss=0.2694, train_acc=0.9049, val_acc=0.9010, val_f1=0.9028\n",
      "New best accuracy: 0.9010 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2299, val loss=0.2543, train_acc=0.9086, val_acc=0.9045, val_f1=0.9061\n",
      "New best accuracy: 0.9045 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2215, val loss=0.2458, train_acc=0.9109, val_acc=0.9075, val_f1=0.9080\n",
      "New best accuracy: 0.9075 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2025, val loss=0.2360, train_acc=0.9236, val_acc=0.9105, val_f1=0.9096\n",
      "New best accuracy: 0.9105 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.1975, val loss=0.2358, train_acc=0.9257, val_acc=0.9135, val_f1=0.9111\n",
      "New best accuracy: 0.9135 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.1844, val loss=0.2328, train_acc=0.9319, val_acc=0.9160, val_f1=0.9148\n",
      "New best accuracy: 0.9160 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1763, val loss=0.2400, train_acc=0.9334, val_acc=0.9115, val_f1=0.9107\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1716, val loss=0.2306, train_acc=0.9359, val_acc=0.9140, val_f1=0.9144\n",
      "Epoch 21: train loss=0.1627, val loss=0.2354, train_acc=0.9394, val_acc=0.9150, val_f1=0.9137\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.1469, val loss=0.2418, train_acc=0.9453, val_acc=0.9125, val_f1=0.9144\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 23: train loss=0.1372, val loss=0.2501, train_acc=0.9514, val_acc=0.9145, val_f1=0.9153\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 24: train loss=0.1361, val loss=0.2489, train_acc=0.9493, val_acc=0.9105, val_f1=0.9116\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 25: train loss=0.1238, val loss=0.2496, train_acc=0.9573, val_acc=0.9160, val_f1=0.9143\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 25.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6123, val loss=0.6458, train_acc=0.6670, val_acc=0.6690, val_f1=0.5978\n",
      "New best accuracy: 0.6690 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5364, val loss=0.4220, train_acc=0.7385, val_acc=0.8205, val_f1=0.8208\n",
      "New best accuracy: 0.8205 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6286, val loss=0.6038, train_acc=0.6686, val_acc=0.6820, val_f1=0.6074\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.4639, val loss=0.5297, train_acc=0.7895, val_acc=0.7450, val_f1=0.7246\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.4526, val loss=0.5105, train_acc=0.7951, val_acc=0.7555, val_f1=0.7509\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 06: train loss=0.5757, val loss=0.5536, train_acc=0.7009, val_acc=0.7395, val_f1=0.7835\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 07: train loss=0.4758, val loss=0.3928, train_acc=0.7845, val_acc=0.8460, val_f1=0.8365\n",
      "New best accuracy: 0.8460 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3853, val loss=0.5236, train_acc=0.8380, val_acc=0.7575, val_f1=0.7985\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.3475, val loss=0.3185, train_acc=0.8570, val_acc=0.8755, val_f1=0.8753\n",
      "New best accuracy: 0.8755 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.3125, val loss=0.2868, train_acc=0.8745, val_acc=0.8915, val_f1=0.8921\n",
      "New best accuracy: 0.8915 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2905, val loss=0.2913, train_acc=0.8835, val_acc=0.8910, val_f1=0.8943\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2762, val loss=0.2683, train_acc=0.8892, val_acc=0.9025, val_f1=0.9015\n",
      "New best accuracy: 0.9025 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2625, val loss=0.2610, train_acc=0.8966, val_acc=0.8990, val_f1=0.8960\n",
      "Epoch 14: train loss=0.2677, val loss=0.2567, train_acc=0.8948, val_acc=0.9025, val_f1=0.8993\n",
      "Epoch 15: train loss=0.2398, val loss=0.2564, train_acc=0.9055, val_acc=0.9055, val_f1=0.9061\n",
      "New best accuracy: 0.9055 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2312, val loss=0.2473, train_acc=0.9107, val_acc=0.9055, val_f1=0.9050\n",
      "Epoch 17: train loss=0.2176, val loss=0.2551, train_acc=0.9140, val_acc=0.9030, val_f1=0.8997\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.2019, val loss=0.2377, train_acc=0.9226, val_acc=0.9110, val_f1=0.9106\n",
      "New best accuracy: 0.9110 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1963, val loss=0.2417, train_acc=0.9270, val_acc=0.9115, val_f1=0.9107\n",
      "New best accuracy: 0.9115 at epoch 19, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1841, val loss=0.2416, train_acc=0.9303, val_acc=0.9085, val_f1=0.9062\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1787, val loss=0.2468, train_acc=0.9324, val_acc=0.9135, val_f1=0.9152\n",
      "New best accuracy: 0.9135 at epoch 21, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1694, val loss=0.2654, train_acc=0.9373, val_acc=0.8970, val_f1=0.9012\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1549, val loss=0.2700, train_acc=0.9410, val_acc=0.9085, val_f1=0.9109\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 23.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6311, val loss=0.6150, train_acc=0.6619, val_acc=0.6920, val_f1=0.6685\n",
      "New best accuracy: 0.6920 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5267, val loss=0.4557, train_acc=0.7446, val_acc=0.8040, val_f1=0.7988\n",
      "New best accuracy: 0.8040 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4989, val loss=0.4616, train_acc=0.7750, val_acc=0.7900, val_f1=0.8075\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.4121, val loss=0.3801, train_acc=0.8259, val_acc=0.8465, val_f1=0.8503\n",
      "New best accuracy: 0.8465 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4696, val loss=0.5204, train_acc=0.7865, val_acc=0.7525, val_f1=0.7267\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.5310, val loss=0.6147, train_acc=0.7444, val_acc=0.6750, val_f1=0.6007\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.5644, val loss=0.4321, train_acc=0.7056, val_acc=0.8120, val_f1=0.8324\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 08: train loss=0.3463, val loss=0.2960, train_acc=0.8600, val_acc=0.8860, val_f1=0.8861\n",
      "New best accuracy: 0.8860 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2977, val loss=0.3022, train_acc=0.8816, val_acc=0.8840, val_f1=0.8767\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2888, val loss=0.2798, train_acc=0.8849, val_acc=0.8980, val_f1=0.8946\n",
      "New best accuracy: 0.8980 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2624, val loss=0.2670, train_acc=0.8970, val_acc=0.8995, val_f1=0.9003\n",
      "New best accuracy: 0.8995 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2491, val loss=0.2667, train_acc=0.9002, val_acc=0.8985, val_f1=0.9011\n",
      "Epoch 13: train loss=0.2386, val loss=0.2809, train_acc=0.9052, val_acc=0.8915, val_f1=0.8945\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2258, val loss=0.2546, train_acc=0.9100, val_acc=0.9070, val_f1=0.9088\n",
      "New best accuracy: 0.9070 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2211, val loss=0.2603, train_acc=0.9121, val_acc=0.9025, val_f1=0.8998\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.2126, val loss=0.2474, train_acc=0.9159, val_acc=0.9100, val_f1=0.9104\n",
      "New best accuracy: 0.9100 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2001, val loss=0.2685, train_acc=0.9210, val_acc=0.8945, val_f1=0.8978\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.1926, val loss=0.2592, train_acc=0.9231, val_acc=0.9155, val_f1=0.9156\n",
      "New best accuracy: 0.9155 at epoch 18, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.1852, val loss=0.2415, train_acc=0.9291, val_acc=0.9125, val_f1=0.9123\n",
      "Epoch 20: train loss=0.1801, val loss=0.2441, train_acc=0.9306, val_acc=0.9150, val_f1=0.9150\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 21: train loss=0.1682, val loss=0.2487, train_acc=0.9341, val_acc=0.9135, val_f1=0.9157\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 22: train loss=0.1588, val loss=0.2383, train_acc=0.9407, val_acc=0.9130, val_f1=0.9143\n",
      "Epoch 23: train loss=0.1541, val loss=0.2413, train_acc=0.9410, val_acc=0.9155, val_f1=0.9129\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1418, val loss=0.2411, train_acc=0.9466, val_acc=0.9115, val_f1=0.9112\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1334, val loss=0.2630, train_acc=0.9489, val_acc=0.9165, val_f1=0.9160\n",
      "New best accuracy: 0.9165 at epoch 25, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.1209, val loss=0.2555, train_acc=0.9550, val_acc=0.9120, val_f1=0.9140\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.1166, val loss=0.2579, train_acc=0.9587, val_acc=0.9135, val_f1=0.9135\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=1e-05: [0.7753199934959412, 0.7814599871635437, 0.7823399901390076]\n",
      "Test F1 Scores for weight_decay=1e-05: [0.775267254685316, 0.7791578508924215, 0.7812737461711041]\n",
      "Average Test Accuracy: 0.7797, Average Test F1: 0.7786\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 0.0001 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6554, val loss=0.6313, train_acc=0.6184, val_acc=0.6525, val_f1=0.6638\n",
      "New best accuracy: 0.6525 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5519, val loss=0.4383, train_acc=0.7195, val_acc=0.7935, val_f1=0.7719\n",
      "New best accuracy: 0.7935 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5345, val loss=0.4994, train_acc=0.7552, val_acc=0.7800, val_f1=0.7913\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.4675, val loss=0.4000, train_acc=0.7904, val_acc=0.8310, val_f1=0.8260\n",
      "New best accuracy: 0.8310 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4859, val loss=0.4470, train_acc=0.7765, val_acc=0.7885, val_f1=0.8095\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.4412, val loss=0.6187, train_acc=0.8060, val_acc=0.6310, val_f1=0.7083\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.5089, val loss=0.4099, train_acc=0.7592, val_acc=0.8250, val_f1=0.8126\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 08: train loss=0.5808, val loss=0.5944, train_acc=0.6926, val_acc=0.6860, val_f1=0.6857\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 09: train loss=0.5604, val loss=0.6129, train_acc=0.7115, val_acc=0.6585, val_f1=0.6528\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 9.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5718, val loss=0.5002, train_acc=0.7019, val_acc=0.7570, val_f1=0.7923\n",
      "New best accuracy: 0.7570 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5756, val loss=0.5408, train_acc=0.7114, val_acc=0.7375, val_f1=0.7235\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4649, val loss=0.5852, train_acc=0.7950, val_acc=0.7090, val_f1=0.6052\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.4033, val loss=0.4033, train_acc=0.8316, val_acc=0.8365, val_f1=0.8488\n",
      "New best accuracy: 0.8365 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3836, val loss=0.3573, train_acc=0.8468, val_acc=0.8615, val_f1=0.8659\n",
      "New best accuracy: 0.8615 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3714, val loss=0.3264, train_acc=0.8474, val_acc=0.8730, val_f1=0.8699\n",
      "New best accuracy: 0.8730 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3322, val loss=0.5188, train_acc=0.8745, val_acc=0.8330, val_f1=0.8459\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.3224, val loss=0.3486, train_acc=0.8721, val_acc=0.8750, val_f1=0.8662\n",
      "New best accuracy: 0.8750 at epoch 8, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.2934, val loss=0.3047, train_acc=0.8878, val_acc=0.8840, val_f1=0.8834\n",
      "New best accuracy: 0.8840 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2682, val loss=0.3245, train_acc=0.8985, val_acc=0.8745, val_f1=0.8762\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2590, val loss=0.2960, train_acc=0.9009, val_acc=0.8825, val_f1=0.8739\n",
      "Epoch 12: train loss=0.2378, val loss=0.2603, train_acc=0.9107, val_acc=0.9020, val_f1=0.8992\n",
      "New best accuracy: 0.9020 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2326, val loss=0.2644, train_acc=0.9123, val_acc=0.9015, val_f1=0.9048\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2256, val loss=0.2439, train_acc=0.9130, val_acc=0.9090, val_f1=0.9100\n",
      "New best accuracy: 0.9090 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2117, val loss=0.2335, train_acc=0.9193, val_acc=0.9190, val_f1=0.9194\n",
      "New best accuracy: 0.9190 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.1981, val loss=0.2454, train_acc=0.9237, val_acc=0.9110, val_f1=0.9086\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1839, val loss=0.2331, train_acc=0.9315, val_acc=0.9195, val_f1=0.9182\n",
      "New best accuracy: 0.9195 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.1863, val loss=0.2259, train_acc=0.9284, val_acc=0.9155, val_f1=0.9144\n",
      "Epoch 19: train loss=0.1713, val loss=0.2543, train_acc=0.9366, val_acc=0.9090, val_f1=0.9047\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1698, val loss=0.2320, train_acc=0.9373, val_acc=0.9145, val_f1=0.9164\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1581, val loss=0.2494, train_acc=0.9439, val_acc=0.9225, val_f1=0.9218\n",
      "New best accuracy: 0.9225 at epoch 21, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1439, val loss=0.2432, train_acc=0.9457, val_acc=0.9095, val_f1=0.9066\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1359, val loss=0.2782, train_acc=0.9504, val_acc=0.9110, val_f1=0.9152\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 23.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6636, val loss=0.6558, train_acc=0.6206, val_acc=0.6610, val_f1=0.6454\n",
      "New best accuracy: 0.6610 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5545, val loss=0.4668, train_acc=0.7381, val_acc=0.7940, val_f1=0.7889\n",
      "New best accuracy: 0.7940 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4548, val loss=0.3893, train_acc=0.7977, val_acc=0.8385, val_f1=0.8294\n",
      "New best accuracy: 0.8385 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4056, val loss=0.3557, train_acc=0.8249, val_acc=0.8570, val_f1=0.8553\n",
      "New best accuracy: 0.8570 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3543, val loss=0.3649, train_acc=0.8515, val_acc=0.8395, val_f1=0.8243\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.3367, val loss=0.3581, train_acc=0.8649, val_acc=0.8490, val_f1=0.8601\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.3134, val loss=0.3159, train_acc=0.8764, val_acc=0.8765, val_f1=0.8819\n",
      "New best accuracy: 0.8765 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3515, val loss=0.3531, train_acc=0.8558, val_acc=0.8665, val_f1=0.8539\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2888, val loss=0.2936, train_acc=0.8864, val_acc=0.8925, val_f1=0.8942\n",
      "New best accuracy: 0.8925 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2524, val loss=0.2743, train_acc=0.9011, val_acc=0.8925, val_f1=0.8893\n",
      "Epoch 11: train loss=0.2362, val loss=0.2675, train_acc=0.9074, val_acc=0.8990, val_f1=0.9006\n",
      "New best accuracy: 0.8990 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2330, val loss=0.2612, train_acc=0.9080, val_acc=0.8990, val_f1=0.8959\n",
      "Epoch 13: train loss=0.2234, val loss=0.2441, train_acc=0.9157, val_acc=0.9080, val_f1=0.9062\n",
      "New best accuracy: 0.9080 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2146, val loss=0.2414, train_acc=0.9196, val_acc=0.9100, val_f1=0.9086\n",
      "New best accuracy: 0.9100 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1901, val loss=0.2395, train_acc=0.9285, val_acc=0.9105, val_f1=0.9097\n",
      "New best accuracy: 0.9105 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.1810, val loss=0.2518, train_acc=0.9323, val_acc=0.9145, val_f1=0.9139\n",
      "New best accuracy: 0.9145 at epoch 16, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1741, val loss=0.2443, train_acc=0.9333, val_acc=0.9085, val_f1=0.9045\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.1639, val loss=0.2556, train_acc=0.9396, val_acc=0.9125, val_f1=0.9093\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.1512, val loss=0.2417, train_acc=0.9420, val_acc=0.9170, val_f1=0.9181\n",
      "New best accuracy: 0.9170 at epoch 19, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 20: train loss=0.1434, val loss=0.2388, train_acc=0.9467, val_acc=0.9140, val_f1=0.9125\n",
      "Epoch 21: train loss=0.1348, val loss=0.2746, train_acc=0.9491, val_acc=0.9080, val_f1=0.9115\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.1278, val loss=0.2368, train_acc=0.9560, val_acc=0.9190, val_f1=0.9188\n",
      "New best accuracy: 0.9190 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.1147, val loss=0.2572, train_acc=0.9571, val_acc=0.9140, val_f1=0.9143\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.0969, val loss=0.2543, train_acc=0.9661, val_acc=0.9075, val_f1=0.9049\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.0960, val loss=0.2566, train_acc=0.9665, val_acc=0.9090, val_f1=0.9079\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.0828, val loss=0.3359, train_acc=0.9716, val_acc=0.9130, val_f1=0.9151\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.0838, val loss=0.2620, train_acc=0.9708, val_acc=0.9130, val_f1=0.9131\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.0001: [0.6970199942588806, 0.7994999885559082, 0.7934399843215942]\n",
      "Test F1 Scores for weight_decay=0.0001: [0.6968228590641065, 0.7994989393493891, 0.7932509367478161]\n",
      "Average Test Accuracy: 0.7633, Average Test F1: 0.7632\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 0.001 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6410, val loss=0.6343, train_acc=0.6415, val_acc=0.6565, val_f1=0.6686\n",
      "New best accuracy: 0.6565 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5139, val loss=0.4451, train_acc=0.7562, val_acc=0.8015, val_f1=0.7773\n",
      "New best accuracy: 0.8015 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4716, val loss=0.4979, train_acc=0.7933, val_acc=0.7995, val_f1=0.8193\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.6705, val loss=0.5988, train_acc=0.6166, val_acc=0.6970, val_f1=0.6259\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.6139, val loss=0.6008, train_acc=0.6649, val_acc=0.6825, val_f1=0.6775\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 06: train loss=0.6010, val loss=0.6485, train_acc=0.6836, val_acc=0.6265, val_f1=0.5010\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 07: train loss=0.5920, val loss=0.5894, train_acc=0.6756, val_acc=0.7070, val_f1=0.7158\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 7.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5624, val loss=0.4926, train_acc=0.7089, val_acc=0.7655, val_f1=0.7163\n",
      "New best accuracy: 0.7655 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4939, val loss=0.6230, train_acc=0.7662, val_acc=0.6470, val_f1=0.6952\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5144, val loss=0.6136, train_acc=0.7495, val_acc=0.6885, val_f1=0.6548\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.5085, val loss=0.4280, train_acc=0.7596, val_acc=0.8225, val_f1=0.8297\n",
      "New best accuracy: 0.8225 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4786, val loss=0.4278, train_acc=0.7795, val_acc=0.8175, val_f1=0.8343\n",
      "Epoch 06: train loss=0.4473, val loss=0.4145, train_acc=0.8024, val_acc=0.8265, val_f1=0.8245\n",
      "New best accuracy: 0.8265 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3540, val loss=0.3506, train_acc=0.8561, val_acc=0.8560, val_f1=0.8486\n",
      "New best accuracy: 0.8560 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3246, val loss=0.3545, train_acc=0.8695, val_acc=0.8630, val_f1=0.8709\n",
      "New best accuracy: 0.8630 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.3083, val loss=0.3138, train_acc=0.8779, val_acc=0.8785, val_f1=0.8799\n",
      "New best accuracy: 0.8785 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.4305, val loss=0.3536, train_acc=0.8033, val_acc=0.8675, val_f1=0.8590\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.3193, val loss=0.3543, train_acc=0.8754, val_acc=0.8520, val_f1=0.8627\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.3685, val loss=0.3044, train_acc=0.8380, val_acc=0.8715, val_f1=0.8689\n",
      "Epoch 13: train loss=0.3015, val loss=0.4126, train_acc=0.8785, val_acc=0.8030, val_f1=0.8193\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2764, val loss=0.2978, train_acc=0.8906, val_acc=0.8895, val_f1=0.8830\n",
      "New best accuracy: 0.8895 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2520, val loss=0.2657, train_acc=0.9009, val_acc=0.9010, val_f1=0.9013\n",
      "New best accuracy: 0.9010 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2428, val loss=0.2748, train_acc=0.9036, val_acc=0.9020, val_f1=0.9055\n",
      "New best accuracy: 0.9020 at epoch 16, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2334, val loss=0.2468, train_acc=0.9126, val_acc=0.9105, val_f1=0.9103\n",
      "New best accuracy: 0.9105 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2366, val loss=0.2602, train_acc=0.9124, val_acc=0.8990, val_f1=0.8938\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.2211, val loss=0.3022, train_acc=0.9133, val_acc=0.8840, val_f1=0.8746\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 20: train loss=0.2234, val loss=0.2491, train_acc=0.9131, val_acc=0.9075, val_f1=0.9059\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 21: train loss=0.2072, val loss=0.2627, train_acc=0.9239, val_acc=0.9050, val_f1=0.9003\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 22: train loss=0.1997, val loss=0.2493, train_acc=0.9251, val_acc=0.9150, val_f1=0.9163\n",
      "New best accuracy: 0.9150 at epoch 22, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 22.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5890, val loss=0.4933, train_acc=0.6825, val_acc=0.7735, val_f1=0.7462\n",
      "New best accuracy: 0.7735 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4523, val loss=0.3659, train_acc=0.8054, val_acc=0.8460, val_f1=0.8425\n",
      "New best accuracy: 0.8460 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3874, val loss=0.3945, train_acc=0.8414, val_acc=0.8350, val_f1=0.8276\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.5689, val loss=0.4291, train_acc=0.7262, val_acc=0.8305, val_f1=0.8294\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.4124, val loss=0.3787, train_acc=0.8254, val_acc=0.8465, val_f1=0.8547\n",
      "New best accuracy: 0.8465 at epoch 5, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 06: train loss=0.5458, val loss=0.4071, train_acc=0.7286, val_acc=0.8325, val_f1=0.8398\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 07: train loss=0.5243, val loss=0.6131, train_acc=0.7301, val_acc=0.6460, val_f1=0.6853\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 7.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.001: [0.6930000185966492, 0.7842000126838684, 0.6699000000953674]\n",
      "Test F1 Scores for weight_decay=0.001: [0.6904478539949609, 0.7837980081992963, 0.6571805536979505]\n",
      "Average Test Accuracy: 0.7157, Average Test F1: 0.7105\n",
      "\n",
      "\n",
      "========================================\n",
      "Best weight decay: 0.0 with average test accuracy: 0.7860\n"
     ]
    }
   ],
   "source": [
    "weight_decays = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "best_avg_acc_wd = 0.0\n",
    "best_weight_decay = None\n",
    "all_wd_results = {}\n",
    "\n",
    "for wd in weight_decays:\n",
    "    print(f\"\\n{'='*40}\\nTesting weight decay: {wd} with lr={best_lr}, optimiser={best_optimiser}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(lr=best_lr, optimiser=best_optimiser, weight_decay=wd, method=\"last\")\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for weight_decay={wd}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for weight_decay={wd}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_wd_results[wd] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_wd:\n",
    "        best_avg_acc_wd = avg_acc\n",
    "        best_weight_decay = wd\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest weight decay: {best_weight_decay} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_wd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027f1de",
   "metadata": {},
   "source": [
    "# Tuning hidden state pooling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30f5ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing method: last\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6334, val loss=0.5687, train_acc=0.6348, val_acc=0.6905, val_f1=0.7319\n",
      "New best accuracy: 0.6905 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5849, val loss=0.5730, train_acc=0.6981, val_acc=0.7040, val_f1=0.6350\n",
      "New best accuracy: 0.7040 at epoch 2, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5197, val loss=0.5268, train_acc=0.7559, val_acc=0.7535, val_f1=0.7455\n",
      "New best accuracy: 0.7535 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4799, val loss=0.4687, train_acc=0.7780, val_acc=0.7845, val_f1=0.7638\n",
      "New best accuracy: 0.7845 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4440, val loss=0.4454, train_acc=0.8021, val_acc=0.8020, val_f1=0.8162\n",
      "New best accuracy: 0.8020 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.4699, val loss=0.5081, train_acc=0.7909, val_acc=0.7695, val_f1=0.7618\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.4319, val loss=0.4325, train_acc=0.8114, val_acc=0.8035, val_f1=0.8262\n",
      "New best accuracy: 0.8035 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3432, val loss=0.4941, train_acc=0.8601, val_acc=0.7685, val_f1=0.7101\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.4410, val loss=0.6354, train_acc=0.8076, val_acc=0.6330, val_f1=0.5557\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.5618, val loss=0.4057, train_acc=0.7037, val_acc=0.8345, val_f1=0.8411\n",
      "New best accuracy: 0.8345 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.4126, val loss=0.3508, train_acc=0.8216, val_acc=0.8665, val_f1=0.8626\n",
      "New best accuracy: 0.8665 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.3169, val loss=0.3276, train_acc=0.8739, val_acc=0.8725, val_f1=0.8661\n",
      "New best accuracy: 0.8725 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.3000, val loss=0.3009, train_acc=0.8831, val_acc=0.8860, val_f1=0.8895\n",
      "New best accuracy: 0.8860 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2775, val loss=0.2792, train_acc=0.8882, val_acc=0.8930, val_f1=0.8922\n",
      "New best accuracy: 0.8930 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2733, val loss=0.2674, train_acc=0.8934, val_acc=0.8975, val_f1=0.8972\n",
      "New best accuracy: 0.8975 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2538, val loss=0.2549, train_acc=0.8998, val_acc=0.9050, val_f1=0.9037\n",
      "New best accuracy: 0.9050 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2529, val loss=0.2751, train_acc=0.8980, val_acc=0.8915, val_f1=0.8933\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.2381, val loss=0.2602, train_acc=0.9073, val_acc=0.9060, val_f1=0.9079\n",
      "New best accuracy: 0.9060 at epoch 18, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.2224, val loss=0.2594, train_acc=0.9149, val_acc=0.9025, val_f1=0.9055\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 20: train loss=0.2116, val loss=0.2780, train_acc=0.9147, val_acc=0.8970, val_f1=0.9014\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 21: train loss=0.2046, val loss=0.2317, train_acc=0.9197, val_acc=0.9190, val_f1=0.9185\n",
      "New best accuracy: 0.9190 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.1924, val loss=0.2367, train_acc=0.9276, val_acc=0.9130, val_f1=0.9102\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.1794, val loss=0.2326, train_acc=0.9303, val_acc=0.9145, val_f1=0.9137\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.1801, val loss=0.2256, train_acc=0.9329, val_acc=0.9145, val_f1=0.9124\n",
      "Epoch 25: train loss=0.1635, val loss=0.2565, train_acc=0.9403, val_acc=0.9055, val_f1=0.9079\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 26: train loss=0.1539, val loss=0.2285, train_acc=0.9446, val_acc=0.9160, val_f1=0.9138\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 27: train loss=0.1498, val loss=0.2373, train_acc=0.9411, val_acc=0.9200, val_f1=0.9197\n",
      "New best accuracy: 0.9200 at epoch 27, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 28: train loss=0.1416, val loss=0.2769, train_acc=0.9487, val_acc=0.9165, val_f1=0.9151\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 29: train loss=0.1232, val loss=0.2759, train_acc=0.9546, val_acc=0.9115, val_f1=0.9083\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 29.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5960, val loss=0.5021, train_acc=0.6791, val_acc=0.7810, val_f1=0.7733\n",
      "New best accuracy: 0.7810 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4814, val loss=0.4858, train_acc=0.7775, val_acc=0.7780, val_f1=0.8023\n",
      "Epoch 03: train loss=0.4305, val loss=0.5928, train_acc=0.8194, val_acc=0.6935, val_f1=0.5698\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3941, val loss=0.3566, train_acc=0.8400, val_acc=0.8655, val_f1=0.8658\n",
      "New best accuracy: 0.8655 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4121, val loss=0.3559, train_acc=0.8210, val_acc=0.8610, val_f1=0.8616\n",
      "Epoch 06: train loss=0.3363, val loss=0.3167, train_acc=0.8671, val_acc=0.8805, val_f1=0.8789\n",
      "New best accuracy: 0.8805 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3087, val loss=0.2913, train_acc=0.8829, val_acc=0.8855, val_f1=0.8780\n",
      "New best accuracy: 0.8855 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2683, val loss=0.3107, train_acc=0.8972, val_acc=0.8820, val_f1=0.8808\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2665, val loss=0.2928, train_acc=0.8970, val_acc=0.8880, val_f1=0.8838\n",
      "New best accuracy: 0.8880 at epoch 9, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.3589, val loss=0.4422, train_acc=0.8433, val_acc=0.8275, val_f1=0.8041\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.3124, val loss=0.2792, train_acc=0.8755, val_acc=0.8940, val_f1=0.8930\n",
      "New best accuracy: 0.8940 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2620, val loss=0.2990, train_acc=0.9030, val_acc=0.8860, val_f1=0.8834\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2593, val loss=0.2631, train_acc=0.8975, val_acc=0.8995, val_f1=0.8954\n",
      "New best accuracy: 0.8995 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2225, val loss=0.3215, train_acc=0.9121, val_acc=0.8775, val_f1=0.8865\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2141, val loss=0.2463, train_acc=0.9177, val_acc=0.9095, val_f1=0.9081\n",
      "New best accuracy: 0.9095 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2007, val loss=0.2631, train_acc=0.9251, val_acc=0.9010, val_f1=0.9044\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1927, val loss=0.2452, train_acc=0.9284, val_acc=0.9135, val_f1=0.9139\n",
      "New best accuracy: 0.9135 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.1834, val loss=0.2369, train_acc=0.9329, val_acc=0.9135, val_f1=0.9134\n",
      "Epoch 19: train loss=0.1751, val loss=0.2285, train_acc=0.9369, val_acc=0.9185, val_f1=0.9175\n",
      "New best accuracy: 0.9185 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.1649, val loss=0.2632, train_acc=0.9403, val_acc=0.9020, val_f1=0.9065\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 21: train loss=0.1610, val loss=0.2467, train_acc=0.9414, val_acc=0.9070, val_f1=0.9097\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 22: train loss=0.1501, val loss=0.2315, train_acc=0.9486, val_acc=0.9145, val_f1=0.9151\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 23: train loss=0.1455, val loss=0.2556, train_acc=0.9459, val_acc=0.9090, val_f1=0.9096\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 24: train loss=0.1335, val loss=0.2685, train_acc=0.9526, val_acc=0.9155, val_f1=0.9163\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 24.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5902, val loss=0.5391, train_acc=0.6883, val_acc=0.7460, val_f1=0.7782\n",
      "New best accuracy: 0.7460 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5657, val loss=0.6467, train_acc=0.7125, val_acc=0.6400, val_f1=0.4562\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5271, val loss=0.5511, train_acc=0.7421, val_acc=0.7105, val_f1=0.6163\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.6115, val loss=0.6010, train_acc=0.6825, val_acc=0.6810, val_f1=0.6745\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 05: train loss=0.4735, val loss=0.4684, train_acc=0.7798, val_acc=0.7820, val_f1=0.7462\n",
      "New best accuracy: 0.7820 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.4210, val loss=0.4623, train_acc=0.8201, val_acc=0.7745, val_f1=0.7285\n",
      "Epoch 07: train loss=0.3867, val loss=0.3624, train_acc=0.8373, val_acc=0.8585, val_f1=0.8631\n",
      "New best accuracy: 0.8585 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3586, val loss=0.4302, train_acc=0.8570, val_acc=0.8110, val_f1=0.8327\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.3418, val loss=0.3491, train_acc=0.8628, val_acc=0.8575, val_f1=0.8472\n",
      "Epoch 10: train loss=0.3207, val loss=0.3683, train_acc=0.8734, val_acc=0.8560, val_f1=0.8668\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.3128, val loss=0.3366, train_acc=0.8774, val_acc=0.8705, val_f1=0.8617\n",
      "New best accuracy: 0.8705 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.3297, val loss=0.2995, train_acc=0.8655, val_acc=0.8830, val_f1=0.8786\n",
      "New best accuracy: 0.8830 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2846, val loss=0.2834, train_acc=0.8901, val_acc=0.8935, val_f1=0.8934\n",
      "New best accuracy: 0.8935 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2703, val loss=0.2712, train_acc=0.8934, val_acc=0.9005, val_f1=0.9006\n",
      "New best accuracy: 0.9005 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2550, val loss=0.2619, train_acc=0.9034, val_acc=0.9065, val_f1=0.9071\n",
      "New best accuracy: 0.9065 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2555, val loss=0.2709, train_acc=0.9022, val_acc=0.8960, val_f1=0.8913\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2257, val loss=0.3211, train_acc=0.9146, val_acc=0.8685, val_f1=0.8784\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.2272, val loss=0.2644, train_acc=0.9139, val_acc=0.8975, val_f1=0.9002\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.2022, val loss=0.2478, train_acc=0.9219, val_acc=0.9100, val_f1=0.9078\n",
      "New best accuracy: 0.9100 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.1956, val loss=0.2425, train_acc=0.9271, val_acc=0.9155, val_f1=0.9165\n",
      "New best accuracy: 0.9155 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.1761, val loss=0.2471, train_acc=0.9339, val_acc=0.9115, val_f1=0.9138\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.2101, val loss=0.2410, train_acc=0.9237, val_acc=0.9085, val_f1=0.9059\n",
      "Epoch 23: train loss=0.1846, val loss=0.2523, train_acc=0.9306, val_acc=0.9120, val_f1=0.9144\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1602, val loss=0.2503, train_acc=0.9435, val_acc=0.9100, val_f1=0.9130\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1533, val loss=0.2303, train_acc=0.9413, val_acc=0.9180, val_f1=0.9179\n",
      "New best accuracy: 0.9180 at epoch 25, saving model.\n",
      "Epoch 26: train loss=0.1338, val loss=0.2376, train_acc=0.9533, val_acc=0.9215, val_f1=0.9199\n",
      "New best accuracy: 0.9215 at epoch 26, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 27: train loss=0.1344, val loss=0.2300, train_acc=0.9534, val_acc=0.9205, val_f1=0.9201\n",
      "Epoch 28: train loss=0.1254, val loss=0.2580, train_acc=0.9551, val_acc=0.9190, val_f1=0.9211\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 29: train loss=0.1190, val loss=0.2448, train_acc=0.9593, val_acc=0.9190, val_f1=0.9177\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 30: train loss=0.1074, val loss=0.2430, train_acc=0.9639, val_acc=0.9190, val_f1=0.9197\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 31: train loss=0.0946, val loss=0.2814, train_acc=0.9667, val_acc=0.9105, val_f1=0.9138\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 32: train loss=0.0856, val loss=0.2797, train_acc=0.9721, val_acc=0.9185, val_f1=0.9197\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 32.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=last: [0.7879599928855896, 0.7544599771499634, 0.7806199789047241]\n",
      "Test F1 Scores for method=last: [0.7875212752890808, 0.7513891338192936, 0.7799460814415736]\n",
      "Average Test Accuracy: 0.7743, Average Test F1: 0.7730\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing method: avg\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6270, val loss=0.5409, train_acc=0.6456, val_acc=0.7550, val_f1=0.7402\n",
      "New best accuracy: 0.7550 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5856, val loss=0.6711, train_acc=0.7081, val_acc=0.5900, val_f1=0.6165\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.6066, val loss=0.5349, train_acc=0.6749, val_acc=0.7375, val_f1=0.7750\n",
      "Epoch 04: train loss=0.5540, val loss=0.5559, train_acc=0.7292, val_acc=0.7650, val_f1=0.7476\n",
      "New best accuracy: 0.7650 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.5198, val loss=0.4825, train_acc=0.7578, val_acc=0.8045, val_f1=0.8117\n",
      "New best accuracy: 0.8045 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.4896, val loss=0.4718, train_acc=0.7955, val_acc=0.8035, val_f1=0.8243\n",
      "Epoch 07: train loss=0.4315, val loss=0.4170, train_acc=0.8313, val_acc=0.8460, val_f1=0.8535\n",
      "New best accuracy: 0.8460 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3920, val loss=0.4031, train_acc=0.8471, val_acc=0.8535, val_f1=0.8424\n",
      "New best accuracy: 0.8535 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3524, val loss=0.3880, train_acc=0.8729, val_acc=0.8510, val_f1=0.8352\n",
      "Epoch 10: train loss=0.3190, val loss=0.3399, train_acc=0.8798, val_acc=0.8740, val_f1=0.8794\n",
      "New best accuracy: 0.8740 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.3106, val loss=0.3843, train_acc=0.8864, val_acc=0.8650, val_f1=0.8737\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2985, val loss=0.3074, train_acc=0.8914, val_acc=0.8990, val_f1=0.9008\n",
      "New best accuracy: 0.8990 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2764, val loss=0.2938, train_acc=0.9011, val_acc=0.8985, val_f1=0.9015\n",
      "Epoch 14: train loss=0.2580, val loss=0.3023, train_acc=0.9001, val_acc=0.8975, val_f1=0.8981\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.2704, val loss=0.3544, train_acc=0.9016, val_acc=0.8805, val_f1=0.8896\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.2545, val loss=0.2979, train_acc=0.9095, val_acc=0.8975, val_f1=0.9009\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.2382, val loss=0.2812, train_acc=0.9155, val_acc=0.9025, val_f1=0.9047\n",
      "New best accuracy: 0.9025 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2311, val loss=0.3036, train_acc=0.9181, val_acc=0.8955, val_f1=0.8996\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.2248, val loss=0.3119, train_acc=0.9207, val_acc=0.8975, val_f1=0.9023\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 20: train loss=0.2191, val loss=0.2837, train_acc=0.9271, val_acc=0.9065, val_f1=0.9058\n",
      "New best accuracy: 0.9065 at epoch 20, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 21: train loss=0.2131, val loss=0.2944, train_acc=0.9254, val_acc=0.8990, val_f1=0.9028\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 22: train loss=0.1997, val loss=0.2958, train_acc=0.9296, val_acc=0.9070, val_f1=0.9072\n",
      "New best accuracy: 0.9070 at epoch 22, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 22.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6441, val loss=0.6004, train_acc=0.6270, val_acc=0.6815, val_f1=0.7295\n",
      "New best accuracy: 0.6815 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5385, val loss=0.4936, train_acc=0.7471, val_acc=0.7960, val_f1=0.7954\n",
      "New best accuracy: 0.7960 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4663, val loss=0.4121, train_acc=0.8076, val_acc=0.8380, val_f1=0.8333\n",
      "New best accuracy: 0.8380 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4094, val loss=0.4131, train_acc=0.8359, val_acc=0.8445, val_f1=0.8577\n",
      "New best accuracy: 0.8445 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3728, val loss=0.3738, train_acc=0.8585, val_acc=0.8660, val_f1=0.8742\n",
      "New best accuracy: 0.8660 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3457, val loss=0.3474, train_acc=0.8719, val_acc=0.8860, val_f1=0.8888\n",
      "New best accuracy: 0.8860 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3268, val loss=0.3499, train_acc=0.8792, val_acc=0.8775, val_f1=0.8842\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.3158, val loss=0.3259, train_acc=0.8896, val_acc=0.8865, val_f1=0.8887\n",
      "New best accuracy: 0.8865 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2850, val loss=0.3154, train_acc=0.8955, val_acc=0.8915, val_f1=0.8933\n",
      "New best accuracy: 0.8915 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2728, val loss=0.3094, train_acc=0.9024, val_acc=0.8985, val_f1=0.8974\n",
      "New best accuracy: 0.8985 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2619, val loss=0.3042, train_acc=0.9073, val_acc=0.8980, val_f1=0.8975\n",
      "Epoch 12: train loss=0.2600, val loss=0.2958, train_acc=0.9083, val_acc=0.9010, val_f1=0.9030\n",
      "New best accuracy: 0.9010 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2511, val loss=0.3022, train_acc=0.9127, val_acc=0.9000, val_f1=0.9000\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2320, val loss=0.3117, train_acc=0.9147, val_acc=0.8920, val_f1=0.8917\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.2258, val loss=0.2930, train_acc=0.9200, val_acc=0.8995, val_f1=0.8997\n",
      "Epoch 16: train loss=0.2090, val loss=0.3246, train_acc=0.9249, val_acc=0.8990, val_f1=0.9032\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2065, val loss=0.3138, train_acc=0.9287, val_acc=0.9020, val_f1=0.9019\n",
      "New best accuracy: 0.9020 at epoch 17, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.1993, val loss=0.3358, train_acc=0.9307, val_acc=0.8950, val_f1=0.8993\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.2026, val loss=0.2909, train_acc=0.9297, val_acc=0.9040, val_f1=0.9029\n",
      "New best accuracy: 0.9040 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.1814, val loss=0.3200, train_acc=0.9354, val_acc=0.8995, val_f1=0.9016\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 21: train loss=0.1826, val loss=0.3067, train_acc=0.9393, val_acc=0.9120, val_f1=0.9132\n",
      "New best accuracy: 0.9120 at epoch 21, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 22: train loss=0.1594, val loss=0.2890, train_acc=0.9439, val_acc=0.9080, val_f1=0.9076\n",
      "Epoch 23: train loss=0.1643, val loss=0.2976, train_acc=0.9424, val_acc=0.9105, val_f1=0.9090\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 24: train loss=0.1491, val loss=0.3124, train_acc=0.9491, val_acc=0.9070, val_f1=0.9106\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 25: train loss=0.1460, val loss=0.3062, train_acc=0.9500, val_acc=0.9115, val_f1=0.9129\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 26: train loss=0.1415, val loss=0.3746, train_acc=0.9537, val_acc=0.9030, val_f1=0.9066\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 27: train loss=0.1313, val loss=0.3646, train_acc=0.9524, val_acc=0.9005, val_f1=0.8977\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 27.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6313, val loss=0.5886, train_acc=0.6456, val_acc=0.7245, val_f1=0.7132\n",
      "New best accuracy: 0.7245 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5658, val loss=0.4994, train_acc=0.7311, val_acc=0.7820, val_f1=0.8034\n",
      "New best accuracy: 0.7820 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.5045, val loss=0.4751, train_acc=0.7808, val_acc=0.8130, val_f1=0.7961\n",
      "New best accuracy: 0.8130 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4889, val loss=0.4379, train_acc=0.7900, val_acc=0.8340, val_f1=0.8358\n",
      "New best accuracy: 0.8340 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4277, val loss=0.4079, train_acc=0.8386, val_acc=0.8625, val_f1=0.8628\n",
      "New best accuracy: 0.8625 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3833, val loss=0.3541, train_acc=0.8569, val_acc=0.8645, val_f1=0.8676\n",
      "New best accuracy: 0.8645 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3617, val loss=0.3465, train_acc=0.8696, val_acc=0.8710, val_f1=0.8776\n",
      "New best accuracy: 0.8710 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3238, val loss=0.3199, train_acc=0.8815, val_acc=0.8885, val_f1=0.8924\n",
      "New best accuracy: 0.8885 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3051, val loss=0.3137, train_acc=0.8871, val_acc=0.8805, val_f1=0.8717\n",
      "Epoch 10: train loss=0.2747, val loss=0.3228, train_acc=0.9012, val_acc=0.8945, val_f1=0.8974\n",
      "New best accuracy: 0.8945 at epoch 10, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2556, val loss=0.2786, train_acc=0.9079, val_acc=0.9010, val_f1=0.8963\n",
      "New best accuracy: 0.9010 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2538, val loss=0.2575, train_acc=0.9099, val_acc=0.9095, val_f1=0.9080\n",
      "New best accuracy: 0.9095 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.2394, val loss=0.2803, train_acc=0.9136, val_acc=0.8935, val_f1=0.8863\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.2185, val loss=0.2999, train_acc=0.9199, val_acc=0.8975, val_f1=0.8905\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.2093, val loss=0.2958, train_acc=0.9209, val_acc=0.9190, val_f1=0.9207\n",
      "New best accuracy: 0.9190 at epoch 15, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.2013, val loss=0.2469, train_acc=0.9293, val_acc=0.9210, val_f1=0.9206\n",
      "New best accuracy: 0.9210 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.1917, val loss=0.3027, train_acc=0.9323, val_acc=0.8935, val_f1=0.8854\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.1890, val loss=0.2434, train_acc=0.9319, val_acc=0.9215, val_f1=0.9205\n",
      "New best accuracy: 0.9215 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.1759, val loss=0.2571, train_acc=0.9371, val_acc=0.9175, val_f1=0.9196\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.1661, val loss=0.2709, train_acc=0.9434, val_acc=0.9080, val_f1=0.9120\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.1691, val loss=0.2479, train_acc=0.9415, val_acc=0.9185, val_f1=0.9169\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 22: train loss=0.1630, val loss=0.2484, train_acc=0.9454, val_acc=0.9240, val_f1=0.9232\n",
      "New best accuracy: 0.9240 at epoch 22, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 23: train loss=0.1398, val loss=0.2631, train_acc=0.9500, val_acc=0.9105, val_f1=0.9092\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 23.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=avg: [0.803879976272583, 0.7996600270271301, 0.8107600212097168]\n",
      "Test F1 Scores for method=avg: [0.8036017489283886, 0.7993687616881542, 0.8102594930797167]\n",
      "Average Test Accuracy: 0.8048, Average Test F1: 0.8044\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing method: max\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5257, val loss=0.3616, train_acc=0.7264, val_acc=0.8435, val_f1=0.8357\n",
      "New best accuracy: 0.8435 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3438, val loss=0.2981, train_acc=0.8520, val_acc=0.8795, val_f1=0.8841\n",
      "New best accuracy: 0.8795 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2860, val loss=0.2875, train_acc=0.8805, val_acc=0.8830, val_f1=0.8897\n",
      "New best accuracy: 0.8830 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2531, val loss=0.2356, train_acc=0.8954, val_acc=0.9085, val_f1=0.9081\n",
      "New best accuracy: 0.9085 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2295, val loss=0.2297, train_acc=0.9069, val_acc=0.9080, val_f1=0.9059\n",
      "Epoch 06: train loss=0.2078, val loss=0.2331, train_acc=0.9161, val_acc=0.9140, val_f1=0.9170\n",
      "New best accuracy: 0.9140 at epoch 6, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1932, val loss=0.2386, train_acc=0.9234, val_acc=0.9050, val_f1=0.9007\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1795, val loss=0.2136, train_acc=0.9309, val_acc=0.9165, val_f1=0.9170\n",
      "New best accuracy: 0.9165 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1723, val loss=0.2233, train_acc=0.9303, val_acc=0.9175, val_f1=0.9198\n",
      "New best accuracy: 0.9175 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1580, val loss=0.2223, train_acc=0.9405, val_acc=0.9165, val_f1=0.9158\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1475, val loss=0.2161, train_acc=0.9419, val_acc=0.9180, val_f1=0.9183\n",
      "New best accuracy: 0.9180 at epoch 11, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1334, val loss=0.2187, train_acc=0.9500, val_acc=0.9190, val_f1=0.9200\n",
      "New best accuracy: 0.9190 at epoch 12, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.1238, val loss=0.2185, train_acc=0.9515, val_acc=0.9185, val_f1=0.9196\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5752, val loss=0.4162, train_acc=0.7034, val_acc=0.8200, val_f1=0.8263\n",
      "New best accuracy: 0.8200 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3746, val loss=0.3493, train_acc=0.8331, val_acc=0.8490, val_f1=0.8333\n",
      "New best accuracy: 0.8490 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2984, val loss=0.2710, train_acc=0.8762, val_acc=0.8870, val_f1=0.8839\n",
      "New best accuracy: 0.8870 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2613, val loss=0.2452, train_acc=0.8924, val_acc=0.9050, val_f1=0.9043\n",
      "New best accuracy: 0.9050 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2374, val loss=0.2462, train_acc=0.9028, val_acc=0.8955, val_f1=0.8896\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2189, val loss=0.2238, train_acc=0.9126, val_acc=0.9140, val_f1=0.9142\n",
      "New best accuracy: 0.9140 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2042, val loss=0.2191, train_acc=0.9187, val_acc=0.9195, val_f1=0.9203\n",
      "New best accuracy: 0.9195 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1849, val loss=0.2134, train_acc=0.9269, val_acc=0.9205, val_f1=0.9201\n",
      "New best accuracy: 0.9205 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1779, val loss=0.2092, train_acc=0.9315, val_acc=0.9240, val_f1=0.9243\n",
      "New best accuracy: 0.9240 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1629, val loss=0.2210, train_acc=0.9330, val_acc=0.9155, val_f1=0.9131\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1543, val loss=0.2256, train_acc=0.9390, val_acc=0.9115, val_f1=0.9082\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1422, val loss=0.2148, train_acc=0.9449, val_acc=0.9240, val_f1=0.9233\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1333, val loss=0.2076, train_acc=0.9474, val_acc=0.9245, val_f1=0.9245\n",
      "New best accuracy: 0.9245 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1259, val loss=0.2097, train_acc=0.9530, val_acc=0.9260, val_f1=0.9255\n",
      "New best accuracy: 0.9260 at epoch 14, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1099, val loss=0.2354, train_acc=0.9584, val_acc=0.9255, val_f1=0.9265\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1075, val loss=0.2224, train_acc=0.9591, val_acc=0.9260, val_f1=0.9267\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.0955, val loss=0.2500, train_acc=0.9647, val_acc=0.9125, val_f1=0.9104\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.0935, val loss=0.2210, train_acc=0.9636, val_acc=0.9235, val_f1=0.9232\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5295, val loss=0.3432, train_acc=0.7256, val_acc=0.8595, val_f1=0.8630\n",
      "New best accuracy: 0.8595 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3413, val loss=0.2998, train_acc=0.8510, val_acc=0.8740, val_f1=0.8797\n",
      "New best accuracy: 0.8740 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2806, val loss=0.2537, train_acc=0.8852, val_acc=0.8980, val_f1=0.9004\n",
      "New best accuracy: 0.8980 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2519, val loss=0.2340, train_acc=0.8991, val_acc=0.9065, val_f1=0.9053\n",
      "New best accuracy: 0.9065 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2246, val loss=0.2328, train_acc=0.9070, val_acc=0.9130, val_f1=0.9108\n",
      "New best accuracy: 0.9130 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2040, val loss=0.2398, train_acc=0.9166, val_acc=0.9070, val_f1=0.9105\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1924, val loss=0.2470, train_acc=0.9235, val_acc=0.9040, val_f1=0.9094\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1800, val loss=0.2128, train_acc=0.9286, val_acc=0.9220, val_f1=0.9238\n",
      "New best accuracy: 0.9220 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1734, val loss=0.2270, train_acc=0.9335, val_acc=0.9110, val_f1=0.9152\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1551, val loss=0.2078, train_acc=0.9417, val_acc=0.9235, val_f1=0.9253\n",
      "New best accuracy: 0.9235 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1444, val loss=0.2069, train_acc=0.9466, val_acc=0.9215, val_f1=0.9199\n",
      "Epoch 12: train loss=0.1400, val loss=0.2060, train_acc=0.9439, val_acc=0.9240, val_f1=0.9250\n",
      "New best accuracy: 0.9240 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1239, val loss=0.2110, train_acc=0.9527, val_acc=0.9205, val_f1=0.9181\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1168, val loss=0.2054, train_acc=0.9555, val_acc=0.9250, val_f1=0.9234\n",
      "New best accuracy: 0.9250 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1077, val loss=0.2150, train_acc=0.9594, val_acc=0.9215, val_f1=0.9192\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.0982, val loss=0.2171, train_acc=0.9651, val_acc=0.9280, val_f1=0.9269\n",
      "New best accuracy: 0.9280 at epoch 16, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.0911, val loss=0.2437, train_acc=0.9666, val_acc=0.9120, val_f1=0.9081\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0792, val loss=0.2477, train_acc=0.9695, val_acc=0.9220, val_f1=0.9208\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0762, val loss=0.2370, train_acc=0.9725, val_acc=0.9215, val_f1=0.9233\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=max: [0.8101800084114075, 0.8043599724769592, 0.7993999719619751]\n",
      "Test F1 Scores for method=max: [0.8101733921357803, 0.8036672674397437, 0.7973176722480991]\n",
      "Average Test Accuracy: 0.8046, Average Test F1: 0.8037\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing method: 2dmaxpool\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5381, val loss=0.3577, train_acc=0.7319, val_acc=0.8470, val_f1=0.8499\n",
      "New best accuracy: 0.8470 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3363, val loss=0.2842, train_acc=0.8540, val_acc=0.8860, val_f1=0.8882\n",
      "New best accuracy: 0.8860 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2858, val loss=0.2904, train_acc=0.8801, val_acc=0.8760, val_f1=0.8657\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.2505, val loss=0.2601, train_acc=0.8986, val_acc=0.8955, val_f1=0.8897\n",
      "New best accuracy: 0.8955 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2339, val loss=0.2255, train_acc=0.9060, val_acc=0.9160, val_f1=0.9163\n",
      "New best accuracy: 0.9160 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2185, val loss=0.2199, train_acc=0.9111, val_acc=0.9165, val_f1=0.9172\n",
      "New best accuracy: 0.9165 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2008, val loss=0.2111, train_acc=0.9220, val_acc=0.9210, val_f1=0.9205\n",
      "New best accuracy: 0.9210 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1876, val loss=0.2409, train_acc=0.9266, val_acc=0.9145, val_f1=0.9109\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1818, val loss=0.2068, train_acc=0.9264, val_acc=0.9185, val_f1=0.9194\n",
      "Epoch 10: train loss=0.1749, val loss=0.2065, train_acc=0.9311, val_acc=0.9240, val_f1=0.9242\n",
      "New best accuracy: 0.9240 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1598, val loss=0.2133, train_acc=0.9375, val_acc=0.9195, val_f1=0.9183\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1476, val loss=0.2228, train_acc=0.9427, val_acc=0.9130, val_f1=0.9097\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1350, val loss=0.2066, train_acc=0.9459, val_acc=0.9230, val_f1=0.9241\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1273, val loss=0.2121, train_acc=0.9513, val_acc=0.9240, val_f1=0.9239\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1208, val loss=0.2173, train_acc=0.9534, val_acc=0.9245, val_f1=0.9246\n",
      "New best accuracy: 0.9245 at epoch 15, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5835, val loss=0.4029, train_acc=0.7075, val_acc=0.8285, val_f1=0.8190\n",
      "New best accuracy: 0.8285 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3568, val loss=0.3146, train_acc=0.8442, val_acc=0.8640, val_f1=0.8545\n",
      "New best accuracy: 0.8640 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3117, val loss=0.2632, train_acc=0.8665, val_acc=0.8960, val_f1=0.8969\n",
      "New best accuracy: 0.8960 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2682, val loss=0.2630, train_acc=0.8900, val_acc=0.8990, val_f1=0.9031\n",
      "New best accuracy: 0.8990 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2382, val loss=0.2420, train_acc=0.9022, val_acc=0.9050, val_f1=0.9049\n",
      "New best accuracy: 0.9050 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2246, val loss=0.2361, train_acc=0.9107, val_acc=0.9085, val_f1=0.9116\n",
      "New best accuracy: 0.9085 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2110, val loss=0.2215, train_acc=0.9163, val_acc=0.9160, val_f1=0.9175\n",
      "New best accuracy: 0.9160 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1935, val loss=0.2151, train_acc=0.9240, val_acc=0.9185, val_f1=0.9174\n",
      "New best accuracy: 0.9185 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1832, val loss=0.2200, train_acc=0.9253, val_acc=0.9180, val_f1=0.9186\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1695, val loss=0.2110, train_acc=0.9339, val_acc=0.9185, val_f1=0.9169\n",
      "Epoch 11: train loss=0.1591, val loss=0.2370, train_acc=0.9399, val_acc=0.9055, val_f1=0.9003\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1455, val loss=0.2117, train_acc=0.9433, val_acc=0.9245, val_f1=0.9255\n",
      "New best accuracy: 0.9245 at epoch 12, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1402, val loss=0.2151, train_acc=0.9446, val_acc=0.9205, val_f1=0.9225\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1278, val loss=0.2095, train_acc=0.9534, val_acc=0.9245, val_f1=0.9248\n",
      "Epoch 15: train loss=0.1193, val loss=0.2193, train_acc=0.9574, val_acc=0.9265, val_f1=0.9266\n",
      "New best accuracy: 0.9265 at epoch 15, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1141, val loss=0.2637, train_acc=0.9555, val_acc=0.9020, val_f1=0.8951\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.1008, val loss=0.2211, train_acc=0.9603, val_acc=0.9175, val_f1=0.9172\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0837, val loss=0.2468, train_acc=0.9694, val_acc=0.9230, val_f1=0.9216\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0784, val loss=0.2378, train_acc=0.9726, val_acc=0.9235, val_f1=0.9243\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5269, val loss=0.3655, train_acc=0.7229, val_acc=0.8505, val_f1=0.8456\n",
      "New best accuracy: 0.8505 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3315, val loss=0.3171, train_acc=0.8594, val_acc=0.8615, val_f1=0.8716\n",
      "New best accuracy: 0.8615 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2864, val loss=0.2670, train_acc=0.8815, val_acc=0.8910, val_f1=0.8901\n",
      "New best accuracy: 0.8910 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2511, val loss=0.2641, train_acc=0.8962, val_acc=0.8870, val_f1=0.8850\n",
      "Epoch 05: train loss=0.2296, val loss=0.2229, train_acc=0.9073, val_acc=0.9170, val_f1=0.9176\n",
      "New best accuracy: 0.9170 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2071, val loss=0.2323, train_acc=0.9194, val_acc=0.9020, val_f1=0.8977\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1991, val loss=0.2161, train_acc=0.9209, val_acc=0.9160, val_f1=0.9149\n",
      "Epoch 08: train loss=0.1729, val loss=0.2171, train_acc=0.9327, val_acc=0.9155, val_f1=0.9136\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1661, val loss=0.2261, train_acc=0.9356, val_acc=0.9145, val_f1=0.9180\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1662, val loss=0.2309, train_acc=0.9329, val_acc=0.9105, val_f1=0.9079\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1445, val loss=0.2272, train_acc=0.9431, val_acc=0.9180, val_f1=0.9162\n",
      "New best accuracy: 0.9180 at epoch 11, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.1401, val loss=0.2187, train_acc=0.9479, val_acc=0.9235, val_f1=0.9249\n",
      "New best accuracy: 0.9235 at epoch 12, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=2dmaxpool: [0.807479977607727, 0.8061599731445312, 0.8098000288009644]\n",
      "Test F1 Scores for method=2dmaxpool: [0.8073537473518646, 0.8057146018126538, 0.8097434524055397]\n",
      "Average Test Accuracy: 0.8078, Average Test F1: 0.8076\n",
      "\n",
      "\n",
      "========================================\n",
      "Best method: 2dmaxpool with average test accuracy: 0.8078\n"
     ]
    }
   ],
   "source": [
    "best_optimiser = \"adamw\"\n",
    "best_weight_decay = 0.0\n",
    "best_lr=0.0005\n",
    "methods = [\"last\", \"avg\", \"max\", \"2dmaxpool\"]\n",
    "\n",
    "best_avg_acc_method = 0.0\n",
    "best_method = None\n",
    "all_method_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n{'='*40}\\nTesting method: {method}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr,\n",
    "        optimiser=best_optimiser,\n",
    "        weight_decay=best_weight_decay,\n",
    "        method=method\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for method={method}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for method={method}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_method_results[method] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_method:\n",
    "        best_avg_acc_method = avg_acc\n",
    "        best_method = method\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest method: {best_method} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_method:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200b60a",
   "metadata": {},
   "source": [
    "# Best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50e895a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BEST CONFIGURATION SUMMARY\n",
      "Best Learning Rate: 0.0005\n",
      "Best Optimizer: adamw\n",
      "Best Weight Decay: 0.0\n",
      "Best Method: 2dmaxpool\n"
     ]
    }
   ],
   "source": [
    "print(\" BEST CONFIGURATION SUMMARY\")\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best Optimizer: {best_optimiser}\")\n",
    "print(f\"Best Weight Decay: {best_weight_decay}\")\n",
    "print(f\"Best Method: {best_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a54fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: INFO - Detected CPU: 13th Gen Intel(R) Core(TM) i9-13900H\n",
      "CarbonTracker: WARNING - No matching TDP found for CPU: 13th Gen Intel(R) Core(TM) i9-13900H. Using average TDP of 35.61W at 50% utilization as fallback.\n",
      "CarbonTracker: WARNING - No API keys provided. Skipping intensity provider initialization.\n",
      "CarbonTracker: The following components were found: GPU with device(s) NVIDIA GeForce RTX 3050 4GB Laptop GPU. CPU with device(s) 13th Gen Intel(R) Core(TM) i9-13900H.\n",
      "CarbonTracker: WARNING - No carbon intensity provider specified. Using average carbon intensity for SG: 498.74 gCO2eq/kWh.\n",
      "CarbonTracker: \n",
      "Predicted consumption for 50 epoch(s):\n",
      "\tTime:\t0:17:57\n",
      "\tEnergy:\t0.027264226070 kWh\n",
      "\tCO2eq:\t13.597809731263 g\n",
      "\tThis is equivalent to:\n",
      "\t0.127320315836 km travelled by car\n",
      "Epoch 01: train loss=0.5508, val loss=0.4060, train_acc=0.7240, val_acc=0.8225, val_f1=0.8133\n",
      "New best accuracy: 0.8225 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3601, val loss=0.3545, train_acc=0.8451, val_acc=0.8500, val_f1=0.8326\n",
      "New best accuracy: 0.8500 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2856, val loss=0.2691, train_acc=0.8848, val_acc=0.8955, val_f1=0.8992\n",
      "New best accuracy: 0.8955 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2595, val loss=0.2493, train_acc=0.8912, val_acc=0.9075, val_f1=0.9071\n",
      "New best accuracy: 0.9075 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2335, val loss=0.2356, train_acc=0.9065, val_acc=0.9075, val_f1=0.9064\n",
      "Epoch 06: train loss=0.2225, val loss=0.2359, train_acc=0.9125, val_acc=0.9025, val_f1=0.8979\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2028, val loss=0.2337, train_acc=0.9171, val_acc=0.9100, val_f1=0.9129\n",
      "New best accuracy: 0.9100 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1917, val loss=0.2284, train_acc=0.9250, val_acc=0.9065, val_f1=0.9019\n",
      "Epoch 09: train loss=0.1769, val loss=0.2310, train_acc=0.9274, val_acc=0.9075, val_f1=0.9026\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1630, val loss=0.2184, train_acc=0.9351, val_acc=0.9185, val_f1=0.9159\n",
      "New best accuracy: 0.9185 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1533, val loss=0.2535, train_acc=0.9420, val_acc=0.8950, val_f1=0.8876\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1407, val loss=0.2164, train_acc=0.9453, val_acc=0.9170, val_f1=0.9142\n",
      "Epoch 13: train loss=0.1315, val loss=0.2081, train_acc=0.9519, val_acc=0.9225, val_f1=0.9225\n",
      "New best accuracy: 0.9225 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1236, val loss=0.2320, train_acc=0.9521, val_acc=0.9190, val_f1=0.9208\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1092, val loss=0.2213, train_acc=0.9586, val_acc=0.9255, val_f1=0.9260\n",
      "New best accuracy: 0.9255 at epoch 15, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.0984, val loss=0.2263, train_acc=0.9639, val_acc=0.9225, val_f1=0.9214\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.0980, val loss=0.2203, train_acc=0.9616, val_acc=0.9235, val_f1=0.9235\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.0846, val loss=0.3324, train_acc=0.9695, val_acc=0.9015, val_f1=0.9075\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "CarbonTracker: Average carbon intensity during training was 498.74 gCO2eq/kWh. \n",
      "CarbonTracker: \n",
      "Actual consumption for 17 epoch(s):\n",
      "\tTime:\t0:05:32\n",
      "\tEnergy:\t0.008423979222 kWh\n",
      "\tCO2eq:\t4.201390728965 g\n",
      "\tThis is equivalent to:\n",
      "\t0.039338864503 km travelled by car\n",
      "CarbonTracker: Finished monitoring.\n",
      "Carbon tracking complete. Logs saved in './carbon_logs'.\n"
     ]
    }
   ],
   "source": [
    "model = biLSTM(\n",
    "    vocab_size=embedding_matrix.shape[0],\n",
    "    embed_dim=embedding_matrix.shape[1],\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,\n",
    "    method=\"2dmaxpool\"\n",
    ").to(device)\n",
    "\n",
    "# Load GloVe weights (frozen)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "model.embedding.weight.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0)\n",
    "\n",
    "tracker = CarbonTracker(epochs=50, log_dir=\"./carbon_logs\", components=\"all\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies, val_f1s = [], [], []\n",
    "\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "total_energy_usage = 0.0\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    tracker.epoch_start()\n",
    "\n",
    "    train_loss, train_acc, energy_used = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    tracker.epoch_end()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    total_energy_usage += energy_used\n",
    "        \n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: \"\n",
    "        f\"train loss={train_loss:.4f}, val loss={val_loss:.4f}, \"\n",
    "        f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in val loss for {epochs_no_improve} epochs.\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "tracker.stop()  \n",
    "\n",
    "print(\"Carbon tracking complete. Logs saved in './carbon_logs'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37f5cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest CarbonTracker file: carbon_logs\\8100.775825_2025-11-14T142428Z_carbontracker_output.log\n",
      "Energy consumed (kWh): 0.02726422607\n",
      "CO2eq (g): 13.597809731263\n",
      "Car travelled equivalent (km): 0.127320315836\n"
     ]
    }
   ],
   "source": [
    "latest_file = get_latest_carbontracker_output()\n",
    "results = parse_carbontracker_log(latest_file)\n",
    "\n",
    "print(\"Latest CarbonTracker file:\", latest_file)\n",
    "print(\"Energy consumed (kWh):\", results[\"energy_kWh\"])\n",
    "print(\"CO2eq (g):\", results[\"co2_grams\"])\n",
    "print(\"Car travelled equivalent (km):\", results[\"car_km\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "921fb4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"train_accuracies\": train_accuracies,\n",
    "    \"val_accuracies\": val_accuracies,\n",
    "    \"val_f1s\": val_f1s,\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(result[\"train_losses\"]) + 1)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(epochs, result[\"train_losses\"], linestyle='--', label='Train Loss', color='blue')\n",
    "axes[0].plot(epochs, result[\"val_losses\"], label='Val Loss', color='orange')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(epochs, result[\"train_accuracies\"], linestyle='--', label='Train Acc', color='blue')\n",
    "axes[1].plot(epochs, result[\"val_accuracies\"], label='Val Acc', color='orange')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(epochs, result[\"val_f1s\"], label='Val F1', color='green')\n",
    "axes[2].set_title('F1 Score')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b295a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32 samples...\n",
      "Accuracy: 0.7909799814224243\n",
      "F1 Score: 0.788443383276329\n",
      "Confusion Matrix:\n",
      " [[17037  7963]\n",
      " [ 2488 22512]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "results = test_worst(\n",
    "            model,\n",
    "            test_loader,\n",
    "            criterion,\n",
    "            return_preds=True,\n",
    "            text_dataset=test_texts,\n",
    "        )\n",
    "print(\"Accuracy:\", results[\"acc\"])\n",
    "print(\"F1 Score:\", results[\"f1\"])\n",
    "print(\"Confusion Matrix:\\n\", results[\"confusion_matrix\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(results[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b734da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg len of test: 1309.43102\n",
      "avg len of misclassified: 1341.1050617165822\n",
      "avg len of bad predictions: 1300.7300896982877\n",
      "avg len of close predictions: 1437.153895893954\n"
     ]
    }
   ],
   "source": [
    "df_wrong = results[\"misclassified\"]\n",
    "\n",
    "all_pos = df_wrong[df_wrong[\"true_label\"] == \"Positive\"]\n",
    "all_neg = df_wrong[df_wrong[\"true_label\"] == \"Negative\"]\n",
    "\n",
    "# Combine them\n",
    "all_combined = pd.concat([all_pos, all_neg])\n",
    "\n",
    "# Save to CSV\n",
    "all_combined.to_csv(\"all_misclassified_BiLSTM_best_examples.csv\", index=False)\n",
    "average_length_test = IMDB_df['review'].str.len().mean()\n",
    "print(f\"avg len of test: {average_length_test}\")\n",
    "\n",
    "average_length_misclassified = all_combined['text'].str.len().mean()\n",
    "print(f\"avg len of misclassified: {average_length_misclassified}\")\n",
    "\n",
    "bad_predictions = all_combined[all_combined[\"pred_conf\"]>0.75]\n",
    "bad_preds_len = bad_predictions['text'].str.len().mean()\n",
    "print(f\"avg len of bad predictions: {bad_preds_len}\")\n",
    "\n",
    "close_predictions = all_combined[all_combined[\"pred_conf\"]<=0.75]\n",
    "close_preds_len =close_predictions['text'].str.len().mean()\n",
    "print(f\"avg len of close predictions: {close_preds_len}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f9aa67",
   "metadata": {},
   "source": [
    "# CNN + LSTM hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acffccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size,\n",
    "                 num_layers=1, dropout=0.3, method=\"avg\", kernel_sizes=(3,4,5), cnn_channels=100):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=cnn_channels, kernel_size=k)\n",
    "            # nn.Conv1d(in_channels=embed_dim, out_channels=cnn_channels, kernel_size=k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels * len(kernel_sizes),\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: LongTensor (batch, seq_len)\n",
    "        returns: logits (batch, output_size)\n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        emb_t = emb.transpose(1, 2)\n",
    "\n",
    "        cnn_outs = []\n",
    "        for conv in self.convs:\n",
    "            c = F.relu(conv(emb_t))                     \n",
    "            c = c.transpose(1, 2)                       \n",
    "            cnn_outs.append(c)\n",
    "\n",
    "        min_len = min(out.size(1) for out in cnn_outs)\n",
    "        cnn_outs = [out[:, :min_len, :] for out in cnn_outs]\n",
    "        cnn_cat = torch.cat(cnn_outs, dim=2)            \n",
    "\n",
    "        lstm_out, _ = self.lstm(cnn_cat)                \n",
    "\n",
    "        if self.method == \"last\":\n",
    "            out = lstm_out[:, -1, :]                    \n",
    "        elif self.method == \"avg\":\n",
    "            out = lstm_out.mean(dim=1)                  \n",
    "        elif self.method == \"max\":\n",
    "            out, _ = lstm_out.max(dim=1)                \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)                              \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408eedc5",
   "metadata": {},
   "source": [
    "# Hyper param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b352a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tuning(lr, optimiser, weight_decay, method, kernel_sizes,cnn_channels):\n",
    "    NUM_RUNS = 3\n",
    "    all_results = []\n",
    "\n",
    "    for run in range(1, NUM_RUNS + 1):\n",
    "        print(f\"\\n{'='*25}\\nStarting training run {run}\\n{'='*25}\")\n",
    "\n",
    "        model = CNNLSTM(\n",
    "            vocab_size=len(vocab), \n",
    "            embed_dim=embedding_matrix.shape[1], \n",
    "            hidden_size=128, \n",
    "            output_size=2, \n",
    "            method=method,\n",
    "            kernel_sizes = kernel_sizes,\n",
    "            cnn_channels = cnn_channels).to(device)\n",
    "\n",
    "\n",
    "        # Load GloVe weights (frozen)\n",
    "        model.embedding.weight.data.copy_(embedding_matrix)\n",
    "        model.embedding.weight.requires_grad = False\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        if optimiser.lower() == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimiser.lower() == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimiser.lower() == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "        \n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_model_state = None\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies, val_f1s = [], [], []\n",
    "\n",
    "        patience = 5\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        total_energy_usage = 0.0\n",
    "\n",
    "        for epoch in range(50):\n",
    "            train_loss, train_acc, energy_used = train_epoch(model, train_loader, optimizer, criterion)\n",
    "            val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            total_energy_usage += energy_used\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d}: \"\n",
    "                f\"train loss={train_loss:.4f}, val loss={val_loss:.4f}, \"\n",
    "                f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"No improvement in val loss for {epochs_no_improve} epochs.\")\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval() \n",
    "\n",
    "        test_results = test_worst(\n",
    "            model,\n",
    "            test_loader,\n",
    "            criterion,\n",
    "            return_preds=True,\n",
    "            text_dataset=test_texts,\n",
    "        )\n",
    "\n",
    "        all_results.append({\n",
    "            \"run\": run,\n",
    "            \"num_epochs\": epoch+1,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"total_energy\": total_energy_usage,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\":train_accuracies,\n",
    "            \"val_accuracies\":val_accuracies,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_f1s\": val_f1s,\n",
    "            \"test_acc\": test_results[\"acc\"],\n",
    "            \"test_f1\": test_results[\"f1\"]\n",
    "        })\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6265682",
   "metadata": {},
   "source": [
    "# Tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd88d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing learning rate: 0.0001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6449, val loss=0.5730, train_acc=0.6186, val_acc=0.7230, val_f1=0.7583\n",
      "New best accuracy: 0.7230 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5257, val loss=0.4882, train_acc=0.7661, val_acc=0.8040, val_f1=0.8112\n",
      "New best accuracy: 0.8040 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4756, val loss=0.4576, train_acc=0.8037, val_acc=0.8300, val_f1=0.8335\n",
      "New best accuracy: 0.8300 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4872, val loss=0.4426, train_acc=0.8069, val_acc=0.8220, val_f1=0.8353\n",
      "Epoch 05: train loss=0.4682, val loss=0.4514, train_acc=0.8099, val_acc=0.8325, val_f1=0.8297\n",
      "New best accuracy: 0.8325 at epoch 5, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.4114, val loss=0.3937, train_acc=0.8414, val_acc=0.8445, val_f1=0.8378\n",
      "New best accuracy: 0.8445 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4065, val loss=0.4094, train_acc=0.8478, val_acc=0.8555, val_f1=0.8511\n",
      "New best accuracy: 0.8555 at epoch 7, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.3784, val loss=0.5075, train_acc=0.8551, val_acc=0.8390, val_f1=0.8456\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.3925, val loss=0.3768, train_acc=0.8548, val_acc=0.8590, val_f1=0.8564\n",
      "New best accuracy: 0.8590 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.3747, val loss=0.3902, train_acc=0.8622, val_acc=0.8495, val_f1=0.8488\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.3484, val loss=0.3470, train_acc=0.8688, val_acc=0.8640, val_f1=0.8689\n",
      "New best accuracy: 0.8640 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.3418, val loss=0.3754, train_acc=0.8731, val_acc=0.8570, val_f1=0.8449\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.3216, val loss=0.3394, train_acc=0.8779, val_acc=0.8760, val_f1=0.8712\n",
      "New best accuracy: 0.8760 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.3139, val loss=0.3279, train_acc=0.8866, val_acc=0.8765, val_f1=0.8729\n",
      "New best accuracy: 0.8765 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.3093, val loss=0.3517, train_acc=0.8835, val_acc=0.8645, val_f1=0.8531\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.3072, val loss=0.3279, train_acc=0.8919, val_acc=0.8800, val_f1=0.8806\n",
      "New best accuracy: 0.8800 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.3037, val loss=0.3129, train_acc=0.8889, val_acc=0.8860, val_f1=0.8814\n",
      "New best accuracy: 0.8860 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2780, val loss=0.3552, train_acc=0.8966, val_acc=0.8650, val_f1=0.8513\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.2616, val loss=0.3017, train_acc=0.9049, val_acc=0.8970, val_f1=0.8952\n",
      "New best accuracy: 0.8970 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.2538, val loss=0.2934, train_acc=0.9080, val_acc=0.8995, val_f1=0.8999\n",
      "New best accuracy: 0.8995 at epoch 20, saving model.\n",
      "Epoch 21: train loss=0.2492, val loss=0.2960, train_acc=0.9091, val_acc=0.8895, val_f1=0.8854\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 22: train loss=0.2426, val loss=0.2966, train_acc=0.9133, val_acc=0.8930, val_f1=0.8898\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 23: train loss=0.2260, val loss=0.2927, train_acc=0.9199, val_acc=0.8955, val_f1=0.8994\n",
      "Epoch 24: train loss=0.2256, val loss=0.2685, train_acc=0.9193, val_acc=0.9025, val_f1=0.9035\n",
      "New best accuracy: 0.9025 at epoch 24, saving model.\n",
      "Epoch 25: train loss=0.2169, val loss=0.2804, train_acc=0.9233, val_acc=0.8985, val_f1=0.8995\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 26: train loss=0.2272, val loss=0.2804, train_acc=0.9201, val_acc=0.9020, val_f1=0.8984\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 27: train loss=0.2016, val loss=0.2873, train_acc=0.9269, val_acc=0.9080, val_f1=0.9097\n",
      "New best accuracy: 0.9080 at epoch 27, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 28: train loss=0.2057, val loss=0.3053, train_acc=0.9281, val_acc=0.8890, val_f1=0.8953\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 29: train loss=0.2038, val loss=0.2815, train_acc=0.9316, val_acc=0.9040, val_f1=0.9010\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 29.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6425, val loss=0.5680, train_acc=0.6166, val_acc=0.7085, val_f1=0.7590\n",
      "New best accuracy: 0.7085 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5280, val loss=0.5002, train_acc=0.7571, val_acc=0.7910, val_f1=0.8112\n",
      "New best accuracy: 0.7910 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4913, val loss=0.4880, train_acc=0.7951, val_acc=0.7950, val_f1=0.8191\n",
      "New best accuracy: 0.7950 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4863, val loss=0.4692, train_acc=0.8014, val_acc=0.7930, val_f1=0.8142\n",
      "Epoch 05: train loss=0.4420, val loss=0.4846, train_acc=0.8227, val_acc=0.7910, val_f1=0.8167\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.4315, val loss=0.4221, train_acc=0.8256, val_acc=0.8305, val_f1=0.8409\n",
      "New best accuracy: 0.8305 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.4265, val loss=0.4012, train_acc=0.8304, val_acc=0.8485, val_f1=0.8545\n",
      "New best accuracy: 0.8485 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3816, val loss=0.3859, train_acc=0.8509, val_acc=0.8515, val_f1=0.8551\n",
      "New best accuracy: 0.8515 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.3714, val loss=0.4001, train_acc=0.8559, val_acc=0.8570, val_f1=0.8524\n",
      "New best accuracy: 0.8570 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.3688, val loss=0.3794, train_acc=0.8616, val_acc=0.8515, val_f1=0.8577\n",
      "Epoch 11: train loss=0.3567, val loss=0.3436, train_acc=0.8646, val_acc=0.8645, val_f1=0.8664\n",
      "New best accuracy: 0.8645 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.3410, val loss=0.3402, train_acc=0.8701, val_acc=0.8650, val_f1=0.8627\n",
      "New best accuracy: 0.8650 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.3350, val loss=0.3425, train_acc=0.8800, val_acc=0.8695, val_f1=0.8711\n",
      "New best accuracy: 0.8695 at epoch 13, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.3227, val loss=0.3330, train_acc=0.8791, val_acc=0.8710, val_f1=0.8764\n",
      "New best accuracy: 0.8710 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.2909, val loss=0.3077, train_acc=0.8875, val_acc=0.8860, val_f1=0.8881\n",
      "New best accuracy: 0.8860 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2940, val loss=0.3108, train_acc=0.8940, val_acc=0.8845, val_f1=0.8815\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2922, val loss=0.3133, train_acc=0.8961, val_acc=0.8780, val_f1=0.8845\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.2769, val loss=0.2899, train_acc=0.8950, val_acc=0.8895, val_f1=0.8926\n",
      "New best accuracy: 0.8895 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.2638, val loss=0.2996, train_acc=0.9046, val_acc=0.8945, val_f1=0.8966\n",
      "New best accuracy: 0.8945 at epoch 19, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 20: train loss=0.2559, val loss=0.3193, train_acc=0.9042, val_acc=0.8775, val_f1=0.8711\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 21: train loss=0.2524, val loss=0.2761, train_acc=0.9071, val_acc=0.8945, val_f1=0.8975\n",
      "Epoch 22: train loss=0.2536, val loss=0.3967, train_acc=0.9106, val_acc=0.8355, val_f1=0.8088\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.2698, val loss=0.3074, train_acc=0.9015, val_acc=0.8795, val_f1=0.8881\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.2363, val loss=0.3203, train_acc=0.9156, val_acc=0.8700, val_f1=0.8814\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 25: train loss=0.2245, val loss=0.2734, train_acc=0.9193, val_acc=0.9005, val_f1=0.9005\n",
      "New best accuracy: 0.9005 at epoch 25, saving model.\n",
      "Epoch 26: train loss=0.2186, val loss=0.2714, train_acc=0.9171, val_acc=0.9055, val_f1=0.9074\n",
      "New best accuracy: 0.9055 at epoch 26, saving model.\n",
      "Epoch 27: train loss=0.2059, val loss=0.2834, train_acc=0.9250, val_acc=0.9045, val_f1=0.9031\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 28: train loss=0.1927, val loss=0.2567, train_acc=0.9309, val_acc=0.9030, val_f1=0.9052\n",
      "Epoch 29: train loss=0.2037, val loss=0.3932, train_acc=0.9280, val_acc=0.8615, val_f1=0.8745\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 30: train loss=0.1961, val loss=0.2732, train_acc=0.9319, val_acc=0.9015, val_f1=0.9029\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 31: train loss=0.1909, val loss=0.3159, train_acc=0.9299, val_acc=0.9030, val_f1=0.9005\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 32: train loss=0.1848, val loss=0.2703, train_acc=0.9336, val_acc=0.9035, val_f1=0.9005\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 33: train loss=0.1722, val loss=0.2723, train_acc=0.9396, val_acc=0.9030, val_f1=0.9073\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 33.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6468, val loss=0.5924, train_acc=0.6114, val_acc=0.7100, val_f1=0.7259\n",
      "New best accuracy: 0.7100 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5248, val loss=0.4807, train_acc=0.7648, val_acc=0.7955, val_f1=0.8109\n",
      "New best accuracy: 0.7955 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4808, val loss=0.4711, train_acc=0.8000, val_acc=0.8220, val_f1=0.8218\n",
      "New best accuracy: 0.8220 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4502, val loss=0.4524, train_acc=0.8201, val_acc=0.8335, val_f1=0.8233\n",
      "New best accuracy: 0.8335 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.4153, val loss=0.3959, train_acc=0.8454, val_acc=0.8590, val_f1=0.8573\n",
      "New best accuracy: 0.8590 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3868, val loss=0.3704, train_acc=0.8480, val_acc=0.8595, val_f1=0.8631\n",
      "New best accuracy: 0.8595 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3828, val loss=0.3497, train_acc=0.8564, val_acc=0.8665, val_f1=0.8656\n",
      "New best accuracy: 0.8665 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.3729, val loss=0.3951, train_acc=0.8626, val_acc=0.8505, val_f1=0.8445\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.3541, val loss=0.3938, train_acc=0.8642, val_acc=0.8360, val_f1=0.8415\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.3367, val loss=0.3423, train_acc=0.8729, val_acc=0.8770, val_f1=0.8712\n",
      "New best accuracy: 0.8770 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.4442, val loss=0.5233, train_acc=0.8356, val_acc=0.8575, val_f1=0.8608\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.4090, val loss=0.3933, train_acc=0.8595, val_acc=0.8390, val_f1=0.8535\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.3425, val loss=0.3368, train_acc=0.8735, val_acc=0.8850, val_f1=0.8823\n",
      "New best accuracy: 0.8850 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.3107, val loss=0.3336, train_acc=0.8861, val_acc=0.8760, val_f1=0.8829\n",
      "Epoch 15: train loss=0.2922, val loss=0.3034, train_acc=0.8938, val_acc=0.8880, val_f1=0.8870\n",
      "New best accuracy: 0.8880 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.2850, val loss=0.3004, train_acc=0.8932, val_acc=0.8920, val_f1=0.8905\n",
      "New best accuracy: 0.8920 at epoch 16, saving model.\n",
      "Epoch 17: train loss=0.2797, val loss=0.2887, train_acc=0.8988, val_acc=0.8950, val_f1=0.8939\n",
      "New best accuracy: 0.8950 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.2573, val loss=0.3021, train_acc=0.9073, val_acc=0.8815, val_f1=0.8768\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 19: train loss=0.2517, val loss=0.3032, train_acc=0.9096, val_acc=0.8940, val_f1=0.8894\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 20: train loss=0.2408, val loss=0.2848, train_acc=0.9147, val_acc=0.8940, val_f1=0.8978\n",
      "Epoch 21: train loss=0.2373, val loss=0.2726, train_acc=0.9124, val_acc=0.9005, val_f1=0.8991\n",
      "New best accuracy: 0.9005 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.2294, val loss=0.2878, train_acc=0.9176, val_acc=0.8960, val_f1=0.8920\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 23: train loss=0.2151, val loss=0.3104, train_acc=0.9233, val_acc=0.8890, val_f1=0.8832\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 24: train loss=0.2171, val loss=0.2693, train_acc=0.9216, val_acc=0.9030, val_f1=0.9020\n",
      "New best accuracy: 0.9030 at epoch 24, saving model.\n",
      "Epoch 25: train loss=0.2120, val loss=0.2719, train_acc=0.9265, val_acc=0.9090, val_f1=0.9088\n",
      "New best accuracy: 0.9090 at epoch 25, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 26: train loss=0.2030, val loss=0.2820, train_acc=0.9281, val_acc=0.9045, val_f1=0.9016\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 27: train loss=0.2046, val loss=0.2620, train_acc=0.9287, val_acc=0.9090, val_f1=0.9093\n",
      "Epoch 28: train loss=0.1939, val loss=0.2699, train_acc=0.9325, val_acc=0.9140, val_f1=0.9141\n",
      "New best accuracy: 0.9140 at epoch 28, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 29: train loss=0.2068, val loss=0.2751, train_acc=0.9265, val_acc=0.9090, val_f1=0.9068\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 30: train loss=0.1706, val loss=0.3000, train_acc=0.9417, val_acc=0.8965, val_f1=0.9020\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 31: train loss=0.1771, val loss=0.3175, train_acc=0.9395, val_acc=0.9065, val_f1=0.9043\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 32: train loss=0.1665, val loss=0.2689, train_acc=0.9437, val_acc=0.9050, val_f1=0.9074\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 32.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.0001: [0.79694002866745, 0.7932999730110168, 0.8024399876594543]\n",
      "Test F1 Scores for lr=0.0001: [0.7968542945332875, 0.7931449281166497, 0.8024392410505885]\n",
      "Average Test Accuracy: 0.7976, Average Test F1: 0.7975\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing learning rate: 0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5825, val loss=0.4603, train_acc=0.6975, val_acc=0.8230, val_f1=0.8314\n",
      "New best accuracy: 0.8230 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4645, val loss=0.4097, train_acc=0.8139, val_acc=0.8590, val_f1=0.8566\n",
      "New best accuracy: 0.8590 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3999, val loss=0.4248, train_acc=0.8519, val_acc=0.8230, val_f1=0.8439\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3217, val loss=0.2875, train_acc=0.8781, val_acc=0.8990, val_f1=0.9001\n",
      "New best accuracy: 0.8990 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2667, val loss=0.2933, train_acc=0.9012, val_acc=0.8845, val_f1=0.8922\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2380, val loss=0.2626, train_acc=0.9120, val_acc=0.9050, val_f1=0.9085\n",
      "New best accuracy: 0.9050 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2059, val loss=0.2435, train_acc=0.9265, val_acc=0.9180, val_f1=0.9191\n",
      "New best accuracy: 0.9180 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1842, val loss=0.2477, train_acc=0.9325, val_acc=0.9135, val_f1=0.9107\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1659, val loss=0.4179, train_acc=0.9410, val_acc=0.8690, val_f1=0.8814\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1510, val loss=0.2604, train_acc=0.9443, val_acc=0.9175, val_f1=0.9195\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1218, val loss=0.2650, train_acc=0.9570, val_acc=0.9160, val_f1=0.9162\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.1066, val loss=0.2838, train_acc=0.9624, val_acc=0.9190, val_f1=0.9208\n",
      "New best accuracy: 0.9190 at epoch 12, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5873, val loss=0.5132, train_acc=0.6894, val_acc=0.7760, val_f1=0.8089\n",
      "New best accuracy: 0.7760 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4404, val loss=0.4986, train_acc=0.8284, val_acc=0.8065, val_f1=0.7767\n",
      "New best accuracy: 0.8065 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3830, val loss=0.3683, train_acc=0.8562, val_acc=0.8670, val_f1=0.8557\n",
      "New best accuracy: 0.8670 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3118, val loss=0.3278, train_acc=0.8811, val_acc=0.8750, val_f1=0.8632\n",
      "New best accuracy: 0.8750 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3009, val loss=0.3122, train_acc=0.8848, val_acc=0.8865, val_f1=0.8792\n",
      "New best accuracy: 0.8865 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2610, val loss=0.2619, train_acc=0.9069, val_acc=0.9110, val_f1=0.9127\n",
      "New best accuracy: 0.9110 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2233, val loss=0.2502, train_acc=0.9210, val_acc=0.9100, val_f1=0.9075\n",
      "Epoch 08: train loss=0.1944, val loss=0.2584, train_acc=0.9296, val_acc=0.9115, val_f1=0.9069\n",
      "New best accuracy: 0.9115 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1848, val loss=0.2751, train_acc=0.9343, val_acc=0.9175, val_f1=0.9155\n",
      "New best accuracy: 0.9175 at epoch 9, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1738, val loss=0.2555, train_acc=0.9405, val_acc=0.9185, val_f1=0.9178\n",
      "New best accuracy: 0.9185 at epoch 10, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1399, val loss=0.2473, train_acc=0.9546, val_acc=0.9115, val_f1=0.9109\n",
      "Epoch 12: train loss=0.1446, val loss=0.2993, train_acc=0.9509, val_acc=0.9155, val_f1=0.9170\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1315, val loss=0.2615, train_acc=0.9535, val_acc=0.9110, val_f1=0.9103\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1113, val loss=0.3427, train_acc=0.9625, val_acc=0.9035, val_f1=0.8992\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.1091, val loss=0.3022, train_acc=0.9674, val_acc=0.9180, val_f1=0.9155\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 16: train loss=0.0915, val loss=0.3631, train_acc=0.9677, val_acc=0.9105, val_f1=0.9139\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 16.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5774, val loss=0.4680, train_acc=0.7036, val_acc=0.8195, val_f1=0.8300\n",
      "New best accuracy: 0.8195 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4221, val loss=0.4251, train_acc=0.8279, val_acc=0.8245, val_f1=0.7968\n",
      "New best accuracy: 0.8245 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3612, val loss=0.3893, train_acc=0.8611, val_acc=0.8360, val_f1=0.8113\n",
      "New best accuracy: 0.8360 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3470, val loss=0.2971, train_acc=0.8634, val_acc=0.8970, val_f1=0.8940\n",
      "New best accuracy: 0.8970 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3031, val loss=0.3189, train_acc=0.8848, val_acc=0.8760, val_f1=0.8643\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2675, val loss=0.2690, train_acc=0.9034, val_acc=0.9045, val_f1=0.8999\n",
      "New best accuracy: 0.9045 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2566, val loss=0.2640, train_acc=0.9069, val_acc=0.9105, val_f1=0.9076\n",
      "New best accuracy: 0.9105 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2102, val loss=0.2441, train_acc=0.9259, val_acc=0.9135, val_f1=0.9128\n",
      "New best accuracy: 0.9135 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1940, val loss=0.2363, train_acc=0.9276, val_acc=0.9180, val_f1=0.9168\n",
      "New best accuracy: 0.9180 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1864, val loss=0.2625, train_acc=0.9323, val_acc=0.9165, val_f1=0.9158\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1600, val loss=0.2593, train_acc=0.9396, val_acc=0.8960, val_f1=0.8895\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1560, val loss=0.4361, train_acc=0.9459, val_acc=0.8375, val_f1=0.8094\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1291, val loss=0.3110, train_acc=0.9541, val_acc=0.8785, val_f1=0.8663\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1298, val loss=0.2814, train_acc=0.9557, val_acc=0.9085, val_f1=0.9093\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.0005: [0.8080599904060364, 0.7950599789619446, 0.7905600070953369]\n",
      "Test F1 Scores for lr=0.0005: [0.8078296753335248, 0.7926040603379186, 0.7888786767348991]\n",
      "Average Test Accuracy: 0.7979, Average Test F1: 0.7964\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing learning rate: 0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5604, val loss=0.4777, train_acc=0.7170, val_acc=0.8030, val_f1=0.7712\n",
      "New best accuracy: 0.8030 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3884, val loss=0.3176, train_acc=0.8459, val_acc=0.8835, val_f1=0.8836\n",
      "New best accuracy: 0.8835 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3380, val loss=0.3130, train_acc=0.8702, val_acc=0.8845, val_f1=0.8852\n",
      "New best accuracy: 0.8845 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2623, val loss=0.2533, train_acc=0.8995, val_acc=0.9130, val_f1=0.9127\n",
      "New best accuracy: 0.9130 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2328, val loss=0.2623, train_acc=0.9117, val_acc=0.9085, val_f1=0.9038\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2004, val loss=0.2440, train_acc=0.9251, val_acc=0.9165, val_f1=0.9156\n",
      "New best accuracy: 0.9165 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.1647, val loss=0.2221, train_acc=0.9389, val_acc=0.9260, val_f1=0.9248\n",
      "New best accuracy: 0.9260 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1932, val loss=0.2693, train_acc=0.9291, val_acc=0.9155, val_f1=0.9165\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1313, val loss=0.2823, train_acc=0.9511, val_acc=0.9000, val_f1=0.8934\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1218, val loss=0.2483, train_acc=0.9585, val_acc=0.9090, val_f1=0.9057\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.0935, val loss=0.3010, train_acc=0.9674, val_acc=0.9160, val_f1=0.9157\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.0787, val loss=0.4225, train_acc=0.9736, val_acc=0.9120, val_f1=0.9143\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5650, val loss=0.4260, train_acc=0.7139, val_acc=0.8300, val_f1=0.8375\n",
      "New best accuracy: 0.8300 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4199, val loss=0.3708, train_acc=0.8356, val_acc=0.8695, val_f1=0.8634\n",
      "New best accuracy: 0.8695 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3331, val loss=0.3079, train_acc=0.8742, val_acc=0.8950, val_f1=0.8909\n",
      "New best accuracy: 0.8950 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2628, val loss=0.3583, train_acc=0.8994, val_acc=0.8710, val_f1=0.8826\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2314, val loss=0.2401, train_acc=0.9176, val_acc=0.9160, val_f1=0.9142\n",
      "New best accuracy: 0.9160 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2045, val loss=0.2561, train_acc=0.9246, val_acc=0.9145, val_f1=0.9148\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1679, val loss=0.2420, train_acc=0.9361, val_acc=0.9115, val_f1=0.9069\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1425, val loss=0.3350, train_acc=0.9489, val_acc=0.9095, val_f1=0.9136\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 09: train loss=0.1133, val loss=0.2675, train_acc=0.9587, val_acc=0.9210, val_f1=0.9219\n",
      "New best accuracy: 0.9210 at epoch 9, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 10: train loss=0.0941, val loss=0.3372, train_acc=0.9669, val_acc=0.9090, val_f1=0.9131\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 10.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5886, val loss=0.4984, train_acc=0.7034, val_acc=0.7950, val_f1=0.8102\n",
      "New best accuracy: 0.7950 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4441, val loss=0.4015, train_acc=0.8177, val_acc=0.8420, val_f1=0.8523\n",
      "New best accuracy: 0.8420 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3515, val loss=0.2924, train_acc=0.8624, val_acc=0.8905, val_f1=0.8906\n",
      "New best accuracy: 0.8905 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2830, val loss=0.3046, train_acc=0.8900, val_acc=0.8690, val_f1=0.8549\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2295, val loss=0.2456, train_acc=0.9141, val_acc=0.9130, val_f1=0.9124\n",
      "New best accuracy: 0.9130 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2152, val loss=0.3236, train_acc=0.9187, val_acc=0.8570, val_f1=0.8379\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1806, val loss=0.3577, train_acc=0.9316, val_acc=0.8270, val_f1=0.7950\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1657, val loss=0.2267, train_acc=0.9361, val_acc=0.9185, val_f1=0.9159\n",
      "New best accuracy: 0.9185 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1420, val loss=0.2402, train_acc=0.9486, val_acc=0.9040, val_f1=0.8993\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1218, val loss=0.2688, train_acc=0.9569, val_acc=0.9015, val_f1=0.8958\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1306, val loss=0.2443, train_acc=0.9499, val_acc=0.9145, val_f1=0.9153\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.0982, val loss=0.3069, train_acc=0.9630, val_acc=0.9115, val_f1=0.9137\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.0680, val loss=0.3022, train_acc=0.9774, val_acc=0.9155, val_f1=0.9171\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for lr=0.001: [0.7983199954032898, 0.809440016746521, 0.7790200114250183]\n",
      "Test F1 Scores for lr=0.001: [0.7968003121250378, 0.8094392975170265, 0.7745128951091673]\n",
      "Average Test Accuracy: 0.7956, Average Test F1: 0.7936\n",
      "\n",
      "\n",
      "========================================\n",
      "Best learning rate: 0.0005 with average test accuracy: 0.7979\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.0005, 0.001]\n",
    "\n",
    "best_avg_acc = 0.0\n",
    "best_lr = None\n",
    "all_lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n{'='*40}\\nTesting learning rate: {lr}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=lr, optimiser=\"adam\", weight_decay=0.0,\n",
    "        cnn_channels=100, kernel_sizes=(3,4,5), method=\"avg\"\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for lr={lr}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for lr={lr}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_lr_results[lr] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    # Update best configuration\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest learning rate: {best_lr} with average test accuracy: {best_avg_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c71c2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "126a5000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing optimiser: adam with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5904, val loss=0.5518, train_acc=0.6869, val_acc=0.6960, val_f1=0.7623\n",
      "New best accuracy: 0.6960 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4530, val loss=0.3970, train_acc=0.8096, val_acc=0.8440, val_f1=0.8537\n",
      "New best accuracy: 0.8440 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3810, val loss=0.3847, train_acc=0.8485, val_acc=0.8580, val_f1=0.8457\n",
      "New best accuracy: 0.8580 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3274, val loss=0.2865, train_acc=0.8701, val_acc=0.8920, val_f1=0.8881\n",
      "New best accuracy: 0.8920 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3006, val loss=0.2827, train_acc=0.8850, val_acc=0.9060, val_f1=0.9058\n",
      "New best accuracy: 0.9060 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2490, val loss=0.2620, train_acc=0.9070, val_acc=0.9130, val_f1=0.9132\n",
      "New best accuracy: 0.9130 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2216, val loss=0.2619, train_acc=0.9193, val_acc=0.9025, val_f1=0.8966\n",
      "Epoch 08: train loss=0.2057, val loss=0.2505, train_acc=0.9201, val_acc=0.9055, val_f1=0.9002\n",
      "Epoch 09: train loss=0.1808, val loss=0.2582, train_acc=0.9346, val_acc=0.8965, val_f1=0.8891\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1682, val loss=0.2899, train_acc=0.9381, val_acc=0.9205, val_f1=0.9188\n",
      "New best accuracy: 0.9205 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1549, val loss=0.2851, train_acc=0.9433, val_acc=0.8860, val_f1=0.8757\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1297, val loss=0.2498, train_acc=0.9574, val_acc=0.9210, val_f1=0.9200\n",
      "New best accuracy: 0.9210 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1168, val loss=0.2790, train_acc=0.9591, val_acc=0.9135, val_f1=0.9092\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1077, val loss=0.2712, train_acc=0.9623, val_acc=0.9155, val_f1=0.9135\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1183, val loss=0.2761, train_acc=0.9606, val_acc=0.9185, val_f1=0.9184\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.0919, val loss=0.3045, train_acc=0.9706, val_acc=0.9235, val_f1=0.9214\n",
      "New best accuracy: 0.9235 at epoch 16, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.0846, val loss=0.3402, train_acc=0.9738, val_acc=0.9300, val_f1=0.9286\n",
      "New best accuracy: 0.9300 at epoch 17, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5837, val loss=0.4803, train_acc=0.6944, val_acc=0.7935, val_f1=0.8191\n",
      "New best accuracy: 0.7935 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5331, val loss=0.4774, train_acc=0.7788, val_acc=0.7995, val_f1=0.7715\n",
      "New best accuracy: 0.7995 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4239, val loss=0.3793, train_acc=0.8296, val_acc=0.8490, val_f1=0.8348\n",
      "New best accuracy: 0.8490 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3748, val loss=0.3675, train_acc=0.8460, val_acc=0.8430, val_f1=0.8230\n",
      "Epoch 05: train loss=0.3307, val loss=0.3256, train_acc=0.8704, val_acc=0.8680, val_f1=0.8545\n",
      "New best accuracy: 0.8680 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2766, val loss=0.3760, train_acc=0.8879, val_acc=0.8350, val_f1=0.8059\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2552, val loss=0.2416, train_acc=0.8954, val_acc=0.9075, val_f1=0.9085\n",
      "New best accuracy: 0.9075 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2220, val loss=0.2429, train_acc=0.9137, val_acc=0.9120, val_f1=0.9111\n",
      "New best accuracy: 0.9120 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2151, val loss=0.2551, train_acc=0.9181, val_acc=0.8975, val_f1=0.9033\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.2176, val loss=0.2355, train_acc=0.9154, val_acc=0.9140, val_f1=0.9122\n",
      "New best accuracy: 0.9140 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1896, val loss=0.3009, train_acc=0.9294, val_acc=0.8825, val_f1=0.8908\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.2420, val loss=0.2365, train_acc=0.9105, val_acc=0.9065, val_f1=0.9034\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1676, val loss=0.2322, train_acc=0.9389, val_acc=0.9110, val_f1=0.9080\n",
      "Epoch 14: train loss=0.1484, val loss=0.2358, train_acc=0.9483, val_acc=0.9220, val_f1=0.9225\n",
      "New best accuracy: 0.9220 at epoch 14, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1484, val loss=0.2214, train_acc=0.9463, val_acc=0.9165, val_f1=0.9145\n",
      "Epoch 16: train loss=0.1851, val loss=0.4261, train_acc=0.9251, val_acc=0.8450, val_f1=0.8587\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.2036, val loss=0.2745, train_acc=0.9243, val_acc=0.9110, val_f1=0.9138\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.1427, val loss=0.2400, train_acc=0.9507, val_acc=0.9160, val_f1=0.9163\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.1150, val loss=0.2544, train_acc=0.9619, val_acc=0.9160, val_f1=0.9145\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 20: train loss=0.0994, val loss=0.2477, train_acc=0.9675, val_acc=0.9190, val_f1=0.9179\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 20.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5813, val loss=0.4713, train_acc=0.7025, val_acc=0.8055, val_f1=0.8200\n",
      "New best accuracy: 0.8055 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5898, val loss=0.5201, train_acc=0.7817, val_acc=0.7310, val_f1=0.7831\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4463, val loss=0.4453, train_acc=0.8199, val_acc=0.8310, val_f1=0.8189\n",
      "New best accuracy: 0.8310 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3848, val loss=0.3449, train_acc=0.8377, val_acc=0.8615, val_f1=0.8639\n",
      "New best accuracy: 0.8615 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3400, val loss=0.3328, train_acc=0.8635, val_acc=0.8785, val_f1=0.8738\n",
      "New best accuracy: 0.8785 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3310, val loss=0.2969, train_acc=0.8678, val_acc=0.8965, val_f1=0.8971\n",
      "New best accuracy: 0.8965 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3350, val loss=0.2783, train_acc=0.8629, val_acc=0.8955, val_f1=0.8990\n",
      "Epoch 08: train loss=0.2846, val loss=0.2686, train_acc=0.8884, val_acc=0.9035, val_f1=0.9051\n",
      "New best accuracy: 0.9035 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2624, val loss=0.2732, train_acc=0.8964, val_acc=0.9050, val_f1=0.9075\n",
      "New best accuracy: 0.9050 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2474, val loss=0.2913, train_acc=0.9015, val_acc=0.8940, val_f1=0.8888\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.5495, val loss=0.4213, train_acc=0.7817, val_acc=0.8450, val_f1=0.8609\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.3156, val loss=0.3459, train_acc=0.8952, val_acc=0.8680, val_f1=0.8541\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.2339, val loss=0.2661, train_acc=0.9145, val_acc=0.9115, val_f1=0.9109\n",
      "New best accuracy: 0.9115 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.2164, val loss=0.2618, train_acc=0.9245, val_acc=0.9045, val_f1=0.9008\n",
      "Epoch 15: train loss=0.1962, val loss=0.2656, train_acc=0.9314, val_acc=0.9155, val_f1=0.9143\n",
      "New best accuracy: 0.9155 at epoch 15, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1798, val loss=0.2784, train_acc=0.9373, val_acc=0.9140, val_f1=0.9121\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.1682, val loss=0.2926, train_acc=0.9443, val_acc=0.9115, val_f1=0.9114\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.1452, val loss=0.3046, train_acc=0.9540, val_acc=0.9085, val_f1=0.9056\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.1524, val loss=0.3459, train_acc=0.9469, val_acc=0.9170, val_f1=0.9169\n",
      "New best accuracy: 0.9170 at epoch 19, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=adam: [0.7942399978637695, 0.8063399791717529, 0.8084200024604797]\n",
      "Test F1 Scores for optimiser=adam: [0.7931327667999079, 0.8061648498605275, 0.8081943425620809]\n",
      "Average Test Accuracy: 0.8030, Average Test F1: 0.8025\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing optimiser: adamw with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5864, val loss=0.4608, train_acc=0.6941, val_acc=0.8275, val_f1=0.8301\n",
      "New best accuracy: 0.8275 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4550, val loss=0.3938, train_acc=0.8206, val_acc=0.8470, val_f1=0.8462\n",
      "New best accuracy: 0.8470 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3879, val loss=0.4304, train_acc=0.8481, val_acc=0.8100, val_f1=0.7727\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3295, val loss=0.3989, train_acc=0.8734, val_acc=0.8720, val_f1=0.8779\n",
      "New best accuracy: 0.8720 at epoch 4, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.2905, val loss=0.2854, train_acc=0.8896, val_acc=0.8985, val_f1=0.8970\n",
      "New best accuracy: 0.8985 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2647, val loss=0.2597, train_acc=0.8966, val_acc=0.9105, val_f1=0.9096\n",
      "New best accuracy: 0.9105 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2212, val loss=0.2552, train_acc=0.9170, val_acc=0.9100, val_f1=0.9060\n",
      "Epoch 08: train loss=0.2043, val loss=0.2622, train_acc=0.9230, val_acc=0.9110, val_f1=0.9131\n",
      "New best accuracy: 0.9110 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1938, val loss=0.2379, train_acc=0.9271, val_acc=0.9230, val_f1=0.9217\n",
      "New best accuracy: 0.9230 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1563, val loss=0.2550, train_acc=0.9436, val_acc=0.9085, val_f1=0.9039\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1404, val loss=0.3204, train_acc=0.9479, val_acc=0.8945, val_f1=0.9009\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1406, val loss=0.2462, train_acc=0.9494, val_acc=0.9015, val_f1=0.8958\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1112, val loss=0.2535, train_acc=0.9635, val_acc=0.9185, val_f1=0.9198\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1139, val loss=0.2862, train_acc=0.9617, val_acc=0.9190, val_f1=0.9209\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6085, val loss=0.5604, train_acc=0.6819, val_acc=0.7775, val_f1=0.8041\n",
      "New best accuracy: 0.7775 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4677, val loss=0.3842, train_acc=0.8194, val_acc=0.8490, val_f1=0.8500\n",
      "New best accuracy: 0.8490 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3811, val loss=0.3342, train_acc=0.8578, val_acc=0.8785, val_f1=0.8762\n",
      "New best accuracy: 0.8785 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3304, val loss=0.3352, train_acc=0.8769, val_acc=0.8860, val_f1=0.8796\n",
      "New best accuracy: 0.8860 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2900, val loss=0.2908, train_acc=0.8899, val_acc=0.8965, val_f1=0.8897\n",
      "New best accuracy: 0.8965 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2549, val loss=0.2769, train_acc=0.9055, val_acc=0.8990, val_f1=0.8930\n",
      "New best accuracy: 0.8990 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2195, val loss=0.2675, train_acc=0.9195, val_acc=0.8905, val_f1=0.8818\n",
      "Epoch 08: train loss=0.2098, val loss=0.2608, train_acc=0.9245, val_acc=0.8980, val_f1=0.8907\n",
      "Epoch 09: train loss=0.1805, val loss=0.2405, train_acc=0.9321, val_acc=0.9195, val_f1=0.9202\n",
      "New best accuracy: 0.9195 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1481, val loss=0.2686, train_acc=0.9490, val_acc=0.9135, val_f1=0.9143\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1498, val loss=0.3387, train_acc=0.9445, val_acc=0.8875, val_f1=0.8773\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1531, val loss=0.2397, train_acc=0.9467, val_acc=0.9105, val_f1=0.9099\n",
      "Epoch 13: train loss=0.1116, val loss=0.3155, train_acc=0.9620, val_acc=0.8840, val_f1=0.8728\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1033, val loss=0.3091, train_acc=0.9637, val_acc=0.9130, val_f1=0.9138\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.0948, val loss=0.3709, train_acc=0.9656, val_acc=0.9035, val_f1=0.8973\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.0754, val loss=0.3656, train_acc=0.9746, val_acc=0.9165, val_f1=0.9167\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.0669, val loss=0.4928, train_acc=0.9775, val_acc=0.9115, val_f1=0.9158\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5811, val loss=0.4816, train_acc=0.6971, val_acc=0.7990, val_f1=0.8210\n",
      "New best accuracy: 0.7990 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4476, val loss=0.3819, train_acc=0.8264, val_acc=0.8665, val_f1=0.8685\n",
      "New best accuracy: 0.8665 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3637, val loss=0.3220, train_acc=0.8596, val_acc=0.8830, val_f1=0.8802\n",
      "New best accuracy: 0.8830 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3510, val loss=0.2923, train_acc=0.8745, val_acc=0.8955, val_f1=0.8912\n",
      "New best accuracy: 0.8955 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2791, val loss=0.2597, train_acc=0.8965, val_acc=0.9110, val_f1=0.9114\n",
      "New best accuracy: 0.9110 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2391, val loss=0.2652, train_acc=0.9143, val_acc=0.9135, val_f1=0.9134\n",
      "New best accuracy: 0.9135 at epoch 6, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2328, val loss=0.2751, train_acc=0.9169, val_acc=0.9050, val_f1=0.9094\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1985, val loss=0.2834, train_acc=0.9266, val_acc=0.9045, val_f1=0.9085\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 09: train loss=0.1739, val loss=0.2557, train_acc=0.9354, val_acc=0.9165, val_f1=0.9147\n",
      "New best accuracy: 0.9165 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1667, val loss=0.2473, train_acc=0.9406, val_acc=0.9180, val_f1=0.9165\n",
      "New best accuracy: 0.9180 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1334, val loss=0.2849, train_acc=0.9501, val_acc=0.9170, val_f1=0.9139\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1238, val loss=0.3157, train_acc=0.9573, val_acc=0.9225, val_f1=0.9235\n",
      "New best accuracy: 0.9225 at epoch 12, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1088, val loss=0.4076, train_acc=0.9611, val_acc=0.8925, val_f1=0.8998\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1124, val loss=0.2705, train_acc=0.9597, val_acc=0.9135, val_f1=0.9153\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.0885, val loss=0.3023, train_acc=0.9706, val_acc=0.9075, val_f1=0.9087\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=adamw: [0.7951400279998779, 0.8173400163650513, 0.8161399960517883]\n",
      "Test F1 Scores for optimiser=adamw: [0.7939226061211876, 0.8173218051787244, 0.8160394426707263]\n",
      "Average Test Accuracy: 0.8095, Average Test F1: 0.8091\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing optimiser: sgd with lr=0.0005\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6914, val loss=0.6903, train_acc=0.5315, val_acc=0.5750, val_f1=0.5148\n",
      "New best accuracy: 0.5750 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6889, val loss=0.6883, train_acc=0.5549, val_acc=0.5770, val_f1=0.5770\n",
      "New best accuracy: 0.5770 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6867, val loss=0.6863, train_acc=0.5766, val_acc=0.5785, val_f1=0.5984\n",
      "New best accuracy: 0.5785 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.6847, val loss=0.6845, train_acc=0.5804, val_acc=0.5725, val_f1=0.5883\n",
      "Epoch 05: train loss=0.6823, val loss=0.6827, train_acc=0.5897, val_acc=0.5825, val_f1=0.6114\n",
      "New best accuracy: 0.5825 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.6802, val loss=0.6811, train_acc=0.5807, val_acc=0.5770, val_f1=0.6162\n",
      "Epoch 07: train loss=0.6792, val loss=0.6797, train_acc=0.5767, val_acc=0.5775, val_f1=0.6168\n",
      "Epoch 08: train loss=0.6769, val loss=0.6785, train_acc=0.5820, val_acc=0.5790, val_f1=0.6193\n",
      "Epoch 09: train loss=0.6762, val loss=0.6776, train_acc=0.5879, val_acc=0.5810, val_f1=0.6249\n",
      "Epoch 10: train loss=0.6744, val loss=0.6768, train_acc=0.5890, val_acc=0.5800, val_f1=0.6230\n",
      "Epoch 11: train loss=0.6737, val loss=0.6763, train_acc=0.5844, val_acc=0.5795, val_f1=0.6271\n",
      "Epoch 12: train loss=0.6728, val loss=0.6758, train_acc=0.5800, val_acc=0.5825, val_f1=0.6323\n",
      "Epoch 13: train loss=0.6717, val loss=0.6754, train_acc=0.5817, val_acc=0.5805, val_f1=0.6331\n",
      "Epoch 14: train loss=0.6725, val loss=0.6751, train_acc=0.5819, val_acc=0.5815, val_f1=0.6331\n",
      "Epoch 15: train loss=0.6719, val loss=0.6748, train_acc=0.5841, val_acc=0.5815, val_f1=0.6337\n",
      "Epoch 16: train loss=0.6695, val loss=0.6745, train_acc=0.5897, val_acc=0.5810, val_f1=0.6344\n",
      "Epoch 17: train loss=0.6725, val loss=0.6742, train_acc=0.5777, val_acc=0.5770, val_f1=0.6366\n",
      "Epoch 18: train loss=0.6717, val loss=0.6738, train_acc=0.5843, val_acc=0.5800, val_f1=0.6357\n",
      "Epoch 19: train loss=0.6704, val loss=0.6735, train_acc=0.5814, val_acc=0.5835, val_f1=0.6342\n",
      "New best accuracy: 0.5835 at epoch 19, saving model.\n",
      "Epoch 20: train loss=0.6714, val loss=0.6731, train_acc=0.5850, val_acc=0.5800, val_f1=0.6379\n",
      "Epoch 21: train loss=0.6687, val loss=0.6726, train_acc=0.5899, val_acc=0.5845, val_f1=0.6441\n",
      "New best accuracy: 0.5845 at epoch 21, saving model.\n",
      "Epoch 22: train loss=0.6698, val loss=0.6721, train_acc=0.5863, val_acc=0.5855, val_f1=0.6394\n",
      "New best accuracy: 0.5855 at epoch 22, saving model.\n",
      "Epoch 23: train loss=0.6687, val loss=0.6716, train_acc=0.5870, val_acc=0.5885, val_f1=0.6405\n",
      "New best accuracy: 0.5885 at epoch 23, saving model.\n",
      "Epoch 24: train loss=0.6685, val loss=0.6710, train_acc=0.5919, val_acc=0.5865, val_f1=0.6418\n",
      "Epoch 25: train loss=0.6657, val loss=0.6704, train_acc=0.5927, val_acc=0.5900, val_f1=0.6416\n",
      "New best accuracy: 0.5900 at epoch 25, saving model.\n",
      "Epoch 26: train loss=0.6668, val loss=0.6696, train_acc=0.5911, val_acc=0.5890, val_f1=0.6478\n",
      "Epoch 27: train loss=0.6654, val loss=0.6688, train_acc=0.5924, val_acc=0.5945, val_f1=0.6553\n",
      "New best accuracy: 0.5945 at epoch 27, saving model.\n",
      "Epoch 28: train loss=0.6653, val loss=0.6680, train_acc=0.6019, val_acc=0.5910, val_f1=0.6600\n",
      "Epoch 29: train loss=0.6642, val loss=0.6667, train_acc=0.5960, val_acc=0.5985, val_f1=0.6540\n",
      "New best accuracy: 0.5985 at epoch 29, saving model.\n",
      "Epoch 30: train loss=0.6624, val loss=0.6655, train_acc=0.6019, val_acc=0.6025, val_f1=0.6572\n",
      "New best accuracy: 0.6025 at epoch 30, saving model.\n",
      "Epoch 31: train loss=0.6598, val loss=0.6643, train_acc=0.6048, val_acc=0.5935, val_f1=0.6645\n",
      "Epoch 32: train loss=0.6594, val loss=0.6625, train_acc=0.6080, val_acc=0.5960, val_f1=0.6669\n",
      "Epoch 33: train loss=0.6569, val loss=0.6600, train_acc=0.6078, val_acc=0.6150, val_f1=0.6649\n",
      "New best accuracy: 0.6150 at epoch 33, saving model.\n",
      "Epoch 34: train loss=0.6531, val loss=0.6567, train_acc=0.6148, val_acc=0.6095, val_f1=0.6717\n",
      "Epoch 35: train loss=0.6487, val loss=0.6522, train_acc=0.6246, val_acc=0.6150, val_f1=0.6732\n",
      "Epoch 36: train loss=0.6437, val loss=0.6473, train_acc=0.6288, val_acc=0.6235, val_f1=0.7011\n",
      "New best accuracy: 0.6235 at epoch 36, saving model.\n",
      "Epoch 37: train loss=0.6351, val loss=0.6370, train_acc=0.6441, val_acc=0.6400, val_f1=0.7099\n",
      "New best accuracy: 0.6400 at epoch 37, saving model.\n",
      "Epoch 38: train loss=0.6235, val loss=0.6236, train_acc=0.6605, val_acc=0.6635, val_f1=0.7133\n",
      "New best accuracy: 0.6635 at epoch 38, saving model.\n",
      "Epoch 39: train loss=0.6148, val loss=0.6183, train_acc=0.6753, val_acc=0.6775, val_f1=0.6993\n",
      "New best accuracy: 0.6775 at epoch 39, saving model.\n",
      "Epoch 40: train loss=0.6129, val loss=0.6078, train_acc=0.6747, val_acc=0.6720, val_f1=0.7335\n",
      "Epoch 41: train loss=0.5926, val loss=0.6413, train_acc=0.6981, val_acc=0.6160, val_f1=0.7168\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 42: train loss=0.5802, val loss=0.5750, train_acc=0.7141, val_acc=0.7220, val_f1=0.7606\n",
      "New best accuracy: 0.7220 at epoch 42, saving model.\n",
      "Epoch 43: train loss=0.5707, val loss=0.5695, train_acc=0.7248, val_acc=0.7155, val_f1=0.7648\n",
      "Epoch 44: train loss=0.5595, val loss=0.5543, train_acc=0.7422, val_acc=0.7450, val_f1=0.7678\n",
      "New best accuracy: 0.7450 at epoch 44, saving model.\n",
      "Epoch 45: train loss=0.5425, val loss=0.5513, train_acc=0.7552, val_acc=0.7655, val_f1=0.7716\n",
      "New best accuracy: 0.7655 at epoch 45, saving model.\n",
      "Epoch 46: train loss=0.5527, val loss=0.5378, train_acc=0.7418, val_acc=0.7425, val_f1=0.7768\n",
      "Epoch 47: train loss=0.5349, val loss=0.5247, train_acc=0.7669, val_acc=0.7920, val_f1=0.8008\n",
      "New best accuracy: 0.7920 at epoch 47, saving model.\n",
      "Epoch 48: train loss=0.5231, val loss=0.5543, train_acc=0.7820, val_acc=0.7795, val_f1=0.7610\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 49: train loss=0.5082, val loss=0.4938, train_acc=0.7875, val_acc=0.8025, val_f1=0.8174\n",
      "New best accuracy: 0.8025 at epoch 49, saving model.\n",
      "Epoch 50: train loss=0.5054, val loss=0.5720, train_acc=0.7871, val_acc=0.6975, val_f1=0.7627\n",
      "No improvement in val loss for 1 epochs.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6922, val loss=0.6912, train_acc=0.5062, val_acc=0.5000, val_f1=0.0000\n",
      "New best accuracy: 0.5000 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6899, val loss=0.6894, train_acc=0.5406, val_acc=0.5580, val_f1=0.4972\n",
      "New best accuracy: 0.5580 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6882, val loss=0.6877, train_acc=0.5700, val_acc=0.5695, val_f1=0.5636\n",
      "New best accuracy: 0.5695 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.6859, val loss=0.6860, train_acc=0.5733, val_acc=0.5725, val_f1=0.5794\n",
      "New best accuracy: 0.5725 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.6845, val loss=0.6845, train_acc=0.5815, val_acc=0.5785, val_f1=0.6070\n",
      "New best accuracy: 0.5785 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.6827, val loss=0.6829, train_acc=0.5761, val_acc=0.5790, val_f1=0.6058\n",
      "New best accuracy: 0.5790 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.6812, val loss=0.6817, train_acc=0.5801, val_acc=0.5770, val_f1=0.6137\n",
      "Epoch 08: train loss=0.6800, val loss=0.6805, train_acc=0.5814, val_acc=0.5785, val_f1=0.6198\n",
      "Epoch 09: train loss=0.6774, val loss=0.6795, train_acc=0.5849, val_acc=0.5820, val_f1=0.6291\n",
      "New best accuracy: 0.5820 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.6765, val loss=0.6786, train_acc=0.5811, val_acc=0.5795, val_f1=0.6287\n",
      "Epoch 11: train loss=0.6759, val loss=0.6780, train_acc=0.5786, val_acc=0.5760, val_f1=0.6281\n",
      "Epoch 12: train loss=0.6749, val loss=0.6775, train_acc=0.5810, val_acc=0.5790, val_f1=0.6284\n",
      "Epoch 13: train loss=0.6747, val loss=0.6771, train_acc=0.5793, val_acc=0.5780, val_f1=0.6208\n",
      "Epoch 14: train loss=0.6751, val loss=0.6768, train_acc=0.5795, val_acc=0.5825, val_f1=0.6297\n",
      "New best accuracy: 0.5825 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.6737, val loss=0.6765, train_acc=0.5819, val_acc=0.5805, val_f1=0.6253\n",
      "Epoch 16: train loss=0.6746, val loss=0.6763, train_acc=0.5800, val_acc=0.5765, val_f1=0.6290\n",
      "Epoch 17: train loss=0.6731, val loss=0.6760, train_acc=0.5810, val_acc=0.5780, val_f1=0.6314\n",
      "Epoch 18: train loss=0.6741, val loss=0.6758, train_acc=0.5844, val_acc=0.5780, val_f1=0.6314\n",
      "Epoch 19: train loss=0.6726, val loss=0.6756, train_acc=0.5823, val_acc=0.5765, val_f1=0.6344\n",
      "Epoch 20: train loss=0.6743, val loss=0.6753, train_acc=0.5756, val_acc=0.5790, val_f1=0.6323\n",
      "Epoch 21: train loss=0.6725, val loss=0.6751, train_acc=0.5829, val_acc=0.5800, val_f1=0.6335\n",
      "Epoch 22: train loss=0.6703, val loss=0.6749, train_acc=0.5861, val_acc=0.5790, val_f1=0.6432\n",
      "Epoch 23: train loss=0.6712, val loss=0.6745, train_acc=0.5841, val_acc=0.5755, val_f1=0.6383\n",
      "Epoch 24: train loss=0.6705, val loss=0.6741, train_acc=0.5855, val_acc=0.5800, val_f1=0.6357\n",
      "Epoch 25: train loss=0.6689, val loss=0.6738, train_acc=0.5845, val_acc=0.5795, val_f1=0.6364\n",
      "Epoch 26: train loss=0.6700, val loss=0.6733, train_acc=0.5804, val_acc=0.5780, val_f1=0.6378\n",
      "Epoch 27: train loss=0.6704, val loss=0.6729, train_acc=0.5876, val_acc=0.5815, val_f1=0.6425\n",
      "Epoch 28: train loss=0.6697, val loss=0.6723, train_acc=0.5871, val_acc=0.5815, val_f1=0.6400\n",
      "Epoch 29: train loss=0.6701, val loss=0.6718, train_acc=0.5819, val_acc=0.5875, val_f1=0.6377\n",
      "New best accuracy: 0.5875 at epoch 29, saving model.\n",
      "Epoch 30: train loss=0.6667, val loss=0.6712, train_acc=0.5904, val_acc=0.5840, val_f1=0.6429\n",
      "Epoch 31: train loss=0.6667, val loss=0.6705, train_acc=0.5949, val_acc=0.5895, val_f1=0.6441\n",
      "New best accuracy: 0.5895 at epoch 31, saving model.\n",
      "Epoch 32: train loss=0.6655, val loss=0.6698, train_acc=0.5867, val_acc=0.5885, val_f1=0.6442\n",
      "Epoch 33: train loss=0.6653, val loss=0.6689, train_acc=0.5941, val_acc=0.5900, val_f1=0.6447\n",
      "New best accuracy: 0.5900 at epoch 33, saving model.\n",
      "Epoch 34: train loss=0.6631, val loss=0.6679, train_acc=0.5975, val_acc=0.5920, val_f1=0.6486\n",
      "New best accuracy: 0.5920 at epoch 34, saving model.\n",
      "Epoch 35: train loss=0.6628, val loss=0.6668, train_acc=0.5988, val_acc=0.5965, val_f1=0.6520\n",
      "New best accuracy: 0.5965 at epoch 35, saving model.\n",
      "Epoch 36: train loss=0.6614, val loss=0.6655, train_acc=0.6039, val_acc=0.5955, val_f1=0.6622\n",
      "Epoch 37: train loss=0.6601, val loss=0.6637, train_acc=0.6008, val_acc=0.6040, val_f1=0.6624\n",
      "New best accuracy: 0.6040 at epoch 37, saving model.\n",
      "Epoch 38: train loss=0.6579, val loss=0.6620, train_acc=0.6120, val_acc=0.6095, val_f1=0.6594\n",
      "New best accuracy: 0.6095 at epoch 38, saving model.\n",
      "Epoch 39: train loss=0.6554, val loss=0.6596, train_acc=0.6069, val_acc=0.6020, val_f1=0.6716\n",
      "Epoch 40: train loss=0.6522, val loss=0.6564, train_acc=0.6158, val_acc=0.6190, val_f1=0.6678\n",
      "New best accuracy: 0.6190 at epoch 40, saving model.\n",
      "Epoch 41: train loss=0.6473, val loss=0.6513, train_acc=0.6282, val_acc=0.6190, val_f1=0.6777\n",
      "Epoch 42: train loss=0.6407, val loss=0.6440, train_acc=0.6348, val_acc=0.6345, val_f1=0.6834\n",
      "New best accuracy: 0.6345 at epoch 42, saving model.\n",
      "Epoch 43: train loss=0.6329, val loss=0.6331, train_acc=0.6486, val_acc=0.6475, val_f1=0.7138\n",
      "New best accuracy: 0.6475 at epoch 43, saving model.\n",
      "Epoch 44: train loss=0.6167, val loss=0.6173, train_acc=0.6693, val_acc=0.6705, val_f1=0.7337\n",
      "New best accuracy: 0.6705 at epoch 44, saving model.\n",
      "Epoch 45: train loss=0.6017, val loss=0.6142, train_acc=0.6900, val_acc=0.6940, val_f1=0.7052\n",
      "New best accuracy: 0.6940 at epoch 45, saving model.\n",
      "Epoch 46: train loss=0.5924, val loss=0.6175, train_acc=0.6929, val_acc=0.6510, val_f1=0.7309\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 47: train loss=0.5806, val loss=0.6066, train_acc=0.7161, val_acc=0.7095, val_f1=0.7117\n",
      "New best accuracy: 0.7095 at epoch 47, saving model.\n",
      "Epoch 48: train loss=0.5745, val loss=0.5611, train_acc=0.7229, val_acc=0.7325, val_f1=0.7685\n",
      "New best accuracy: 0.7325 at epoch 48, saving model.\n",
      "Epoch 49: train loss=0.5717, val loss=0.5855, train_acc=0.7262, val_acc=0.6805, val_f1=0.7497\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 50: train loss=0.5576, val loss=0.5395, train_acc=0.7446, val_acc=0.7555, val_f1=0.7824\n",
      "New best accuracy: 0.7555 at epoch 50, saving model.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6931, val loss=0.6913, train_acc=0.4916, val_acc=0.5000, val_f1=0.0000\n",
      "New best accuracy: 0.5000 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6901, val loss=0.6885, train_acc=0.5258, val_acc=0.5600, val_f1=0.4614\n",
      "New best accuracy: 0.5600 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6869, val loss=0.6860, train_acc=0.5603, val_acc=0.5720, val_f1=0.5597\n",
      "New best accuracy: 0.5720 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.6846, val loss=0.6839, train_acc=0.5749, val_acc=0.5715, val_f1=0.5793\n",
      "Epoch 05: train loss=0.6817, val loss=0.6820, train_acc=0.5815, val_acc=0.5725, val_f1=0.5891\n",
      "New best accuracy: 0.5725 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.6798, val loss=0.6805, train_acc=0.5736, val_acc=0.5760, val_f1=0.6070\n",
      "New best accuracy: 0.5760 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.6785, val loss=0.6793, train_acc=0.5820, val_acc=0.5790, val_f1=0.6264\n",
      "New best accuracy: 0.5790 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.6779, val loss=0.6783, train_acc=0.5731, val_acc=0.5785, val_f1=0.6255\n",
      "Epoch 09: train loss=0.6746, val loss=0.6775, train_acc=0.5889, val_acc=0.5800, val_f1=0.6216\n",
      "New best accuracy: 0.5800 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.6752, val loss=0.6770, train_acc=0.5816, val_acc=0.5810, val_f1=0.6292\n",
      "New best accuracy: 0.5810 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.6750, val loss=0.6766, train_acc=0.5823, val_acc=0.5810, val_f1=0.6292\n",
      "Epoch 12: train loss=0.6725, val loss=0.6762, train_acc=0.5843, val_acc=0.5795, val_f1=0.6319\n",
      "Epoch 13: train loss=0.6743, val loss=0.6759, train_acc=0.5789, val_acc=0.5775, val_f1=0.6321\n",
      "Epoch 14: train loss=0.6740, val loss=0.6756, train_acc=0.5749, val_acc=0.5810, val_f1=0.6299\n",
      "Epoch 15: train loss=0.6733, val loss=0.6753, train_acc=0.5796, val_acc=0.5810, val_f1=0.6328\n",
      "Epoch 16: train loss=0.6709, val loss=0.6750, train_acc=0.5869, val_acc=0.5810, val_f1=0.6353\n",
      "Epoch 17: train loss=0.6723, val loss=0.6747, train_acc=0.5797, val_acc=0.5820, val_f1=0.6359\n",
      "New best accuracy: 0.5820 at epoch 17, saving model.\n",
      "Epoch 18: train loss=0.6708, val loss=0.6744, train_acc=0.5827, val_acc=0.5830, val_f1=0.6329\n",
      "New best accuracy: 0.5830 at epoch 18, saving model.\n",
      "Epoch 19: train loss=0.6706, val loss=0.6740, train_acc=0.5819, val_acc=0.5805, val_f1=0.6354\n",
      "Epoch 20: train loss=0.6701, val loss=0.6736, train_acc=0.5857, val_acc=0.5790, val_f1=0.6383\n",
      "Epoch 21: train loss=0.6704, val loss=0.6731, train_acc=0.5873, val_acc=0.5820, val_f1=0.6387\n",
      "Epoch 22: train loss=0.6691, val loss=0.6726, train_acc=0.5841, val_acc=0.5825, val_f1=0.6396\n",
      "Epoch 23: train loss=0.6690, val loss=0.6720, train_acc=0.5886, val_acc=0.5835, val_f1=0.6389\n",
      "New best accuracy: 0.5835 at epoch 23, saving model.\n",
      "Epoch 24: train loss=0.6686, val loss=0.6714, train_acc=0.5870, val_acc=0.5880, val_f1=0.6389\n",
      "New best accuracy: 0.5880 at epoch 24, saving model.\n",
      "Epoch 25: train loss=0.6674, val loss=0.6707, train_acc=0.5952, val_acc=0.5865, val_f1=0.6449\n",
      "Epoch 26: train loss=0.6678, val loss=0.6698, train_acc=0.5911, val_acc=0.5910, val_f1=0.6516\n",
      "New best accuracy: 0.5910 at epoch 26, saving model.\n",
      "Epoch 27: train loss=0.6661, val loss=0.6689, train_acc=0.5881, val_acc=0.5920, val_f1=0.6474\n",
      "New best accuracy: 0.5920 at epoch 27, saving model.\n",
      "Epoch 28: train loss=0.6645, val loss=0.6678, train_acc=0.6004, val_acc=0.5950, val_f1=0.6530\n",
      "New best accuracy: 0.5950 at epoch 28, saving model.\n",
      "Epoch 29: train loss=0.6623, val loss=0.6668, train_acc=0.6001, val_acc=0.5970, val_f1=0.6402\n",
      "New best accuracy: 0.5970 at epoch 29, saving model.\n",
      "Epoch 30: train loss=0.6608, val loss=0.6645, train_acc=0.6031, val_acc=0.6030, val_f1=0.6630\n",
      "New best accuracy: 0.6030 at epoch 30, saving model.\n",
      "Epoch 31: train loss=0.6575, val loss=0.6622, train_acc=0.6036, val_acc=0.5985, val_f1=0.6656\n",
      "Epoch 32: train loss=0.6577, val loss=0.6589, train_acc=0.6118, val_acc=0.6180, val_f1=0.6687\n",
      "New best accuracy: 0.6180 at epoch 32, saving model.\n",
      "Epoch 33: train loss=0.6515, val loss=0.6535, train_acc=0.6152, val_acc=0.6225, val_f1=0.6719\n",
      "New best accuracy: 0.6225 at epoch 33, saving model.\n",
      "Epoch 34: train loss=0.6458, val loss=0.6448, train_acc=0.6238, val_acc=0.6335, val_f1=0.6814\n",
      "New best accuracy: 0.6335 at epoch 34, saving model.\n",
      "Epoch 35: train loss=0.6326, val loss=0.6348, train_acc=0.6506, val_acc=0.6485, val_f1=0.7231\n",
      "New best accuracy: 0.6485 at epoch 35, saving model.\n",
      "Epoch 36: train loss=0.6151, val loss=0.6173, train_acc=0.6746, val_acc=0.6815, val_f1=0.7170\n",
      "New best accuracy: 0.6815 at epoch 36, saving model.\n",
      "Epoch 37: train loss=0.6132, val loss=0.6656, train_acc=0.6765, val_acc=0.6285, val_f1=0.5911\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 38: train loss=0.5982, val loss=0.6052, train_acc=0.6917, val_acc=0.6680, val_f1=0.7382\n",
      "Epoch 39: train loss=0.5867, val loss=0.5956, train_acc=0.7080, val_acc=0.6730, val_f1=0.7413\n",
      "Epoch 40: train loss=0.5772, val loss=0.5921, train_acc=0.7215, val_acc=0.6610, val_f1=0.7392\n",
      "Epoch 41: train loss=0.6038, val loss=0.5770, train_acc=0.6773, val_acc=0.7000, val_f1=0.7498\n",
      "New best accuracy: 0.7000 at epoch 41, saving model.\n",
      "Epoch 42: train loss=0.5660, val loss=0.5873, train_acc=0.7260, val_acc=0.7505, val_f1=0.7307\n",
      "New best accuracy: 0.7505 at epoch 42, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 43: train loss=0.5394, val loss=0.5285, train_acc=0.7636, val_acc=0.7690, val_f1=0.7930\n",
      "New best accuracy: 0.7690 at epoch 43, saving model.\n",
      "Epoch 44: train loss=0.5438, val loss=0.5528, train_acc=0.7629, val_acc=0.7200, val_f1=0.7731\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 45: train loss=0.5240, val loss=0.5211, train_acc=0.7719, val_acc=0.8010, val_f1=0.7998\n",
      "New best accuracy: 0.8010 at epoch 45, saving model.\n",
      "Epoch 46: train loss=0.5093, val loss=0.5016, train_acc=0.7887, val_acc=0.7810, val_f1=0.8062\n",
      "Epoch 47: train loss=0.5057, val loss=0.5010, train_acc=0.7924, val_acc=0.8105, val_f1=0.8098\n",
      "New best accuracy: 0.8105 at epoch 47, saving model.\n",
      "Epoch 48: train loss=0.5026, val loss=0.4809, train_acc=0.7893, val_acc=0.8195, val_f1=0.8272\n",
      "New best accuracy: 0.8195 at epoch 48, saving model.\n",
      "Epoch 49: train loss=0.4948, val loss=0.5015, train_acc=0.7931, val_acc=0.7770, val_f1=0.8068\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 50: train loss=0.4862, val loss=0.5018, train_acc=0.8004, val_acc=0.8155, val_f1=0.8024\n",
      "No improvement in val loss for 2 epochs.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for optimiser=sgd: [0.6363400220870972, 0.6193199753761292, 0.6701400279998779]\n",
      "Test F1 Scores for optimiser=sgd: [0.6107802555864849, 0.5961922921425727, 0.658954664512496]\n",
      "Average Test Accuracy: 0.6419, Average Test F1: 0.6220\n",
      "\n",
      "\n",
      "========================================\n",
      "Best optimiser: adamw with average test accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "optimisers = [\"adam\", \"adamw\", \"sgd\"]  \n",
    "\n",
    "best_avg_acc_opt = 0.0\n",
    "best_optimiser = None\n",
    "all_optimiser_results = {}\n",
    "\n",
    "for opt in optimisers:\n",
    "    print(f\"\\n{'='*40}\\nTesting optimiser: {opt} with lr={best_lr}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr, optimiser=opt, weight_decay=0.0,\n",
    "        cnn_channels=100, kernel_sizes=(3,4,5), method=\"avg\"\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for optimiser={opt}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for optimiser={opt}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_optimiser_results[opt] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_opt:\n",
    "        best_avg_acc_opt = avg_acc\n",
    "        best_optimiser = opt\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest optimiser: {best_optimiser} with average test accuracy: {best_avg_acc_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca34507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing weight decay: 0.0 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5902, val loss=0.4960, train_acc=0.6941, val_acc=0.8160, val_f1=0.8132\n",
      "New best accuracy: 0.8160 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4351, val loss=0.4274, train_acc=0.8267, val_acc=0.8275, val_f1=0.8445\n",
      "New best accuracy: 0.8275 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4064, val loss=0.3488, train_acc=0.8406, val_acc=0.8700, val_f1=0.8660\n",
      "New best accuracy: 0.8700 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3391, val loss=0.3285, train_acc=0.8695, val_acc=0.8815, val_f1=0.8732\n",
      "New best accuracy: 0.8815 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3187, val loss=0.2874, train_acc=0.8822, val_acc=0.8975, val_f1=0.8983\n",
      "New best accuracy: 0.8975 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2630, val loss=0.3118, train_acc=0.9010, val_acc=0.9000, val_f1=0.9036\n",
      "New best accuracy: 0.9000 at epoch 6, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2352, val loss=0.2521, train_acc=0.9149, val_acc=0.9170, val_f1=0.9163\n",
      "New best accuracy: 0.9170 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2324, val loss=0.2485, train_acc=0.9146, val_acc=0.9200, val_f1=0.9183\n",
      "New best accuracy: 0.9200 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1930, val loss=0.2454, train_acc=0.9335, val_acc=0.9165, val_f1=0.9145\n",
      "Epoch 10: train loss=0.1828, val loss=0.2462, train_acc=0.9339, val_acc=0.9180, val_f1=0.9162\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1451, val loss=0.3104, train_acc=0.9514, val_acc=0.9130, val_f1=0.9135\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1364, val loss=0.2872, train_acc=0.9527, val_acc=0.8995, val_f1=0.8922\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1315, val loss=0.3966, train_acc=0.9564, val_acc=0.9100, val_f1=0.9139\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1535, val loss=0.2803, train_acc=0.9470, val_acc=0.9195, val_f1=0.9177\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5832, val loss=0.4763, train_acc=0.6997, val_acc=0.8250, val_f1=0.8207\n",
      "New best accuracy: 0.8250 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4413, val loss=0.4373, train_acc=0.8175, val_acc=0.8320, val_f1=0.8102\n",
      "New best accuracy: 0.8320 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3797, val loss=0.3498, train_acc=0.8505, val_acc=0.8730, val_f1=0.8778\n",
      "New best accuracy: 0.8730 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3248, val loss=0.2983, train_acc=0.8751, val_acc=0.8905, val_f1=0.8872\n",
      "New best accuracy: 0.8905 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3152, val loss=0.3943, train_acc=0.8804, val_acc=0.8515, val_f1=0.8300\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2510, val loss=0.3086, train_acc=0.9080, val_acc=0.9010, val_f1=0.9033\n",
      "New best accuracy: 0.9010 at epoch 6, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.2418, val loss=0.2532, train_acc=0.9117, val_acc=0.9110, val_f1=0.9077\n",
      "New best accuracy: 0.9110 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2084, val loss=0.2611, train_acc=0.9249, val_acc=0.9150, val_f1=0.9120\n",
      "New best accuracy: 0.9150 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1859, val loss=0.2792, train_acc=0.9331, val_acc=0.8930, val_f1=0.8846\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1696, val loss=0.2534, train_acc=0.9363, val_acc=0.9100, val_f1=0.9062\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1563, val loss=0.2962, train_acc=0.9441, val_acc=0.9150, val_f1=0.9165\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.1458, val loss=0.2676, train_acc=0.9479, val_acc=0.9180, val_f1=0.9179\n",
      "New best accuracy: 0.9180 at epoch 12, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6074, val loss=0.4659, train_acc=0.6687, val_acc=0.8140, val_f1=0.8265\n",
      "New best accuracy: 0.8140 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4613, val loss=0.4069, train_acc=0.8203, val_acc=0.8530, val_f1=0.8557\n",
      "New best accuracy: 0.8530 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3974, val loss=0.3521, train_acc=0.8481, val_acc=0.8690, val_f1=0.8738\n",
      "New best accuracy: 0.8690 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3287, val loss=0.3092, train_acc=0.8766, val_acc=0.8905, val_f1=0.8898\n",
      "New best accuracy: 0.8905 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2853, val loss=0.2664, train_acc=0.8869, val_acc=0.9070, val_f1=0.9045\n",
      "New best accuracy: 0.9070 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2476, val loss=0.2869, train_acc=0.9089, val_acc=0.9020, val_f1=0.9051\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2191, val loss=0.2550, train_acc=0.9195, val_acc=0.9130, val_f1=0.9138\n",
      "New best accuracy: 0.9130 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1935, val loss=0.2627, train_acc=0.9303, val_acc=0.9055, val_f1=0.9002\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1793, val loss=0.2485, train_acc=0.9347, val_acc=0.9180, val_f1=0.9187\n",
      "New best accuracy: 0.9180 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1702, val loss=0.2413, train_acc=0.9394, val_acc=0.9135, val_f1=0.9121\n",
      "Epoch 11: train loss=0.1372, val loss=0.2585, train_acc=0.9501, val_acc=0.9175, val_f1=0.9148\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1463, val loss=0.2516, train_acc=0.9506, val_acc=0.9230, val_f1=0.9223\n",
      "New best accuracy: 0.9230 at epoch 12, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1170, val loss=0.2381, train_acc=0.9603, val_acc=0.9225, val_f1=0.9218\n",
      "Epoch 14: train loss=0.1193, val loss=0.3611, train_acc=0.9585, val_acc=0.8565, val_f1=0.8365\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1012, val loss=0.2623, train_acc=0.9646, val_acc=0.9150, val_f1=0.9116\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1173, val loss=0.2560, train_acc=0.9593, val_acc=0.9155, val_f1=0.9140\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.0673, val loss=0.3445, train_acc=0.9784, val_acc=0.9220, val_f1=0.9203\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.0705, val loss=0.3542, train_acc=0.9775, val_acc=0.9135, val_f1=0.9121\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.0: [0.7863199710845947, 0.8115599751472473, 0.8073800206184387]\n",
      "Test F1 Scores for weight_decay=0.0: [0.7831768976161473, 0.8112128781763107, 0.8072090236310728]\n",
      "Average Test Accuracy: 0.8018, Average Test F1: 0.8005\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 1e-05 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5989, val loss=0.4913, train_acc=0.6926, val_acc=0.8175, val_f1=0.8064\n",
      "New best accuracy: 0.8175 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4362, val loss=0.3915, train_acc=0.8103, val_acc=0.8305, val_f1=0.8336\n",
      "New best accuracy: 0.8305 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.7202, val loss=0.6513, train_acc=0.5870, val_acc=0.6165, val_f1=0.7010\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.5852, val loss=0.5304, train_acc=0.7087, val_acc=0.7170, val_f1=0.7697\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.4957, val loss=0.5202, train_acc=0.7891, val_acc=0.7995, val_f1=0.8245\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 06: train loss=0.3824, val loss=0.3457, train_acc=0.8371, val_acc=0.8695, val_f1=0.8776\n",
      "New best accuracy: 0.8695 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2815, val loss=0.2702, train_acc=0.8866, val_acc=0.8895, val_f1=0.8947\n",
      "New best accuracy: 0.8895 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2453, val loss=0.2446, train_acc=0.9021, val_acc=0.9015, val_f1=0.8995\n",
      "New best accuracy: 0.9015 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2244, val loss=0.2412, train_acc=0.9133, val_acc=0.9015, val_f1=0.9006\n",
      "Epoch 10: train loss=0.1943, val loss=0.2268, train_acc=0.9290, val_acc=0.9090, val_f1=0.9112\n",
      "New best accuracy: 0.9090 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1862, val loss=0.2360, train_acc=0.9314, val_acc=0.9050, val_f1=0.9060\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1663, val loss=0.2363, train_acc=0.9413, val_acc=0.9115, val_f1=0.9123\n",
      "New best accuracy: 0.9115 at epoch 12, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1459, val loss=0.2528, train_acc=0.9485, val_acc=0.9150, val_f1=0.9147\n",
      "New best accuracy: 0.9150 at epoch 13, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1234, val loss=0.2968, train_acc=0.9580, val_acc=0.8810, val_f1=0.8900\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1094, val loss=0.2623, train_acc=0.9647, val_acc=0.9130, val_f1=0.9118\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5888, val loss=0.4870, train_acc=0.6884, val_acc=0.8030, val_f1=0.8000\n",
      "New best accuracy: 0.8030 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4630, val loss=0.4257, train_acc=0.8209, val_acc=0.8335, val_f1=0.8465\n",
      "New best accuracy: 0.8335 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3794, val loss=0.3331, train_acc=0.8492, val_acc=0.8655, val_f1=0.8715\n",
      "New best accuracy: 0.8655 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3169, val loss=0.2926, train_acc=0.8729, val_acc=0.8830, val_f1=0.8888\n",
      "New best accuracy: 0.8830 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3427, val loss=0.4196, train_acc=0.8689, val_acc=0.8605, val_f1=0.8671\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.3008, val loss=0.3384, train_acc=0.8862, val_acc=0.8640, val_f1=0.8502\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.3509, val loss=0.3102, train_acc=0.8746, val_acc=0.8830, val_f1=0.8745\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 08: train loss=0.2675, val loss=0.2610, train_acc=0.9018, val_acc=0.9150, val_f1=0.9150\n",
      "New best accuracy: 0.9150 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2343, val loss=0.2616, train_acc=0.9171, val_acc=0.9130, val_f1=0.9151\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2085, val loss=0.2466, train_acc=0.9239, val_acc=0.9185, val_f1=0.9180\n",
      "New best accuracy: 0.9185 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1834, val loss=0.2667, train_acc=0.9366, val_acc=0.9175, val_f1=0.9206\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1774, val loss=0.2551, train_acc=0.9341, val_acc=0.9095, val_f1=0.9061\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1812, val loss=0.2574, train_acc=0.9351, val_acc=0.9180, val_f1=0.9205\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1468, val loss=0.2700, train_acc=0.9471, val_acc=0.9160, val_f1=0.9180\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1524, val loss=0.2384, train_acc=0.9459, val_acc=0.9210, val_f1=0.9204\n",
      "New best accuracy: 0.9210 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.1265, val loss=0.2384, train_acc=0.9543, val_acc=0.9205, val_f1=0.9210\n",
      "Epoch 17: train loss=0.1148, val loss=0.3023, train_acc=0.9595, val_acc=0.8995, val_f1=0.8921\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 18: train loss=0.1005, val loss=0.3099, train_acc=0.9649, val_acc=0.9165, val_f1=0.9154\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 19: train loss=0.1152, val loss=0.3919, train_acc=0.9569, val_acc=0.9140, val_f1=0.9159\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 20: train loss=0.0818, val loss=0.4527, train_acc=0.9722, val_acc=0.9170, val_f1=0.9188\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 21: train loss=0.0773, val loss=0.3004, train_acc=0.9710, val_acc=0.8985, val_f1=0.8917\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 21.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5878, val loss=0.4718, train_acc=0.7064, val_acc=0.8230, val_f1=0.8254\n",
      "New best accuracy: 0.8230 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4761, val loss=0.5464, train_acc=0.8161, val_acc=0.7660, val_f1=0.8040\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4054, val loss=0.3745, train_acc=0.8289, val_acc=0.8415, val_f1=0.8510\n",
      "New best accuracy: 0.8415 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3496, val loss=0.3110, train_acc=0.8559, val_acc=0.8855, val_f1=0.8865\n",
      "New best accuracy: 0.8855 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3151, val loss=0.3091, train_acc=0.8768, val_acc=0.8840, val_f1=0.8810\n",
      "Epoch 06: train loss=0.3054, val loss=0.3202, train_acc=0.8781, val_acc=0.8600, val_f1=0.8450\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2830, val loss=0.2667, train_acc=0.8874, val_acc=0.9080, val_f1=0.9061\n",
      "New best accuracy: 0.9080 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2399, val loss=0.2634, train_acc=0.9100, val_acc=0.9015, val_f1=0.8978\n",
      "Epoch 09: train loss=0.2126, val loss=0.2747, train_acc=0.9185, val_acc=0.9040, val_f1=0.9051\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1955, val loss=0.2528, train_acc=0.9265, val_acc=0.9180, val_f1=0.9197\n",
      "New best accuracy: 0.9180 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.2129, val loss=0.2461, train_acc=0.9207, val_acc=0.9065, val_f1=0.9082\n",
      "Epoch 12: train loss=0.1594, val loss=0.2735, train_acc=0.9433, val_acc=0.9165, val_f1=0.9187\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1416, val loss=0.2740, train_acc=0.9499, val_acc=0.9105, val_f1=0.9065\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1296, val loss=0.2799, train_acc=0.9550, val_acc=0.9050, val_f1=0.9101\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.1305, val loss=0.2347, train_acc=0.9543, val_acc=0.9185, val_f1=0.9185\n",
      "New best accuracy: 0.9185 at epoch 15, saving model.\n",
      "Epoch 16: train loss=0.1241, val loss=0.2653, train_acc=0.9570, val_acc=0.8945, val_f1=0.8871\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1051, val loss=0.2572, train_acc=0.9650, val_acc=0.9135, val_f1=0.9110\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.1044, val loss=0.2609, train_acc=0.9650, val_acc=0.9090, val_f1=0.9098\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.0910, val loss=0.2773, train_acc=0.9701, val_acc=0.9220, val_f1=0.9217\n",
      "New best accuracy: 0.9220 at epoch 19, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 20: train loss=0.0860, val loss=0.3227, train_acc=0.9720, val_acc=0.9145, val_f1=0.9174\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 20.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=1e-05: [0.7950400114059448, 0.8008800148963928, 0.8027600049972534]\n",
      "Test F1 Scores for weight_decay=1e-05: [0.7943073841163449, 0.7998722926323332, 0.8023210946334165]\n",
      "Average Test Accuracy: 0.7996, Average Test F1: 0.7988\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 0.0001 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5957, val loss=0.5194, train_acc=0.6830, val_acc=0.7925, val_f1=0.7713\n",
      "New best accuracy: 0.7925 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6235, val loss=0.6202, train_acc=0.6769, val_acc=0.6825, val_f1=0.7088\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.5396, val loss=0.4884, train_acc=0.7625, val_acc=0.8210, val_f1=0.8118\n",
      "New best accuracy: 0.8210 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4520, val loss=0.4811, train_acc=0.8280, val_acc=0.8080, val_f1=0.7739\n",
      "Epoch 05: train loss=0.4091, val loss=0.3609, train_acc=0.8405, val_acc=0.8645, val_f1=0.8545\n",
      "New best accuracy: 0.8645 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3415, val loss=0.3322, train_acc=0.8659, val_acc=0.8600, val_f1=0.8438\n",
      "Epoch 07: train loss=0.2840, val loss=0.2912, train_acc=0.8955, val_acc=0.8840, val_f1=0.8755\n",
      "New best accuracy: 0.8840 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2538, val loss=0.2516, train_acc=0.9046, val_acc=0.9070, val_f1=0.9058\n",
      "New best accuracy: 0.9070 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2305, val loss=0.2618, train_acc=0.9095, val_acc=0.8935, val_f1=0.8858\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2108, val loss=0.2559, train_acc=0.9214, val_acc=0.9030, val_f1=0.8996\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1773, val loss=0.2578, train_acc=0.9373, val_acc=0.8955, val_f1=0.8885\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1585, val loss=0.3415, train_acc=0.9437, val_acc=0.8620, val_f1=0.8446\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.1628, val loss=0.2566, train_acc=0.9426, val_acc=0.9085, val_f1=0.9060\n",
      "New best accuracy: 0.9085 at epoch 13, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5696, val loss=0.4749, train_acc=0.7126, val_acc=0.8200, val_f1=0.8189\n",
      "New best accuracy: 0.8200 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4643, val loss=0.4126, train_acc=0.8105, val_acc=0.8505, val_f1=0.8571\n",
      "New best accuracy: 0.8505 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4151, val loss=0.4451, train_acc=0.8415, val_acc=0.8375, val_f1=0.8195\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3684, val loss=0.4396, train_acc=0.8602, val_acc=0.8265, val_f1=0.7967\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 05: train loss=0.3009, val loss=0.2845, train_acc=0.8858, val_acc=0.9085, val_f1=0.9084\n",
      "New best accuracy: 0.9085 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2781, val loss=0.3002, train_acc=0.8995, val_acc=0.8760, val_f1=0.8648\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2465, val loss=0.3230, train_acc=0.9094, val_acc=0.8605, val_f1=0.8428\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.2105, val loss=0.2490, train_acc=0.9247, val_acc=0.9165, val_f1=0.9174\n",
      "New best accuracy: 0.9165 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1945, val loss=0.2459, train_acc=0.9295, val_acc=0.9210, val_f1=0.9219\n",
      "New best accuracy: 0.9210 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1769, val loss=0.2573, train_acc=0.9367, val_acc=0.9300, val_f1=0.9299\n",
      "New best accuracy: 0.9300 at epoch 10, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1755, val loss=0.2850, train_acc=0.9349, val_acc=0.9205, val_f1=0.9208\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1629, val loss=0.2527, train_acc=0.9426, val_acc=0.9170, val_f1=0.9143\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1346, val loss=0.2912, train_acc=0.9533, val_acc=0.9200, val_f1=0.9204\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1218, val loss=0.3871, train_acc=0.9559, val_acc=0.9065, val_f1=0.9114\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5987, val loss=0.4937, train_acc=0.6894, val_acc=0.7500, val_f1=0.7899\n",
      "New best accuracy: 0.7500 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4659, val loss=1.2065, train_acc=0.8054, val_acc=0.5050, val_f1=0.6689\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4734, val loss=0.3961, train_acc=0.7814, val_acc=0.8200, val_f1=0.8352\n",
      "New best accuracy: 0.8200 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3697, val loss=0.3540, train_acc=0.8464, val_acc=0.8510, val_f1=0.8642\n",
      "New best accuracy: 0.8510 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3219, val loss=0.2886, train_acc=0.8689, val_acc=0.8890, val_f1=0.8910\n",
      "New best accuracy: 0.8890 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2869, val loss=0.2720, train_acc=0.8838, val_acc=0.9010, val_f1=0.9007\n",
      "New best accuracy: 0.9010 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2476, val loss=0.2628, train_acc=0.9070, val_acc=0.8935, val_f1=0.8978\n",
      "Epoch 08: train loss=0.2542, val loss=0.3021, train_acc=0.9049, val_acc=0.8845, val_f1=0.8927\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2248, val loss=0.2534, train_acc=0.9120, val_acc=0.9100, val_f1=0.9094\n",
      "New best accuracy: 0.9100 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2064, val loss=0.2399, train_acc=0.9219, val_acc=0.9085, val_f1=0.9068\n",
      "Epoch 11: train loss=0.1820, val loss=0.2756, train_acc=0.9314, val_acc=0.8885, val_f1=0.8786\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1722, val loss=0.3167, train_acc=0.9340, val_acc=0.9010, val_f1=0.9053\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1593, val loss=0.2361, train_acc=0.9417, val_acc=0.9130, val_f1=0.9097\n",
      "New best accuracy: 0.9130 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1379, val loss=0.2500, train_acc=0.9534, val_acc=0.9125, val_f1=0.9116\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1366, val loss=0.2584, train_acc=0.9516, val_acc=0.9090, val_f1=0.9050\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1094, val loss=0.2581, train_acc=0.9607, val_acc=0.9180, val_f1=0.9184\n",
      "New best accuracy: 0.9180 at epoch 16, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.0947, val loss=0.2697, train_acc=0.9684, val_acc=0.9095, val_f1=0.9054\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.0802, val loss=0.2660, train_acc=0.9739, val_acc=0.9070, val_f1=0.9022\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.0001: [0.7777400016784668, 0.8021199703216553, 0.805400013923645]\n",
      "Test F1 Scores for weight_decay=0.0001: [0.7735151293905091, 0.800675813670097, 0.8053545544928158]\n",
      "Average Test Accuracy: 0.7951, Average Test F1: 0.7932\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing weight decay: 0.001 with lr=0.0005, optimiser=adamw\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5927, val loss=0.4685, train_acc=0.6883, val_acc=0.8155, val_f1=0.8245\n",
      "New best accuracy: 0.8155 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4603, val loss=0.3984, train_acc=0.8170, val_acc=0.8400, val_f1=0.8475\n",
      "New best accuracy: 0.8400 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4086, val loss=0.3760, train_acc=0.8433, val_acc=0.8660, val_f1=0.8716\n",
      "New best accuracy: 0.8660 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3515, val loss=0.3184, train_acc=0.8680, val_acc=0.8905, val_f1=0.8948\n",
      "New best accuracy: 0.8905 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2847, val loss=0.2986, train_acc=0.8976, val_acc=0.8855, val_f1=0.8913\n",
      "Epoch 06: train loss=0.2634, val loss=0.2550, train_acc=0.9005, val_acc=0.9145, val_f1=0.9140\n",
      "New best accuracy: 0.9145 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2312, val loss=0.2703, train_acc=0.9163, val_acc=0.9015, val_f1=0.8959\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2023, val loss=0.2433, train_acc=0.9239, val_acc=0.9200, val_f1=0.9207\n",
      "New best accuracy: 0.9200 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1773, val loss=0.2520, train_acc=0.9386, val_acc=0.9275, val_f1=0.9280\n",
      "New best accuracy: 0.9275 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1617, val loss=0.2420, train_acc=0.9406, val_acc=0.9210, val_f1=0.9213\n",
      "Epoch 11: train loss=0.1367, val loss=0.2982, train_acc=0.9514, val_acc=0.9115, val_f1=0.9074\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1304, val loss=0.2436, train_acc=0.9561, val_acc=0.9250, val_f1=0.9257\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1012, val loss=0.3140, train_acc=0.9679, val_acc=0.9125, val_f1=0.9094\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1069, val loss=0.3790, train_acc=0.9653, val_acc=0.8830, val_f1=0.8720\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.0904, val loss=0.3055, train_acc=0.9695, val_acc=0.9105, val_f1=0.9082\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5851, val loss=0.4734, train_acc=0.6957, val_acc=0.8130, val_f1=0.8275\n",
      "New best accuracy: 0.8130 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4439, val loss=0.4287, train_acc=0.8265, val_acc=0.8115, val_f1=0.8346\n",
      "Epoch 03: train loss=0.3862, val loss=0.3925, train_acc=0.8556, val_acc=0.8555, val_f1=0.8406\n",
      "New best accuracy: 0.8555 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3824, val loss=0.3749, train_acc=0.8420, val_acc=0.8495, val_f1=0.8601\n",
      "Epoch 05: train loss=0.3052, val loss=0.2891, train_acc=0.8748, val_acc=0.8910, val_f1=0.8909\n",
      "New best accuracy: 0.8910 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2950, val loss=0.2880, train_acc=0.8864, val_acc=0.8930, val_f1=0.8960\n",
      "New best accuracy: 0.8930 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2877, val loss=0.2565, train_acc=0.8920, val_acc=0.9070, val_f1=0.9056\n",
      "New best accuracy: 0.9070 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2508, val loss=0.2611, train_acc=0.9010, val_acc=0.9080, val_f1=0.9055\n",
      "New best accuracy: 0.9080 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2305, val loss=0.2597, train_acc=0.9106, val_acc=0.9040, val_f1=0.9072\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.2040, val loss=0.2408, train_acc=0.9266, val_acc=0.9100, val_f1=0.9077\n",
      "New best accuracy: 0.9100 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1829, val loss=0.2255, train_acc=0.9314, val_acc=0.9185, val_f1=0.9185\n",
      "New best accuracy: 0.9185 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1965, val loss=0.2402, train_acc=0.9255, val_acc=0.9150, val_f1=0.9151\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1625, val loss=0.2440, train_acc=0.9411, val_acc=0.9155, val_f1=0.9129\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1551, val loss=0.2434, train_acc=0.9411, val_acc=0.9210, val_f1=0.9203\n",
      "New best accuracy: 0.9210 at epoch 14, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.1441, val loss=0.2845, train_acc=0.9480, val_acc=0.9155, val_f1=0.9161\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 16: train loss=0.1476, val loss=0.2994, train_acc=0.9534, val_acc=0.8895, val_f1=0.8807\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 16.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6006, val loss=0.5100, train_acc=0.6787, val_acc=0.7845, val_f1=0.7872\n",
      "New best accuracy: 0.7845 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4579, val loss=0.3958, train_acc=0.8133, val_acc=0.8445, val_f1=0.8446\n",
      "New best accuracy: 0.8445 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3766, val loss=0.3304, train_acc=0.8510, val_acc=0.8785, val_f1=0.8728\n",
      "New best accuracy: 0.8785 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3405, val loss=0.2938, train_acc=0.8749, val_acc=0.8965, val_f1=0.8910\n",
      "New best accuracy: 0.8965 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2835, val loss=0.2770, train_acc=0.8934, val_acc=0.9035, val_f1=0.9053\n",
      "New best accuracy: 0.9035 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2426, val loss=0.2752, train_acc=0.9110, val_acc=0.9065, val_f1=0.9043\n",
      "New best accuracy: 0.9065 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2223, val loss=0.2552, train_acc=0.9196, val_acc=0.9010, val_f1=0.8958\n",
      "Epoch 08: train loss=0.1994, val loss=0.2673, train_acc=0.9237, val_acc=0.9125, val_f1=0.9129\n",
      "New best accuracy: 0.9125 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1799, val loss=0.2406, train_acc=0.9349, val_acc=0.9175, val_f1=0.9160\n",
      "New best accuracy: 0.9175 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1478, val loss=0.2437, train_acc=0.9467, val_acc=0.9030, val_f1=0.8984\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1350, val loss=0.2956, train_acc=0.9513, val_acc=0.9065, val_f1=0.9117\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1277, val loss=0.2614, train_acc=0.9561, val_acc=0.9095, val_f1=0.9054\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1054, val loss=0.2732, train_acc=0.9655, val_acc=0.9070, val_f1=0.9096\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.0835, val loss=0.3057, train_acc=0.9729, val_acc=0.9030, val_f1=0.8995\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for weight_decay=0.001: [0.81632000207901, 0.8099600076675415, 0.7899199724197388]\n",
      "Test F1 Scores for weight_decay=0.001: [0.8162162471612534, 0.809575760751329, 0.7868879010687736]\n",
      "Average Test Accuracy: 0.8054, Average Test F1: 0.8042\n",
      "\n",
      "\n",
      "========================================\n",
      "Best weight decay: 0.001 with average test accuracy: 0.8054\n"
     ]
    }
   ],
   "source": [
    "weight_decays = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "best_avg_acc_wd = 0.0\n",
    "best_weight_decay = None\n",
    "all_wd_results = {}\n",
    "\n",
    "for wd in weight_decays:\n",
    "    print(f\"\\n{'='*40}\\nTesting weight decay: {wd} with lr={best_lr}, optimiser={best_optimiser}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr, optimiser=best_optimiser, weight_decay=wd,\n",
    "        cnn_channels=100, kernel_sizes=(3,4,5), method=\"avg\"\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for weight_decay={wd}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for weight_decay={wd}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_wd_results[wd] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_wd:\n",
    "        best_avg_acc_wd = avg_acc\n",
    "        best_weight_decay = wd\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest weight decay: {best_weight_decay} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_wd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "298dff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing num_channels: 50 with lr=0.0005, optimiser=adamw, weight_Decay=0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6117, val loss=0.4935, train_acc=0.6620, val_acc=0.7840, val_f1=0.8118\n",
      "New best accuracy: 0.7840 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4696, val loss=0.4882, train_acc=0.8135, val_acc=0.7750, val_f1=0.8109\n",
      "Epoch 03: train loss=0.3875, val loss=0.3964, train_acc=0.8486, val_acc=0.8500, val_f1=0.8605\n",
      "New best accuracy: 0.8500 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3503, val loss=0.4261, train_acc=0.8582, val_acc=0.8210, val_f1=0.8438\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3341, val loss=0.3624, train_acc=0.8745, val_acc=0.8655, val_f1=0.8739\n",
      "New best accuracy: 0.8655 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2788, val loss=0.3020, train_acc=0.8962, val_acc=0.8985, val_f1=0.9014\n",
      "New best accuracy: 0.8985 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2654, val loss=0.2975, train_acc=0.9028, val_acc=0.8805, val_f1=0.8700\n",
      "Epoch 08: train loss=0.2297, val loss=0.2698, train_acc=0.9169, val_acc=0.9025, val_f1=0.8982\n",
      "New best accuracy: 0.9025 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2171, val loss=0.2623, train_acc=0.9201, val_acc=0.9020, val_f1=0.8966\n",
      "Epoch 10: train loss=0.1981, val loss=0.3161, train_acc=0.9295, val_acc=0.8780, val_f1=0.8662\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1875, val loss=0.2443, train_acc=0.9321, val_acc=0.9130, val_f1=0.9112\n",
      "New best accuracy: 0.9130 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1669, val loss=0.2591, train_acc=0.9396, val_acc=0.9015, val_f1=0.8959\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1512, val loss=0.3112, train_acc=0.9480, val_acc=0.9170, val_f1=0.9170\n",
      "New best accuracy: 0.9170 at epoch 13, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.2430, val loss=0.3577, train_acc=0.9024, val_acc=0.8820, val_f1=0.8868\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.2202, val loss=0.3339, train_acc=0.9173, val_acc=0.9050, val_f1=0.9074\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 16: train loss=0.1941, val loss=0.2628, train_acc=0.9285, val_acc=0.9100, val_f1=0.9075\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 16.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6023, val loss=0.5663, train_acc=0.6739, val_acc=0.7495, val_f1=0.6894\n",
      "New best accuracy: 0.7495 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4817, val loss=0.4796, train_acc=0.8060, val_acc=0.8205, val_f1=0.8041\n",
      "New best accuracy: 0.8205 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4197, val loss=0.4492, train_acc=0.8400, val_acc=0.8270, val_f1=0.8043\n",
      "New best accuracy: 0.8270 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3762, val loss=0.3529, train_acc=0.8568, val_acc=0.8705, val_f1=0.8675\n",
      "New best accuracy: 0.8705 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3485, val loss=0.3117, train_acc=0.8638, val_acc=0.8850, val_f1=0.8824\n",
      "New best accuracy: 0.8850 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3292, val loss=0.3686, train_acc=0.8764, val_acc=0.8635, val_f1=0.8479\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2837, val loss=0.3187, train_acc=0.8940, val_acc=0.8665, val_f1=0.8524\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.2543, val loss=0.2571, train_acc=0.9040, val_acc=0.9090, val_f1=0.9074\n",
      "New best accuracy: 0.9090 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2432, val loss=0.2475, train_acc=0.9091, val_acc=0.9185, val_f1=0.9183\n",
      "New best accuracy: 0.9185 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2025, val loss=0.2466, train_acc=0.9240, val_acc=0.9175, val_f1=0.9165\n",
      "Epoch 11: train loss=0.1833, val loss=0.2484, train_acc=0.9324, val_acc=0.9190, val_f1=0.9192\n",
      "New best accuracy: 0.9190 at epoch 11, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1781, val loss=0.2472, train_acc=0.9364, val_acc=0.9145, val_f1=0.9141\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1773, val loss=0.3410, train_acc=0.9370, val_acc=0.9020, val_f1=0.9079\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1487, val loss=0.2738, train_acc=0.9461, val_acc=0.9215, val_f1=0.9221\n",
      "New best accuracy: 0.9215 at epoch 14, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1607, val loss=0.2931, train_acc=0.9441, val_acc=0.9105, val_f1=0.9072\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6200, val loss=0.5339, train_acc=0.6605, val_acc=0.7965, val_f1=0.7825\n",
      "New best accuracy: 0.7965 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4781, val loss=0.4980, train_acc=0.8034, val_acc=0.8030, val_f1=0.8249\n",
      "New best accuracy: 0.8030 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4155, val loss=0.3916, train_acc=0.8377, val_acc=0.8500, val_f1=0.8384\n",
      "New best accuracy: 0.8500 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3658, val loss=0.3183, train_acc=0.8599, val_acc=0.8875, val_f1=0.8873\n",
      "New best accuracy: 0.8875 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3300, val loss=0.3234, train_acc=0.8734, val_acc=0.8785, val_f1=0.8697\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2754, val loss=0.2829, train_acc=0.8960, val_acc=0.8990, val_f1=0.8942\n",
      "New best accuracy: 0.8990 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2505, val loss=0.2412, train_acc=0.9103, val_acc=0.9115, val_f1=0.9107\n",
      "New best accuracy: 0.9115 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2224, val loss=0.2446, train_acc=0.9151, val_acc=0.9125, val_f1=0.9121\n",
      "New best accuracy: 0.9125 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2127, val loss=0.2392, train_acc=0.9233, val_acc=0.9135, val_f1=0.9119\n",
      "New best accuracy: 0.9135 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1928, val loss=0.4320, train_acc=0.9293, val_acc=0.8440, val_f1=0.8173\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1836, val loss=0.2598, train_acc=0.9340, val_acc=0.9135, val_f1=0.9150\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1794, val loss=0.2383, train_acc=0.9320, val_acc=0.9175, val_f1=0.9159\n",
      "New best accuracy: 0.9175 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1465, val loss=0.2556, train_acc=0.9494, val_acc=0.9175, val_f1=0.9170\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1440, val loss=0.2763, train_acc=0.9495, val_acc=0.9140, val_f1=0.9116\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1180, val loss=0.2685, train_acc=0.9575, val_acc=0.9070, val_f1=0.9041\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.1076, val loss=0.3134, train_acc=0.9620, val_acc=0.9225, val_f1=0.9228\n",
      "New best accuracy: 0.9225 at epoch 16, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.0991, val loss=0.3121, train_acc=0.9671, val_acc=0.9185, val_f1=0.9189\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for num_channels=50: [0.8040199875831604, 0.8075000047683716, 0.8144400119781494]\n",
      "Test F1 Scores for num_channels=50: [0.8032265602977606, 0.807193665516769, 0.8141561706807099]\n",
      "Average Test Accuracy: 0.8087, Average Test F1: 0.8082\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing num_channels: 100 with lr=0.0005, optimiser=adamw, weight_Decay=0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5948, val loss=0.4801, train_acc=0.6961, val_acc=0.7975, val_f1=0.8155\n",
      "New best accuracy: 0.7975 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4672, val loss=0.4295, train_acc=0.8056, val_acc=0.8195, val_f1=0.8366\n",
      "New best accuracy: 0.8195 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3746, val loss=0.3585, train_acc=0.8535, val_acc=0.8740, val_f1=0.8807\n",
      "New best accuracy: 0.8740 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3323, val loss=0.3131, train_acc=0.8705, val_acc=0.8945, val_f1=0.8932\n",
      "New best accuracy: 0.8945 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2927, val loss=0.3372, train_acc=0.8916, val_acc=0.8610, val_f1=0.8464\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2479, val loss=0.3061, train_acc=0.9107, val_acc=0.8830, val_f1=0.8717\n",
      "Epoch 07: train loss=0.2161, val loss=0.2469, train_acc=0.9227, val_acc=0.9125, val_f1=0.9096\n",
      "New best accuracy: 0.9125 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1835, val loss=0.2522, train_acc=0.9323, val_acc=0.9180, val_f1=0.9175\n",
      "New best accuracy: 0.9180 at epoch 8, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1862, val loss=0.2542, train_acc=0.9327, val_acc=0.9155, val_f1=0.9129\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1501, val loss=0.2614, train_acc=0.9464, val_acc=0.9180, val_f1=0.9178\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1397, val loss=0.3163, train_acc=0.9499, val_acc=0.9090, val_f1=0.9135\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.1389, val loss=0.2694, train_acc=0.9515, val_acc=0.9220, val_f1=0.9218\n",
      "New best accuracy: 0.9220 at epoch 12, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5907, val loss=0.4800, train_acc=0.6905, val_acc=0.7920, val_f1=0.8145\n",
      "New best accuracy: 0.7920 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4414, val loss=0.4297, train_acc=0.8255, val_acc=0.8270, val_f1=0.8433\n",
      "New best accuracy: 0.8270 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3616, val loss=0.3310, train_acc=0.8592, val_acc=0.8715, val_f1=0.8638\n",
      "New best accuracy: 0.8715 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3379, val loss=0.2978, train_acc=0.8672, val_acc=0.8940, val_f1=0.8935\n",
      "New best accuracy: 0.8940 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2834, val loss=0.2880, train_acc=0.8925, val_acc=0.9095, val_f1=0.9104\n",
      "New best accuracy: 0.9095 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2630, val loss=0.3056, train_acc=0.8992, val_acc=0.8735, val_f1=0.8591\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2279, val loss=0.3049, train_acc=0.9174, val_acc=0.9030, val_f1=0.9063\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.2084, val loss=0.2566, train_acc=0.9236, val_acc=0.9120, val_f1=0.9109\n",
      "New best accuracy: 0.9120 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1799, val loss=0.3298, train_acc=0.9325, val_acc=0.9070, val_f1=0.9115\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1698, val loss=0.2620, train_acc=0.9381, val_acc=0.9190, val_f1=0.9185\n",
      "New best accuracy: 0.9190 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1646, val loss=0.2522, train_acc=0.9443, val_acc=0.9240, val_f1=0.9237\n",
      "New best accuracy: 0.9240 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1387, val loss=0.2640, train_acc=0.9494, val_acc=0.9205, val_f1=0.9172\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1261, val loss=0.2816, train_acc=0.9551, val_acc=0.9195, val_f1=0.9189\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1337, val loss=0.2487, train_acc=0.9529, val_acc=0.9145, val_f1=0.9117\n",
      "Epoch 15: train loss=0.1023, val loss=0.2905, train_acc=0.9673, val_acc=0.9185, val_f1=0.9184\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.0834, val loss=0.3713, train_acc=0.9715, val_acc=0.9070, val_f1=0.9021\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.0802, val loss=0.3657, train_acc=0.9728, val_acc=0.9190, val_f1=0.9210\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0670, val loss=0.3579, train_acc=0.9786, val_acc=0.9185, val_f1=0.9168\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0594, val loss=0.2966, train_acc=0.9804, val_acc=0.9005, val_f1=0.8949\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6085, val loss=0.4870, train_acc=0.6727, val_acc=0.8155, val_f1=0.8285\n",
      "New best accuracy: 0.8155 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4319, val loss=0.3559, train_acc=0.8317, val_acc=0.8660, val_f1=0.8659\n",
      "New best accuracy: 0.8660 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3563, val loss=0.3427, train_acc=0.8584, val_acc=0.8700, val_f1=0.8585\n",
      "New best accuracy: 0.8700 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3318, val loss=0.2753, train_acc=0.8709, val_acc=0.8955, val_f1=0.8972\n",
      "New best accuracy: 0.8955 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2444, val loss=0.2724, train_acc=0.9044, val_acc=0.8980, val_f1=0.9012\n",
      "New best accuracy: 0.8980 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2249, val loss=0.2448, train_acc=0.9104, val_acc=0.9085, val_f1=0.9085\n",
      "New best accuracy: 0.9085 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2088, val loss=0.2731, train_acc=0.9149, val_acc=0.8970, val_f1=0.8897\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.1962, val loss=0.2354, train_acc=0.9264, val_acc=0.9225, val_f1=0.9225\n",
      "New best accuracy: 0.9225 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.1736, val loss=0.2384, train_acc=0.9321, val_acc=0.9205, val_f1=0.9197\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1998, val loss=0.2585, train_acc=0.9229, val_acc=0.9130, val_f1=0.9109\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1652, val loss=0.2387, train_acc=0.9365, val_acc=0.9105, val_f1=0.9086\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1348, val loss=0.3825, train_acc=0.9513, val_acc=0.9000, val_f1=0.9064\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.1214, val loss=0.3929, train_acc=0.9543, val_acc=0.8970, val_f1=0.9039\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for num_channels=100: [0.8103799819946289, 0.8052200078964233, 0.8089600205421448]\n",
      "Test F1 Scores for num_channels=100: [0.8103533243744652, 0.8041270110933318, 0.808792924481137]\n",
      "Average Test Accuracy: 0.8082, Average Test F1: 0.8078\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing num_channels: 150 with lr=0.0005, optimiser=adamw, weight_Decay=0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5692, val loss=0.4277, train_acc=0.7145, val_acc=0.8360, val_f1=0.8363\n",
      "New best accuracy: 0.8360 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4392, val loss=0.3911, train_acc=0.8260, val_acc=0.8510, val_f1=0.8623\n",
      "New best accuracy: 0.8510 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3488, val loss=0.3283, train_acc=0.8630, val_acc=0.8815, val_f1=0.8855\n",
      "New best accuracy: 0.8815 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3049, val loss=0.2860, train_acc=0.8882, val_acc=0.8895, val_f1=0.8819\n",
      "New best accuracy: 0.8895 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2503, val loss=0.2563, train_acc=0.9075, val_acc=0.9130, val_f1=0.9119\n",
      "New best accuracy: 0.9130 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2254, val loss=0.3054, train_acc=0.9201, val_acc=0.8975, val_f1=0.9021\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1983, val loss=0.2470, train_acc=0.9244, val_acc=0.9175, val_f1=0.9174\n",
      "New best accuracy: 0.9175 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1660, val loss=0.3102, train_acc=0.9423, val_acc=0.9160, val_f1=0.9192\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1470, val loss=0.2523, train_acc=0.9480, val_acc=0.9185, val_f1=0.9207\n",
      "New best accuracy: 0.9185 at epoch 9, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1447, val loss=0.4900, train_acc=0.9505, val_acc=0.8440, val_f1=0.8182\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1162, val loss=0.2732, train_acc=0.9619, val_acc=0.9150, val_f1=0.9110\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.1214, val loss=0.2759, train_acc=0.9561, val_acc=0.9200, val_f1=0.9216\n",
      "New best accuracy: 0.9200 at epoch 12, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5868, val loss=0.4856, train_acc=0.6993, val_acc=0.8125, val_f1=0.8266\n",
      "New best accuracy: 0.8125 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4399, val loss=0.3792, train_acc=0.8316, val_acc=0.8535, val_f1=0.8444\n",
      "New best accuracy: 0.8535 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3450, val loss=0.3069, train_acc=0.8649, val_acc=0.8860, val_f1=0.8895\n",
      "New best accuracy: 0.8860 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2907, val loss=0.2682, train_acc=0.8909, val_acc=0.9005, val_f1=0.8968\n",
      "New best accuracy: 0.9005 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2628, val loss=0.2532, train_acc=0.9025, val_acc=0.9170, val_f1=0.9160\n",
      "New best accuracy: 0.9170 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2263, val loss=0.2562, train_acc=0.9183, val_acc=0.9075, val_f1=0.9028\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1954, val loss=0.2783, train_acc=0.9265, val_acc=0.9090, val_f1=0.9111\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1807, val loss=0.2440, train_acc=0.9317, val_acc=0.9165, val_f1=0.9152\n",
      "Epoch 09: train loss=0.1626, val loss=0.2727, train_acc=0.9396, val_acc=0.9190, val_f1=0.9192\n",
      "New best accuracy: 0.9190 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1386, val loss=0.2769, train_acc=0.9515, val_acc=0.9225, val_f1=0.9238\n",
      "New best accuracy: 0.9225 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1279, val loss=0.2823, train_acc=0.9549, val_acc=0.9200, val_f1=0.9207\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1239, val loss=0.2987, train_acc=0.9561, val_acc=0.9145, val_f1=0.9166\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.0954, val loss=0.2944, train_acc=0.9677, val_acc=0.9230, val_f1=0.9233\n",
      "New best accuracy: 0.9230 at epoch 13, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5848, val loss=0.4379, train_acc=0.6933, val_acc=0.8340, val_f1=0.8407\n",
      "New best accuracy: 0.8340 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4240, val loss=0.3899, train_acc=0.8344, val_acc=0.8445, val_f1=0.8287\n",
      "New best accuracy: 0.8445 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3548, val loss=0.3257, train_acc=0.8625, val_acc=0.8765, val_f1=0.8824\n",
      "New best accuracy: 0.8765 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3904, val loss=0.2838, train_acc=0.8526, val_acc=0.9105, val_f1=0.9110\n",
      "New best accuracy: 0.9105 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2781, val loss=0.2593, train_acc=0.8956, val_acc=0.9055, val_f1=0.9081\n",
      "Epoch 06: train loss=0.3179, val loss=0.5301, train_acc=0.8811, val_acc=0.7370, val_f1=0.7746\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.3158, val loss=0.2688, train_acc=0.8818, val_acc=0.9045, val_f1=0.8998\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.2427, val loss=0.2575, train_acc=0.9067, val_acc=0.9040, val_f1=0.8984\n",
      "Epoch 09: train loss=0.2088, val loss=0.2762, train_acc=0.9215, val_acc=0.9055, val_f1=0.9010\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1961, val loss=0.2591, train_acc=0.9293, val_acc=0.9165, val_f1=0.9171\n",
      "New best accuracy: 0.9165 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1782, val loss=0.2429, train_acc=0.9360, val_acc=0.9110, val_f1=0.9084\n",
      "Epoch 12: train loss=0.1637, val loss=0.2493, train_acc=0.9409, val_acc=0.9080, val_f1=0.9036\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1457, val loss=0.2706, train_acc=0.9456, val_acc=0.9150, val_f1=0.9179\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1361, val loss=0.2282, train_acc=0.9495, val_acc=0.9150, val_f1=0.9126\n",
      "Epoch 15: train loss=0.1085, val loss=0.3899, train_acc=0.9621, val_acc=0.8715, val_f1=0.8567\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1161, val loss=0.2663, train_acc=0.9585, val_acc=0.8955, val_f1=0.8882\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.0832, val loss=0.2513, train_acc=0.9710, val_acc=0.9175, val_f1=0.9159\n",
      "New best accuracy: 0.9175 at epoch 17, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0906, val loss=0.2606, train_acc=0.9694, val_acc=0.9215, val_f1=0.9191\n",
      "New best accuracy: 0.9215 at epoch 18, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0623, val loss=0.3036, train_acc=0.9795, val_acc=0.9115, val_f1=0.9083\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for num_channels=150: [0.8058199882507324, 0.809440016746521, 0.7808799743652344]\n",
      "Test F1 Scores for num_channels=150: [0.8052833121290557, 0.8092089444000262, 0.7767952007516188]\n",
      "Average Test Accuracy: 0.7987, Average Test F1: 0.7971\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing num_channels: 200 with lr=0.0005, optimiser=adamw, weight_Decay=0.001\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5896, val loss=0.4863, train_acc=0.6979, val_acc=0.8220, val_f1=0.8053\n",
      "New best accuracy: 0.8220 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4283, val loss=0.4403, train_acc=0.8314, val_acc=0.8355, val_f1=0.8121\n",
      "New best accuracy: 0.8355 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3362, val loss=0.3182, train_acc=0.8676, val_acc=0.8790, val_f1=0.8725\n",
      "New best accuracy: 0.8790 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2739, val loss=0.3587, train_acc=0.8994, val_acc=0.8645, val_f1=0.8751\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2353, val loss=0.2477, train_acc=0.9105, val_acc=0.9160, val_f1=0.9138\n",
      "New best accuracy: 0.9160 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2146, val loss=0.2759, train_acc=0.9224, val_acc=0.9170, val_f1=0.9143\n",
      "New best accuracy: 0.9170 at epoch 6, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1931, val loss=0.3087, train_acc=0.9249, val_acc=0.9015, val_f1=0.9067\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1633, val loss=0.2516, train_acc=0.9393, val_acc=0.9145, val_f1=0.9111\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 09: train loss=0.1434, val loss=0.2899, train_acc=0.9500, val_acc=0.8995, val_f1=0.8926\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 10: train loss=0.1555, val loss=0.2485, train_acc=0.9449, val_acc=0.9105, val_f1=0.9066\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 10.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5579, val loss=0.4757, train_acc=0.7258, val_acc=0.8070, val_f1=0.8093\n",
      "New best accuracy: 0.8070 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4210, val loss=0.4586, train_acc=0.8370, val_acc=0.8385, val_f1=0.8222\n",
      "New best accuracy: 0.8385 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3493, val loss=0.3114, train_acc=0.8680, val_acc=0.8940, val_f1=0.8927\n",
      "New best accuracy: 0.8940 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3032, val loss=0.2760, train_acc=0.8864, val_acc=0.9075, val_f1=0.9073\n",
      "New best accuracy: 0.9075 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2562, val loss=0.2582, train_acc=0.9039, val_acc=0.9045, val_f1=0.9003\n",
      "Epoch 06: train loss=0.2192, val loss=0.2409, train_acc=0.9189, val_acc=0.9175, val_f1=0.9146\n",
      "New best accuracy: 0.9175 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.1945, val loss=0.2941, train_acc=0.9269, val_acc=0.9115, val_f1=0.9140\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.1758, val loss=0.2479, train_acc=0.9340, val_acc=0.9150, val_f1=0.9138\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.1491, val loss=0.2767, train_acc=0.9486, val_acc=0.9100, val_f1=0.9131\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 10: train loss=0.1387, val loss=0.2551, train_acc=0.9500, val_acc=0.9205, val_f1=0.9200\n",
      "New best accuracy: 0.9205 at epoch 10, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 11: train loss=0.1183, val loss=0.2242, train_acc=0.9577, val_acc=0.9175, val_f1=0.9153\n",
      "Epoch 12: train loss=0.0985, val loss=0.2295, train_acc=0.9670, val_acc=0.9175, val_f1=0.9171\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.0966, val loss=0.2547, train_acc=0.9663, val_acc=0.9190, val_f1=0.9186\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.0697, val loss=0.2898, train_acc=0.9776, val_acc=0.9185, val_f1=0.9156\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.0583, val loss=0.3315, train_acc=0.9820, val_acc=0.9235, val_f1=0.9232\n",
      "New best accuracy: 0.9235 at epoch 15, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 16: train loss=0.0748, val loss=0.3067, train_acc=0.9765, val_acc=0.9150, val_f1=0.9159\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 16.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5915, val loss=0.4605, train_acc=0.6874, val_acc=0.8180, val_f1=0.8250\n",
      "New best accuracy: 0.8180 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4380, val loss=0.3867, train_acc=0.8321, val_acc=0.8435, val_f1=0.8549\n",
      "New best accuracy: 0.8435 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3779, val loss=0.3590, train_acc=0.8516, val_acc=0.8690, val_f1=0.8578\n",
      "New best accuracy: 0.8690 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2954, val loss=0.2599, train_acc=0.8840, val_acc=0.9095, val_f1=0.9084\n",
      "New best accuracy: 0.9095 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2564, val loss=0.2574, train_acc=0.9051, val_acc=0.9050, val_f1=0.9046\n",
      "Epoch 06: train loss=0.2332, val loss=0.2622, train_acc=0.9135, val_acc=0.9070, val_f1=0.9100\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.1900, val loss=0.2597, train_acc=0.9306, val_acc=0.9165, val_f1=0.9181\n",
      "New best accuracy: 0.9165 at epoch 7, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.1769, val loss=0.2296, train_acc=0.9377, val_acc=0.9150, val_f1=0.9125\n",
      "Epoch 09: train loss=0.1357, val loss=0.2574, train_acc=0.9523, val_acc=0.9190, val_f1=0.9184\n",
      "New best accuracy: 0.9190 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1095, val loss=0.3204, train_acc=0.9645, val_acc=0.9150, val_f1=0.9168\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1202, val loss=0.3119, train_acc=0.9590, val_acc=0.9110, val_f1=0.9091\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.0764, val loss=0.2998, train_acc=0.9746, val_acc=0.8980, val_f1=0.8919\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.0754, val loss=0.5547, train_acc=0.9750, val_acc=0.8625, val_f1=0.8440\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 13.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for num_channels=200: [0.7770199775695801, 0.8008000254631042, 0.7981200218200684]\n",
      "Test F1 Scores for num_channels=200: [0.7715817053151794, 0.7999527184634775, 0.7963032462527275]\n",
      "Average Test Accuracy: 0.7920, Average Test F1: 0.7893\n",
      "\n",
      "\n",
      "========================================\n",
      "Best num_channels: 50 with average test accuracy: 0.8087\n"
     ]
    }
   ],
   "source": [
    "cnn_channels_list = [50, 100, 150, 200]\n",
    "\n",
    "best_avg_acc_ch = 0.0\n",
    "best_ch = None\n",
    "all_ch_results = {}\n",
    "\n",
    "for ch in cnn_channels_list:\n",
    "    print(f\"\\n{'='*40}\\nTesting num_channels: {ch} with lr={best_lr}, optimiser={best_optimiser}, weight_Decay={best_weight_decay}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr, optimiser=best_optimiser, weight_decay=best_weight_decay,\n",
    "        cnn_channels=ch, kernel_sizes=(3,4,5), method=\"avg\"\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for num_channels={ch}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for num_channels={ch}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_ch_results[ch] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_ch:\n",
    "        best_avg_acc_ch = avg_acc\n",
    "        best_ch = ch\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest num_channels: {best_ch} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_ch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f876f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing kernel_size: (2, 3, 4) with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6399, val loss=0.5704, train_acc=0.6382, val_acc=0.7060, val_f1=0.7536\n",
      "New best accuracy: 0.7060 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5013, val loss=0.4335, train_acc=0.7894, val_acc=0.8385, val_f1=0.8371\n",
      "New best accuracy: 0.8385 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3959, val loss=0.3973, train_acc=0.8436, val_acc=0.8440, val_f1=0.8276\n",
      "New best accuracy: 0.8440 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3545, val loss=0.3279, train_acc=0.8635, val_acc=0.8760, val_f1=0.8679\n",
      "New best accuracy: 0.8760 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3123, val loss=0.2922, train_acc=0.8866, val_acc=0.8965, val_f1=0.8938\n",
      "New best accuracy: 0.8965 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2863, val loss=0.2731, train_acc=0.8914, val_acc=0.9010, val_f1=0.8977\n",
      "New best accuracy: 0.9010 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2560, val loss=0.2590, train_acc=0.9036, val_acc=0.9095, val_f1=0.9059\n",
      "New best accuracy: 0.9095 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2179, val loss=0.2474, train_acc=0.9161, val_acc=0.9085, val_f1=0.9049\n",
      "Epoch 09: train loss=0.2106, val loss=0.2971, train_acc=0.9243, val_acc=0.9080, val_f1=0.9030\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1999, val loss=0.2423, train_acc=0.9294, val_acc=0.9215, val_f1=0.9202\n",
      "New best accuracy: 0.9215 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1830, val loss=0.2436, train_acc=0.9326, val_acc=0.9215, val_f1=0.9220\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1687, val loss=0.2309, train_acc=0.9374, val_acc=0.9210, val_f1=0.9206\n",
      "Epoch 13: train loss=0.1559, val loss=0.2445, train_acc=0.9405, val_acc=0.9160, val_f1=0.9129\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1493, val loss=0.2534, train_acc=0.9450, val_acc=0.9200, val_f1=0.9172\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1371, val loss=0.3417, train_acc=0.9519, val_acc=0.8940, val_f1=0.8848\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.1321, val loss=0.2468, train_acc=0.9521, val_acc=0.9195, val_f1=0.9194\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.1170, val loss=0.2828, train_acc=0.9603, val_acc=0.9185, val_f1=0.9175\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6433, val loss=0.5881, train_acc=0.6359, val_acc=0.6675, val_f1=0.7405\n",
      "New best accuracy: 0.6675 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.5167, val loss=0.4724, train_acc=0.7761, val_acc=0.7690, val_f1=0.8017\n",
      "New best accuracy: 0.7690 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4332, val loss=0.4159, train_acc=0.8303, val_acc=0.8435, val_f1=0.8304\n",
      "New best accuracy: 0.8435 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3910, val loss=0.3901, train_acc=0.8465, val_acc=0.8480, val_f1=0.8599\n",
      "New best accuracy: 0.8480 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3554, val loss=0.3261, train_acc=0.8664, val_acc=0.8810, val_f1=0.8837\n",
      "New best accuracy: 0.8810 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3097, val loss=0.3179, train_acc=0.8834, val_acc=0.8875, val_f1=0.8936\n",
      "New best accuracy: 0.8875 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2765, val loss=0.3526, train_acc=0.9001, val_acc=0.8550, val_f1=0.8365\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2527, val loss=0.2533, train_acc=0.9061, val_acc=0.9125, val_f1=0.9105\n",
      "New best accuracy: 0.9125 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2326, val loss=0.2443, train_acc=0.9170, val_acc=0.9145, val_f1=0.9123\n",
      "New best accuracy: 0.9145 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2162, val loss=0.3061, train_acc=0.9231, val_acc=0.8745, val_f1=0.8617\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2070, val loss=0.2851, train_acc=0.9259, val_acc=0.8850, val_f1=0.8743\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1890, val loss=0.2373, train_acc=0.9329, val_acc=0.9210, val_f1=0.9205\n",
      "New best accuracy: 0.9210 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1804, val loss=0.2688, train_acc=0.9356, val_acc=0.9200, val_f1=0.9229\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1782, val loss=0.2561, train_acc=0.9370, val_acc=0.9095, val_f1=0.9049\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 15: train loss=0.1485, val loss=0.2431, train_acc=0.9471, val_acc=0.9165, val_f1=0.9160\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 16: train loss=0.1499, val loss=0.2603, train_acc=0.9457, val_acc=0.9170, val_f1=0.9153\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 17: train loss=0.1420, val loss=0.2427, train_acc=0.9487, val_acc=0.9180, val_f1=0.9159\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 17.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6184, val loss=0.5045, train_acc=0.6634, val_acc=0.7850, val_f1=0.8007\n",
      "New best accuracy: 0.7850 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4855, val loss=0.4524, train_acc=0.8129, val_acc=0.8345, val_f1=0.8240\n",
      "New best accuracy: 0.8345 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4161, val loss=0.3624, train_acc=0.8414, val_acc=0.8705, val_f1=0.8663\n",
      "New best accuracy: 0.8705 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3461, val loss=0.3815, train_acc=0.8685, val_acc=0.8525, val_f1=0.8345\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3129, val loss=0.4849, train_acc=0.8842, val_acc=0.8010, val_f1=0.7549\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.2854, val loss=0.2622, train_acc=0.8964, val_acc=0.9040, val_f1=0.9007\n",
      "New best accuracy: 0.9040 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2736, val loss=0.2848, train_acc=0.8985, val_acc=0.9050, val_f1=0.9000\n",
      "New best accuracy: 0.9050 at epoch 7, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2521, val loss=0.2840, train_acc=0.9103, val_acc=0.8960, val_f1=0.8884\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.2088, val loss=0.2552, train_acc=0.9260, val_acc=0.9180, val_f1=0.9187\n",
      "New best accuracy: 0.9180 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2077, val loss=0.2366, train_acc=0.9256, val_acc=0.9250, val_f1=0.9245\n",
      "New best accuracy: 0.9250 at epoch 10, saving model.\n",
      "Epoch 11: train loss=0.1870, val loss=0.2604, train_acc=0.9351, val_acc=0.9260, val_f1=0.9273\n",
      "New best accuracy: 0.9260 at epoch 11, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 12: train loss=0.1872, val loss=0.3262, train_acc=0.9343, val_acc=0.8905, val_f1=0.8805\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 13: train loss=0.1649, val loss=0.2864, train_acc=0.9434, val_acc=0.9120, val_f1=0.9161\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 14: train loss=0.1514, val loss=0.2828, train_acc=0.9471, val_acc=0.9220, val_f1=0.9206\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 15: train loss=0.1540, val loss=0.2489, train_acc=0.9444, val_acc=0.9210, val_f1=0.9183\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 15.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for kernel_size=(2, 3, 4): [0.7918800115585327, 0.8103200197219849, 0.8128200173377991]\n",
      "Test F1 Scores for kernel_size=(2, 3, 4): [0.7892093342031742, 0.8097791979989651, 0.8128192661766511]\n",
      "Average Test Accuracy: 0.8050, Average Test F1: 0.8039\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing kernel_size: (3, 4, 5) with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6037, val loss=0.5334, train_acc=0.6655, val_acc=0.7840, val_f1=0.7509\n",
      "New best accuracy: 0.7840 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4614, val loss=0.4389, train_acc=0.8216, val_acc=0.8260, val_f1=0.8395\n",
      "New best accuracy: 0.8260 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3940, val loss=0.3533, train_acc=0.8475, val_acc=0.8675, val_f1=0.8694\n",
      "New best accuracy: 0.8675 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4381, val loss=0.3818, train_acc=0.8230, val_acc=0.8585, val_f1=0.8609\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3662, val loss=0.3260, train_acc=0.8558, val_acc=0.8815, val_f1=0.8816\n",
      "New best accuracy: 0.8815 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3182, val loss=0.3249, train_acc=0.8769, val_acc=0.8690, val_f1=0.8548\n",
      "Epoch 07: train loss=0.2916, val loss=0.2709, train_acc=0.8884, val_acc=0.9065, val_f1=0.9062\n",
      "New best accuracy: 0.9065 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2557, val loss=0.2943, train_acc=0.9056, val_acc=0.9045, val_f1=0.9056\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2370, val loss=0.2515, train_acc=0.9140, val_acc=0.9090, val_f1=0.9070\n",
      "New best accuracy: 0.9090 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2158, val loss=0.2516, train_acc=0.9225, val_acc=0.9060, val_f1=0.9044\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1985, val loss=0.2654, train_acc=0.9284, val_acc=0.9115, val_f1=0.9127\n",
      "New best accuracy: 0.9115 at epoch 11, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1911, val loss=0.2447, train_acc=0.9323, val_acc=0.9120, val_f1=0.9092\n",
      "New best accuracy: 0.9120 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1788, val loss=0.2412, train_acc=0.9363, val_acc=0.9175, val_f1=0.9162\n",
      "New best accuracy: 0.9175 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1559, val loss=0.2529, train_acc=0.9435, val_acc=0.9145, val_f1=0.9146\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1600, val loss=0.2693, train_acc=0.9459, val_acc=0.9070, val_f1=0.9090\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1454, val loss=0.2826, train_acc=0.9503, val_acc=0.8835, val_f1=0.8732\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.1319, val loss=0.2614, train_acc=0.9557, val_acc=0.9095, val_f1=0.9079\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.1185, val loss=0.3079, train_acc=0.9595, val_acc=0.9105, val_f1=0.9087\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6141, val loss=0.4902, train_acc=0.6670, val_acc=0.8135, val_f1=0.8176\n",
      "New best accuracy: 0.8135 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4599, val loss=0.4094, train_acc=0.8176, val_acc=0.8330, val_f1=0.8216\n",
      "New best accuracy: 0.8330 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4071, val loss=0.4107, train_acc=0.8434, val_acc=0.8370, val_f1=0.8164\n",
      "New best accuracy: 0.8370 at epoch 3, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3482, val loss=0.3065, train_acc=0.8724, val_acc=0.8865, val_f1=0.8854\n",
      "New best accuracy: 0.8865 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3083, val loss=0.3081, train_acc=0.8814, val_acc=0.8980, val_f1=0.9012\n",
      "New best accuracy: 0.8980 at epoch 5, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2887, val loss=0.2862, train_acc=0.8924, val_acc=0.9030, val_f1=0.9027\n",
      "New best accuracy: 0.9030 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2412, val loss=0.2746, train_acc=0.9123, val_acc=0.9045, val_f1=0.9073\n",
      "New best accuracy: 0.9045 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2122, val loss=0.3126, train_acc=0.9237, val_acc=0.8740, val_f1=0.8605\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2093, val loss=0.3088, train_acc=0.9219, val_acc=0.8800, val_f1=0.8677\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1859, val loss=0.2883, train_acc=0.9310, val_acc=0.9135, val_f1=0.9158\n",
      "New best accuracy: 0.9135 at epoch 10, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.1700, val loss=0.2504, train_acc=0.9397, val_acc=0.9180, val_f1=0.9154\n",
      "New best accuracy: 0.9180 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1555, val loss=0.2662, train_acc=0.9427, val_acc=0.9010, val_f1=0.8950\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1646, val loss=0.2940, train_acc=0.9396, val_acc=0.8950, val_f1=0.8890\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1595, val loss=0.2595, train_acc=0.9464, val_acc=0.9165, val_f1=0.9175\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.1239, val loss=0.2502, train_acc=0.9574, val_acc=0.9145, val_f1=0.9114\n",
      "Epoch 16: train loss=0.1041, val loss=0.3308, train_acc=0.9661, val_acc=0.9175, val_f1=0.9196\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 17: train loss=0.1006, val loss=0.3051, train_acc=0.9640, val_acc=0.9205, val_f1=0.9225\n",
      "New best accuracy: 0.9205 at epoch 17, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 18: train loss=0.0942, val loss=0.2977, train_acc=0.9689, val_acc=0.9090, val_f1=0.9062\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 19: train loss=0.1115, val loss=0.3469, train_acc=0.9627, val_acc=0.9090, val_f1=0.9109\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 20: train loss=0.0854, val loss=0.3144, train_acc=0.9719, val_acc=0.9165, val_f1=0.9166\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 20.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6002, val loss=0.4807, train_acc=0.6774, val_acc=0.8045, val_f1=0.8194\n",
      "New best accuracy: 0.8045 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4486, val loss=0.4601, train_acc=0.8211, val_acc=0.8285, val_f1=0.8110\n",
      "New best accuracy: 0.8285 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3858, val loss=0.3632, train_acc=0.8492, val_acc=0.8780, val_f1=0.8749\n",
      "New best accuracy: 0.8780 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3361, val loss=0.3095, train_acc=0.8719, val_acc=0.8940, val_f1=0.8919\n",
      "New best accuracy: 0.8940 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2960, val loss=0.3286, train_acc=0.8860, val_acc=0.8755, val_f1=0.8658\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2547, val loss=0.2585, train_acc=0.9061, val_acc=0.9090, val_f1=0.9060\n",
      "New best accuracy: 0.9090 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2514, val loss=0.2663, train_acc=0.9113, val_acc=0.9075, val_f1=0.9111\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2225, val loss=0.2511, train_acc=0.9174, val_acc=0.9160, val_f1=0.9164\n",
      "New best accuracy: 0.9160 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2003, val loss=0.2449, train_acc=0.9287, val_acc=0.9145, val_f1=0.9127\n",
      "Epoch 10: train loss=0.1895, val loss=0.2491, train_acc=0.9325, val_acc=0.9215, val_f1=0.9223\n",
      "New best accuracy: 0.9215 at epoch 10, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1702, val loss=0.2726, train_acc=0.9411, val_acc=0.8920, val_f1=0.8837\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1575, val loss=0.3441, train_acc=0.9454, val_acc=0.8910, val_f1=0.8983\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1545, val loss=0.2753, train_acc=0.9456, val_acc=0.9170, val_f1=0.9162\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1346, val loss=0.2427, train_acc=0.9580, val_acc=0.9255, val_f1=0.9252\n",
      "New best accuracy: 0.9255 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1276, val loss=0.3167, train_acc=0.9531, val_acc=0.9200, val_f1=0.9188\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1320, val loss=0.2798, train_acc=0.9550, val_acc=0.9085, val_f1=0.9048\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.0990, val loss=0.3774, train_acc=0.9677, val_acc=0.8670, val_f1=0.8509\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0868, val loss=0.3734, train_acc=0.9721, val_acc=0.9140, val_f1=0.9137\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0917, val loss=0.2785, train_acc=0.9694, val_acc=0.9200, val_f1=0.9195\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for kernel_size=(3, 4, 5): [0.8095600008964539, 0.810699999332428, 0.8106600046157837]\n",
      "Test F1 Scores for kernel_size=(3, 4, 5): [0.8091275287896305, 0.8106952294440601, 0.8102658203354227]\n",
      "Average Test Accuracy: 0.8103, Average Test F1: 0.8100\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing kernel_size: (3, 5, 7) with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50\n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6069, val loss=0.5034, train_acc=0.6825, val_acc=0.7935, val_f1=0.8060\n",
      "New best accuracy: 0.7935 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4483, val loss=0.4094, train_acc=0.8161, val_acc=0.8395, val_f1=0.8277\n",
      "New best accuracy: 0.8395 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3988, val loss=0.4455, train_acc=0.8439, val_acc=0.8025, val_f1=0.7628\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3591, val loss=0.3323, train_acc=0.8585, val_acc=0.8740, val_f1=0.8672\n",
      "New best accuracy: 0.8740 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3176, val loss=0.3146, train_acc=0.8760, val_acc=0.8865, val_f1=0.8911\n",
      "New best accuracy: 0.8865 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2941, val loss=0.3443, train_acc=0.8889, val_acc=0.8660, val_f1=0.8514\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.2593, val loss=0.2822, train_acc=0.9045, val_acc=0.9030, val_f1=0.9058\n",
      "New best accuracy: 0.9030 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2401, val loss=0.2762, train_acc=0.9074, val_acc=0.9090, val_f1=0.9115\n",
      "New best accuracy: 0.9090 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2044, val loss=0.2720, train_acc=0.9269, val_acc=0.9130, val_f1=0.9123\n",
      "New best accuracy: 0.9130 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2022, val loss=0.3921, train_acc=0.9236, val_acc=0.8845, val_f1=0.8931\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1797, val loss=0.2443, train_acc=0.9384, val_acc=0.9190, val_f1=0.9184\n",
      "New best accuracy: 0.9190 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.1577, val loss=0.2545, train_acc=0.9434, val_acc=0.9145, val_f1=0.9133\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1360, val loss=0.2785, train_acc=0.9501, val_acc=0.9065, val_f1=0.9023\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1296, val loss=0.2956, train_acc=0.9517, val_acc=0.9120, val_f1=0.9091\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 15: train loss=0.1315, val loss=0.3321, train_acc=0.9553, val_acc=0.9020, val_f1=0.9074\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 16: train loss=0.1103, val loss=0.3101, train_acc=0.9605, val_acc=0.9095, val_f1=0.9105\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 16.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5744, val loss=0.4992, train_acc=0.7135, val_acc=0.8045, val_f1=0.7797\n",
      "New best accuracy: 0.8045 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4588, val loss=0.4084, train_acc=0.8181, val_acc=0.8320, val_f1=0.8412\n",
      "New best accuracy: 0.8320 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4071, val loss=0.3974, train_acc=0.8383, val_acc=0.8495, val_f1=0.8401\n",
      "New best accuracy: 0.8495 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3610, val loss=0.3273, train_acc=0.8566, val_acc=0.8800, val_f1=0.8812\n",
      "New best accuracy: 0.8800 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3330, val loss=0.3196, train_acc=0.8729, val_acc=0.8890, val_f1=0.8919\n",
      "New best accuracy: 0.8890 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3030, val loss=0.2912, train_acc=0.8888, val_acc=0.8885, val_f1=0.8836\n",
      "Epoch 07: train loss=0.2691, val loss=0.3046, train_acc=0.9022, val_acc=0.8815, val_f1=0.8717\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.2460, val loss=0.2647, train_acc=0.9101, val_acc=0.9015, val_f1=0.8980\n",
      "New best accuracy: 0.9015 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2178, val loss=0.2817, train_acc=0.9246, val_acc=0.8980, val_f1=0.8926\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.2072, val loss=0.2822, train_acc=0.9245, val_acc=0.9035, val_f1=0.8992\n",
      "New best accuracy: 0.9035 at epoch 10, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1999, val loss=0.2684, train_acc=0.9283, val_acc=0.9045, val_f1=0.9007\n",
      "New best accuracy: 0.9045 at epoch 11, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 12: train loss=0.1740, val loss=0.3034, train_acc=0.9363, val_acc=0.9075, val_f1=0.9040\n",
      "New best accuracy: 0.9075 at epoch 12, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 13: train loss=0.1831, val loss=0.2508, train_acc=0.9361, val_acc=0.9115, val_f1=0.9101\n",
      "New best accuracy: 0.9115 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1788, val loss=0.3037, train_acc=0.9383, val_acc=0.9095, val_f1=0.9111\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 15: train loss=0.1481, val loss=0.2585, train_acc=0.9470, val_acc=0.9060, val_f1=0.9076\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 16: train loss=0.1275, val loss=0.2854, train_acc=0.9556, val_acc=0.9130, val_f1=0.9110\n",
      "New best accuracy: 0.9130 at epoch 16, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 17: train loss=0.1207, val loss=0.2856, train_acc=0.9579, val_acc=0.9110, val_f1=0.9114\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 18: train loss=0.1082, val loss=0.2693, train_acc=0.9617, val_acc=0.9030, val_f1=0.9012\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 18.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6309, val loss=0.5620, train_acc=0.6699, val_acc=0.7235, val_f1=0.7731\n",
      "New best accuracy: 0.7235 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4877, val loss=0.4265, train_acc=0.7877, val_acc=0.8185, val_f1=0.8126\n",
      "New best accuracy: 0.8185 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4548, val loss=0.3952, train_acc=0.8117, val_acc=0.8550, val_f1=0.8583\n",
      "New best accuracy: 0.8550 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4136, val loss=0.3603, train_acc=0.8456, val_acc=0.8615, val_f1=0.8660\n",
      "New best accuracy: 0.8615 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3570, val loss=0.3392, train_acc=0.8659, val_acc=0.8745, val_f1=0.8802\n",
      "New best accuracy: 0.8745 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.3077, val loss=0.3155, train_acc=0.8832, val_acc=0.8840, val_f1=0.8762\n",
      "New best accuracy: 0.8840 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2680, val loss=0.2674, train_acc=0.9004, val_acc=0.9000, val_f1=0.8976\n",
      "New best accuracy: 0.9000 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2342, val loss=0.2542, train_acc=0.9144, val_acc=0.9095, val_f1=0.9096\n",
      "New best accuracy: 0.9095 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2303, val loss=0.2524, train_acc=0.9134, val_acc=0.9110, val_f1=0.9107\n",
      "New best accuracy: 0.9110 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2176, val loss=0.2529, train_acc=0.9196, val_acc=0.9105, val_f1=0.9114\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.1902, val loss=0.2541, train_acc=0.9323, val_acc=0.9055, val_f1=0.9033\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1831, val loss=0.2412, train_acc=0.9345, val_acc=0.9130, val_f1=0.9125\n",
      "New best accuracy: 0.9130 at epoch 12, saving model.\n",
      "Epoch 13: train loss=0.1669, val loss=0.3246, train_acc=0.9409, val_acc=0.8770, val_f1=0.8645\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 14: train loss=0.1551, val loss=0.2365, train_acc=0.9441, val_acc=0.9165, val_f1=0.9164\n",
      "New best accuracy: 0.9165 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1353, val loss=0.3013, train_acc=0.9524, val_acc=0.9120, val_f1=0.9136\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1131, val loss=0.2884, train_acc=0.9600, val_acc=0.9115, val_f1=0.9102\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.1085, val loss=0.2973, train_acc=0.9624, val_acc=0.9170, val_f1=0.9176\n",
      "New best accuracy: 0.9170 at epoch 17, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.0906, val loss=0.3805, train_acc=0.9701, val_acc=0.9125, val_f1=0.9130\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.1267, val loss=0.2865, train_acc=0.9545, val_acc=0.9200, val_f1=0.9206\n",
      "New best accuracy: 0.9200 at epoch 19, saving model.\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for kernel_size=(3, 5, 7): [0.800599992275238, 0.7935600280761719, 0.8053399920463562]\n",
      "Test F1 Scores for kernel_size=(3, 5, 7): [0.799300060389867, 0.7913411775013324, 0.8047050413286531]\n",
      "Average Test Accuracy: 0.7998, Average Test F1: 0.7984\n",
      "\n",
      "\n",
      "========================================\n",
      "Best kernel_size: (3, 4, 5) with average test accuracy: 0.8103\n"
     ]
    }
   ],
   "source": [
    "kernel_size_sets = [(2,3,4), (3,4,5), (3,5,7)]\n",
    "\n",
    "best_avg_acc_kernel_size = 0.0\n",
    "best_kernel_size = None\n",
    "all_kernel_size_results = {}\n",
    "\n",
    "for ks in kernel_size_sets:\n",
    "    print(f\"\\n{'='*40}\\nTesting kernel_size: {ks} with lr={best_lr}, optimiser={best_optimiser}, weight_Decay={best_weight_decay}, num_channels={best_ch}\\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr, optimiser=best_optimiser, weight_decay=best_weight_decay,\n",
    "        cnn_channels=best_ch, kernel_sizes=ks, method=\"avg\"\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for kernel_size={ks}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for kernel_size={ks}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_kernel_size_results[ks] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_kernel_size:\n",
    "        best_avg_acc_kernel_size = avg_acc\n",
    "        best_kernel_size = ks\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest kernel_size: {best_kernel_size} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_kernel_size:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "428344ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing method: last with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50, kernel_sze=(3, 4, 5) \n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6910, val loss=0.6903, train_acc=0.5008, val_acc=0.5000, val_f1=0.0000\n",
      "New best accuracy: 0.5000 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6896, val loss=0.6896, train_acc=0.5080, val_acc=0.5130, val_f1=0.6676\n",
      "New best accuracy: 0.5130 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.6896, val loss=0.6891, train_acc=0.5086, val_acc=0.5125, val_f1=0.6673\n",
      "Epoch 04: train loss=0.6885, val loss=0.6917, train_acc=0.5146, val_acc=0.5135, val_f1=0.6678\n",
      "New best accuracy: 0.5135 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.6866, val loss=0.6910, train_acc=0.5118, val_acc=0.5135, val_f1=0.6676\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.6862, val loss=0.6942, train_acc=0.5102, val_acc=0.5135, val_f1=0.6676\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 07: train loss=0.6860, val loss=0.6885, train_acc=0.5134, val_acc=0.5015, val_f1=0.0119\n",
      "Epoch 08: train loss=0.6836, val loss=0.6903, train_acc=0.5170, val_acc=0.5130, val_f1=0.6694\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.6830, val loss=0.6908, train_acc=0.5186, val_acc=0.5140, val_f1=0.6703\n",
      "New best accuracy: 0.5140 at epoch 9, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.6827, val loss=0.6911, train_acc=0.5171, val_acc=0.5125, val_f1=0.6701\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.6813, val loss=0.6925, train_acc=0.5171, val_acc=0.5145, val_f1=0.6689\n",
      "New best accuracy: 0.5145 at epoch 11, saving model.\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.6805, val loss=0.7029, train_acc=0.5134, val_acc=0.5140, val_f1=0.6678\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6913, val loss=0.6902, train_acc=0.5101, val_acc=0.5130, val_f1=0.6676\n",
      "New best accuracy: 0.5130 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6890, val loss=0.6909, train_acc=0.5128, val_acc=0.5000, val_f1=0.0000\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.6898, val loss=0.6897, train_acc=0.5084, val_acc=0.5130, val_f1=0.6676\n",
      "Epoch 04: train loss=0.6887, val loss=0.6900, train_acc=0.5096, val_acc=0.5135, val_f1=0.6678\n",
      "New best accuracy: 0.5135 at epoch 4, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.6881, val loss=0.6896, train_acc=0.5134, val_acc=0.5155, val_f1=0.6698\n",
      "New best accuracy: 0.5155 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.6871, val loss=0.6901, train_acc=0.5070, val_acc=0.5135, val_f1=0.6685\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 07: train loss=0.6854, val loss=0.6931, train_acc=0.5144, val_acc=0.5095, val_f1=0.6667\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 08: train loss=0.6824, val loss=0.6954, train_acc=0.5172, val_acc=0.5120, val_f1=0.6676\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 09: train loss=0.6831, val loss=0.6932, train_acc=0.5150, val_acc=0.5110, val_f1=0.6685\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 10: train loss=0.6819, val loss=0.6903, train_acc=0.5185, val_acc=0.5140, val_f1=0.6685\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 10.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.6898, val loss=0.6898, train_acc=0.4975, val_acc=0.5130, val_f1=0.6676\n",
      "New best accuracy: 0.5130 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6886, val loss=0.6911, train_acc=0.5129, val_acc=0.5135, val_f1=0.6678\n",
      "New best accuracy: 0.5135 at epoch 2, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.6896, val loss=0.6902, train_acc=0.5116, val_acc=0.5130, val_f1=0.6676\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 04: train loss=0.6895, val loss=0.6901, train_acc=0.5131, val_acc=0.5170, val_f1=0.6717\n",
      "New best accuracy: 0.5170 at epoch 4, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 05: train loss=0.6871, val loss=0.6903, train_acc=0.5202, val_acc=0.5135, val_f1=0.6680\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 06: train loss=0.6870, val loss=0.6895, train_acc=0.5101, val_acc=0.5145, val_f1=0.6696\n",
      "Epoch 07: train loss=0.6850, val loss=0.6934, train_acc=0.5108, val_acc=0.5095, val_f1=0.6680\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.6836, val loss=0.6913, train_acc=0.5152, val_acc=0.5125, val_f1=0.6680\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.6826, val loss=0.6928, train_acc=0.5161, val_acc=0.5110, val_f1=0.6685\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 10: train loss=0.6803, val loss=0.6953, train_acc=0.5144, val_acc=0.5105, val_f1=0.6682\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 11: train loss=0.6811, val loss=0.6932, train_acc=0.5138, val_acc=0.5080, val_f1=0.6678\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 11.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=last: [0.49772000312805176, 0.49751999974250793, 0.49834001064300537]\n",
      "Test F1 Scores for method=last: [0.35533851654514126, 0.35557902183158124, 0.3537408617848681]\n",
      "Average Test Accuracy: 0.4979, Average Test F1: 0.3549\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing method: avg with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50, kernel_sze=(3, 4, 5) \n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.6149, val loss=0.5616, train_acc=0.6640, val_acc=0.7605, val_f1=0.7343\n",
      "New best accuracy: 0.7605 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.6515, val loss=0.5949, train_acc=0.6538, val_acc=0.7105, val_f1=0.7166\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 03: train loss=0.4960, val loss=0.4522, train_acc=0.7880, val_acc=0.8305, val_f1=0.8268\n",
      "New best accuracy: 0.8305 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.4288, val loss=0.4047, train_acc=0.8347, val_acc=0.8410, val_f1=0.8319\n",
      "New best accuracy: 0.8410 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3951, val loss=0.4658, train_acc=0.8434, val_acc=0.8075, val_f1=0.8312\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.3573, val loss=0.3437, train_acc=0.8609, val_acc=0.8660, val_f1=0.8561\n",
      "New best accuracy: 0.8660 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.3236, val loss=0.2912, train_acc=0.8730, val_acc=0.8910, val_f1=0.8911\n",
      "New best accuracy: 0.8910 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2855, val loss=0.2774, train_acc=0.8931, val_acc=0.9025, val_f1=0.9005\n",
      "New best accuracy: 0.9025 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2639, val loss=0.2696, train_acc=0.9036, val_acc=0.9070, val_f1=0.9080\n",
      "New best accuracy: 0.9070 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2479, val loss=0.2594, train_acc=0.9116, val_acc=0.9050, val_f1=0.9007\n",
      "Epoch 11: train loss=0.2372, val loss=0.2529, train_acc=0.9103, val_acc=0.9100, val_f1=0.9061\n",
      "New best accuracy: 0.9100 at epoch 11, saving model.\n",
      "Epoch 12: train loss=0.2073, val loss=0.2859, train_acc=0.9230, val_acc=0.9050, val_f1=0.9084\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.2157, val loss=0.2547, train_acc=0.9184, val_acc=0.9105, val_f1=0.9113\n",
      "New best accuracy: 0.9105 at epoch 13, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 14: train loss=0.1830, val loss=0.2392, train_acc=0.9329, val_acc=0.9145, val_f1=0.9115\n",
      "New best accuracy: 0.9145 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1756, val loss=0.2654, train_acc=0.9357, val_acc=0.9065, val_f1=0.9090\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1610, val loss=0.2479, train_acc=0.9423, val_acc=0.9175, val_f1=0.9150\n",
      "New best accuracy: 0.9175 at epoch 16, saving model.\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.1476, val loss=0.2593, train_acc=0.9475, val_acc=0.9115, val_f1=0.9111\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.1468, val loss=0.2531, train_acc=0.9479, val_acc=0.9175, val_f1=0.9169\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.1105, val loss=0.3111, train_acc=0.9625, val_acc=0.9160, val_f1=0.9158\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.6068, val loss=0.5979, train_acc=0.6811, val_acc=0.6575, val_f1=0.7418\n",
      "New best accuracy: 0.6575 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4594, val loss=0.3977, train_acc=0.8194, val_acc=0.8475, val_f1=0.8509\n",
      "New best accuracy: 0.8475 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.3961, val loss=0.4452, train_acc=0.8433, val_acc=0.8165, val_f1=0.7837\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 04: train loss=0.3559, val loss=0.3240, train_acc=0.8658, val_acc=0.8795, val_f1=0.8831\n",
      "New best accuracy: 0.8795 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.3179, val loss=0.3319, train_acc=0.8766, val_acc=0.8850, val_f1=0.8894\n",
      "New best accuracy: 0.8850 at epoch 5, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.2812, val loss=0.3247, train_acc=0.8910, val_acc=0.8660, val_f1=0.8508\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 07: train loss=0.2586, val loss=0.2900, train_acc=0.9070, val_acc=0.8980, val_f1=0.9006\n",
      "New best accuracy: 0.8980 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2304, val loss=0.3143, train_acc=0.9169, val_acc=0.8875, val_f1=0.8780\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.2199, val loss=0.2494, train_acc=0.9236, val_acc=0.9190, val_f1=0.9173\n",
      "New best accuracy: 0.9190 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.2016, val loss=0.2855, train_acc=0.9263, val_acc=0.8860, val_f1=0.8761\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.2001, val loss=0.2508, train_acc=0.9264, val_acc=0.9180, val_f1=0.9178\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.1704, val loss=0.2603, train_acc=0.9407, val_acc=0.9145, val_f1=0.9117\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.1715, val loss=0.2616, train_acc=0.9383, val_acc=0.9125, val_f1=0.9153\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.1501, val loss=0.2817, train_acc=0.9480, val_acc=0.9155, val_f1=0.9160\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5925, val loss=0.5084, train_acc=0.6841, val_acc=0.8055, val_f1=0.7973\n",
      "New best accuracy: 0.8055 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.4717, val loss=0.4741, train_acc=0.8060, val_acc=0.8210, val_f1=0.8092\n",
      "New best accuracy: 0.8210 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.4068, val loss=0.3661, train_acc=0.8448, val_acc=0.8650, val_f1=0.8643\n",
      "New best accuracy: 0.8650 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.3622, val loss=0.4080, train_acc=0.8592, val_acc=0.8475, val_f1=0.8626\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.3338, val loss=0.3020, train_acc=0.8761, val_acc=0.9010, val_f1=0.8996\n",
      "New best accuracy: 0.9010 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.2907, val loss=0.2630, train_acc=0.8881, val_acc=0.9070, val_f1=0.9066\n",
      "New best accuracy: 0.9070 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.2604, val loss=0.2528, train_acc=0.9060, val_acc=0.9090, val_f1=0.9062\n",
      "New best accuracy: 0.9090 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.2434, val loss=0.2495, train_acc=0.9095, val_acc=0.9135, val_f1=0.9136\n",
      "New best accuracy: 0.9135 at epoch 8, saving model.\n",
      "Epoch 09: train loss=0.2129, val loss=0.2495, train_acc=0.9247, val_acc=0.9145, val_f1=0.9148\n",
      "New best accuracy: 0.9145 at epoch 9, saving model.\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 10: train loss=0.1944, val loss=0.2757, train_acc=0.9319, val_acc=0.9125, val_f1=0.9152\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 11: train loss=0.1814, val loss=0.2469, train_acc=0.9365, val_acc=0.9145, val_f1=0.9139\n",
      "Epoch 12: train loss=0.1715, val loss=0.2555, train_acc=0.9390, val_acc=0.9125, val_f1=0.9100\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 13: train loss=0.1733, val loss=0.2448, train_acc=0.9387, val_acc=0.9160, val_f1=0.9147\n",
      "New best accuracy: 0.9160 at epoch 13, saving model.\n",
      "Epoch 14: train loss=0.1721, val loss=0.2402, train_acc=0.9401, val_acc=0.9170, val_f1=0.9174\n",
      "New best accuracy: 0.9170 at epoch 14, saving model.\n",
      "Epoch 15: train loss=0.1402, val loss=0.2443, train_acc=0.9505, val_acc=0.9150, val_f1=0.9134\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 16: train loss=0.1298, val loss=0.2620, train_acc=0.9557, val_acc=0.9165, val_f1=0.9166\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 17: train loss=0.1063, val loss=0.2985, train_acc=0.9633, val_acc=0.9195, val_f1=0.9203\n",
      "New best accuracy: 0.9195 at epoch 17, saving model.\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 18: train loss=0.1086, val loss=0.2602, train_acc=0.9641, val_acc=0.9110, val_f1=0.9082\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 19: train loss=0.0992, val loss=0.3295, train_acc=0.9669, val_acc=0.9070, val_f1=0.9049\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 19.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=avg: [0.7638400197029114, 0.8005599975585938, 0.8145999908447266]\n",
      "Test F1 Scores for method=avg: [0.7574493276643258, 0.7995207153885745, 0.8144647447989585]\n",
      "Average Test Accuracy: 0.7930, Average Test F1: 0.7905\n",
      "\n",
      "\n",
      "========================================\n",
      "Testing method: max with lr=0.0005, optimiser=adamw, weight_Decay=0.001, num_channels=50, kernel_sze=(3, 4, 5) \n",
      "========================================\n",
      "\n",
      "=========================\n",
      "Starting training run 1\n",
      "=========================\n",
      "Epoch 01: train loss=0.5078, val loss=0.3956, train_acc=0.7356, val_acc=0.8325, val_f1=0.8456\n",
      "New best accuracy: 0.8325 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3389, val loss=0.2879, train_acc=0.8541, val_acc=0.8785, val_f1=0.8832\n",
      "New best accuracy: 0.8785 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2718, val loss=0.2632, train_acc=0.8860, val_acc=0.8925, val_f1=0.8875\n",
      "New best accuracy: 0.8925 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2313, val loss=0.2417, train_acc=0.9104, val_acc=0.9050, val_f1=0.9034\n",
      "New best accuracy: 0.9050 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2112, val loss=0.2541, train_acc=0.9160, val_acc=0.9045, val_f1=0.9057\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 06: train loss=0.1816, val loss=0.2269, train_acc=0.9261, val_acc=0.9120, val_f1=0.9107\n",
      "New best accuracy: 0.9120 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.1693, val loss=0.2222, train_acc=0.9359, val_acc=0.9180, val_f1=0.9171\n",
      "New best accuracy: 0.9180 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1431, val loss=0.2689, train_acc=0.9453, val_acc=0.8970, val_f1=0.8909\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1280, val loss=0.2451, train_acc=0.9524, val_acc=0.9055, val_f1=0.9038\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 10: train loss=0.1052, val loss=0.2568, train_acc=0.9623, val_acc=0.9015, val_f1=0.8982\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 11: train loss=0.0832, val loss=0.2690, train_acc=0.9706, val_acc=0.9140, val_f1=0.9126\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 12: train loss=0.0676, val loss=0.3081, train_acc=0.9776, val_acc=0.9020, val_f1=0.8995\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 12.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 2\n",
      "=========================\n",
      "Epoch 01: train loss=0.5395, val loss=0.3767, train_acc=0.7161, val_acc=0.8400, val_f1=0.8456\n",
      "New best accuracy: 0.8400 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3451, val loss=0.3060, train_acc=0.8538, val_acc=0.8760, val_f1=0.8674\n",
      "New best accuracy: 0.8760 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2719, val loss=0.2533, train_acc=0.8838, val_acc=0.8980, val_f1=0.8996\n",
      "New best accuracy: 0.8980 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2338, val loss=0.2362, train_acc=0.9085, val_acc=0.9090, val_f1=0.9095\n",
      "New best accuracy: 0.9090 at epoch 4, saving model.\n",
      "Epoch 05: train loss=0.2021, val loss=0.2332, train_acc=0.9215, val_acc=0.9045, val_f1=0.9025\n",
      "Epoch 06: train loss=0.1823, val loss=0.2182, train_acc=0.9291, val_acc=0.9190, val_f1=0.9196\n",
      "New best accuracy: 0.9190 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.1585, val loss=0.2378, train_acc=0.9397, val_acc=0.9010, val_f1=0.8964\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.1339, val loss=0.2289, train_acc=0.9501, val_acc=0.9165, val_f1=0.9171\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.1181, val loss=0.2476, train_acc=0.9566, val_acc=0.9080, val_f1=0.9110\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 10: train loss=0.0993, val loss=0.2554, train_acc=0.9637, val_acc=0.9045, val_f1=0.9087\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 11: train loss=0.0931, val loss=0.2731, train_acc=0.9681, val_acc=0.9045, val_f1=0.9081\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 11.\n",
      "Processed 32 samples...\n",
      "\n",
      "=========================\n",
      "Starting training run 3\n",
      "=========================\n",
      "Epoch 01: train loss=0.5039, val loss=0.3631, train_acc=0.7331, val_acc=0.8480, val_f1=0.8447\n",
      "New best accuracy: 0.8480 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3497, val loss=0.3055, train_acc=0.8515, val_acc=0.8755, val_f1=0.8692\n",
      "New best accuracy: 0.8755 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2843, val loss=0.2604, train_acc=0.8806, val_acc=0.8975, val_f1=0.8978\n",
      "New best accuracy: 0.8975 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2360, val loss=0.2673, train_acc=0.9060, val_acc=0.8925, val_f1=0.8988\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2145, val loss=0.2416, train_acc=0.9159, val_acc=0.9025, val_f1=0.9018\n",
      "New best accuracy: 0.9025 at epoch 5, saving model.\n",
      "Epoch 06: train loss=0.1808, val loss=0.2232, train_acc=0.9306, val_acc=0.9185, val_f1=0.9200\n",
      "New best accuracy: 0.9185 at epoch 6, saving model.\n",
      "Epoch 07: train loss=0.1567, val loss=0.3650, train_acc=0.9413, val_acc=0.8705, val_f1=0.8829\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 08: train loss=0.1468, val loss=0.2284, train_acc=0.9426, val_acc=0.9140, val_f1=0.9154\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 09: train loss=0.1213, val loss=0.2231, train_acc=0.9554, val_acc=0.9150, val_f1=0.9159\n",
      "Epoch 10: train loss=0.0917, val loss=0.2566, train_acc=0.9683, val_acc=0.9080, val_f1=0.9053\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.0819, val loss=0.2544, train_acc=0.9699, val_acc=0.9125, val_f1=0.9114\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.0559, val loss=0.2770, train_acc=0.9828, val_acc=0.9095, val_f1=0.9091\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.0693, val loss=0.2846, train_acc=0.9755, val_acc=0.9155, val_f1=0.9144\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.0404, val loss=0.3008, train_acc=0.9871, val_acc=0.8980, val_f1=0.8927\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "Processed 32 samples...\n",
      "Test Accuracies for method=max: [0.7983999848365784, 0.7952600121498108, 0.7950800061225891]\n",
      "Test F1 Scores for method=max: [0.7980525311237472, 0.7951684608670941, 0.7950626541030433]\n",
      "Average Test Accuracy: 0.7962, Average Test F1: 0.7961\n",
      "\n",
      "\n",
      "========================================\n",
      "Best method: max with average test accuracy: 0.7962\n"
     ]
    }
   ],
   "source": [
    "methods = [\"last\", \"avg\", \"max\"]\n",
    "\n",
    "best_avg_acc_method = 0.0\n",
    "best_method = None\n",
    "all_method_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n{'='*40}\\nTesting method: {method} with lr={best_lr}, optimiser={best_optimiser}, weight_Decay={best_weight_decay}, num_channels={best_ch}, kernel_sze={best_kernel_size} \\n{'='*40}\")\n",
    "    \n",
    "    results = hyperparam_tuning(\n",
    "        lr=best_lr, optimiser=best_optimiser, weight_decay=best_weight_decay,\n",
    "        cnn_channels=best_ch, kernel_sizes=best_kernel_size, method=method\n",
    "    )\n",
    "    \n",
    "    test_accs = [res[\"test_acc\"] for res in results]\n",
    "    test_f1s = [res[\"test_f1\"] for res in results]\n",
    "    \n",
    "    avg_acc = sum(test_accs) / len(test_accs)\n",
    "    avg_f1 = sum(test_f1s) / len(test_f1s)\n",
    "    \n",
    "    print(f\"Test Accuracies for method={method}: {test_accs}\")\n",
    "    print(f\"Test F1 Scores for method={method}: {test_f1s}\")\n",
    "    print(f\"Average Test Accuracy: {avg_acc:.4f}, Average Test F1: {avg_f1:.4f}\\n\")\n",
    "    \n",
    "    all_method_results[method] = {\n",
    "        \"test_accs\": test_accs,\n",
    "        \"test_f1s\": test_f1s,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_f1\": avg_f1\n",
    "    }\n",
    "    \n",
    "    if avg_acc > best_avg_acc_method:\n",
    "        best_avg_acc_method = avg_acc\n",
    "        best_method = method\n",
    "\n",
    "print(f\"\\n{'='*40}\\nBest method: {best_method} \"\n",
    "      f\"with average test accuracy: {best_avg_acc_method:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST TRAINING CONFIGURATION:\n",
      "Learning Rate : 0.0005\n",
      "Optimizer     : adamw\n",
      "Weight Decay  : 0.001\n",
      "CNN Channels  : 50\n",
      "Kernel Sizes  : (3,4,5)\n",
      "Method        : max\n"
     ]
    }
   ],
   "source": [
    "print(\"BEST TRAINING CONFIGURATION:\")\n",
    "print(f\"Learning Rate : {best_lr}\")\n",
    "print(f\"Optimizer     : {best_optimiser}\")\n",
    "print(f\"Weight Decay  : {best_weight_decay}\")\n",
    "print(f\"CNN Channels  : {best_ch}\")\n",
    "print(f\"Kernel Sizes  : {best_kernel_size}\")\n",
    "print(f\"Method        : {best_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c80fa0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: INFO - Detected CPU: 13th Gen Intel(R) Core(TM) i9-13900H\n",
      "CarbonTracker: WARNING - No matching TDP found for CPU: 13th Gen Intel(R) Core(TM) i9-13900H. Using average TDP of 35.61W at 50% utilization as fallback.\n",
      "CarbonTracker: WARNING - No API keys provided. Skipping intensity provider initialization.\n",
      "CarbonTracker: The following components were found: GPU with device(s) NVIDIA GeForce RTX 3050 4GB Laptop GPU. CPU with device(s) 13th Gen Intel(R) Core(TM) i9-13900H.\n",
      "CarbonTracker: WARNING - No carbon intensity provider specified. Using average carbon intensity for SG: 498.74 gCO2eq/kWh.\n",
      "CarbonTracker: \n",
      "Predicted consumption for 50 epoch(s):\n",
      "\tTime:\t0:24:59\n",
      "\tEnergy:\t0.037068318685 kWh\n",
      "\tCO2eq:\t18.487520725507 g\n",
      "\tThis is equivalent to:\n",
      "\t0.173104126643 km travelled by car\n",
      "Epoch 01: train loss=0.5823, val loss=0.5562, train_acc=0.6869, val_acc=0.7025, val_f1=0.7664\n",
      "New best accuracy: 0.7025 at epoch 1, saving model.\n",
      "Epoch 02: train loss=0.3759, val loss=0.3052, train_acc=0.8375, val_acc=0.8730, val_f1=0.8683\n",
      "New best accuracy: 0.8730 at epoch 2, saving model.\n",
      "Epoch 03: train loss=0.2858, val loss=0.2628, train_acc=0.8816, val_acc=0.8960, val_f1=0.8936\n",
      "New best accuracy: 0.8960 at epoch 3, saving model.\n",
      "Epoch 04: train loss=0.2379, val loss=0.2841, train_acc=0.9040, val_acc=0.8780, val_f1=0.8692\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 05: train loss=0.2098, val loss=0.2874, train_acc=0.9170, val_acc=0.8815, val_f1=0.8710\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 06: train loss=0.1825, val loss=0.3113, train_acc=0.9297, val_acc=0.8735, val_f1=0.8594\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 07: train loss=0.1656, val loss=0.2309, train_acc=0.9371, val_acc=0.9125, val_f1=0.9109\n",
      "New best accuracy: 0.9125 at epoch 7, saving model.\n",
      "Epoch 08: train loss=0.1491, val loss=0.2337, train_acc=0.9426, val_acc=0.9100, val_f1=0.9074\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 09: train loss=0.1219, val loss=0.2276, train_acc=0.9554, val_acc=0.9245, val_f1=0.9259\n",
      "New best accuracy: 0.9245 at epoch 9, saving model.\n",
      "Epoch 10: train loss=0.1040, val loss=0.2368, train_acc=0.9623, val_acc=0.9170, val_f1=0.9185\n",
      "No improvement in val loss for 1 epochs.\n",
      "Epoch 11: train loss=0.0809, val loss=0.3329, train_acc=0.9724, val_acc=0.8770, val_f1=0.8654\n",
      "No improvement in val loss for 2 epochs.\n",
      "Epoch 12: train loss=0.0650, val loss=0.2712, train_acc=0.9776, val_acc=0.9150, val_f1=0.9178\n",
      "No improvement in val loss for 3 epochs.\n",
      "Epoch 13: train loss=0.0570, val loss=0.3195, train_acc=0.9806, val_acc=0.9050, val_f1=0.9008\n",
      "No improvement in val loss for 4 epochs.\n",
      "Epoch 14: train loss=0.0415, val loss=0.2942, train_acc=0.9861, val_acc=0.9130, val_f1=0.9117\n",
      "No improvement in val loss for 5 epochs.\n",
      "Early stopping triggered at epoch 14.\n",
      "CarbonTracker: Average carbon intensity during training was 498.74 gCO2eq/kWh. \n",
      "CarbonTracker: \n",
      "Actual consumption for 13 epoch(s):\n",
      "\tTime:\t0:06:02\n",
      "\tEnergy:\t0.009127517669 kWh\n",
      "\tCO2eq:\t4.552274774425 g\n",
      "\tThis is equivalent to:\n",
      "\t0.042624295641 km travelled by car\n",
      "CarbonTracker: Finished monitoring.\n",
      "Carbon tracking complete. Logs saved in './carbon_logs'.\n"
     ]
    }
   ],
   "source": [
    "model = CNNLSTM(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=embedding_matrix.shape[1], \n",
    "    hidden_size=128, \n",
    "    output_size=2, \n",
    "    method=best_method,\n",
    "    kernel_sizes = best_kernel_size,\n",
    "    cnn_channels = best_ch).to(device)\n",
    "\n",
    "\n",
    "# Load GloVe weights (frozen)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "model.embedding.weight.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "\n",
    "\n",
    "tracker = CarbonTracker(epochs=50, log_dir=\"./carbon_logs\", components=\"all\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies, val_f1s = [], [], []\n",
    "\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "total_energy_usage = 0.0\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    tracker.epoch_start()\n",
    "\n",
    "    train_loss, train_acc, energy_used = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    tracker.epoch_end()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    total_energy_usage += energy_used\n",
    "        \n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: \"\n",
    "        f\"train loss={train_loss:.4f}, val loss={val_loss:.4f}, \"\n",
    "        f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in val loss for {epochs_no_improve} epochs.\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "tracker.stop()  \n",
    "\n",
    "print(\"Carbon tracking complete. Logs saved in './carbon_logs'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2648f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest CarbonTracker file: carbon_logs\\8100.595734_2025-11-14T144907Z_carbontracker_output.log\n",
      "Energy consumed (kWh): 0.037068318685\n",
      "CO2eq (g): 18.487520725507\n",
      "Car travelled equivalent (km): 0.173104126643\n"
     ]
    }
   ],
   "source": [
    "latest_file = get_latest_carbontracker_output()\n",
    "results = parse_carbontracker_log(latest_file)\n",
    "\n",
    "print(\"Latest CarbonTracker file:\", latest_file)\n",
    "print(\"Energy consumed (kWh):\", results[\"energy_kWh\"])\n",
    "print(\"CO2eq (g):\", results[\"co2_grams\"])\n",
    "print(\"Car travelled equivalent (km):\", results[\"car_km\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d8733cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"train_accuracies\": train_accuracies,\n",
    "    \"val_accuracies\": val_accuracies,\n",
    "    \"val_f1s\": val_f1s,\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(result[\"train_losses\"]) + 1)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(epochs, result[\"train_losses\"], linestyle='--', label='Train Loss', color='blue')\n",
    "axes[0].plot(epochs, result[\"val_losses\"], label='Val Loss', color='orange')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(epochs, result[\"train_accuracies\"], linestyle='--', label='Train Acc', color='blue')\n",
    "axes[1].plot(epochs, result[\"val_accuracies\"], label='Val Acc', color='orange')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(epochs, result[\"val_f1s\"], label='Val F1', color='green')\n",
    "axes[2].set_title('F1 Score')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57ccdde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32 samples...\n",
      "Accuracy: 0.788860023021698\n",
      "F1 Score: 0.7883235875009341\n",
      "Confusion Matrix:\n",
      " [[20980  4020]\n",
      " [ 6537 18463]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "results = test_worst(\n",
    "            model,\n",
    "            test_loader,\n",
    "            criterion,\n",
    "            return_preds=True,\n",
    "            text_dataset=test_texts,\n",
    "        )\n",
    "print(\"Accuracy:\", results[\"acc\"])\n",
    "print(\"F1 Score:\", results[\"f1\"])\n",
    "print(\"Confusion Matrix:\\n\", results[\"confusion_matrix\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(results[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2f7b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg len of test: 1309.43102\n",
      "avg len of misclassified: 1420.9912854030501\n",
      "avg len of bad predictions: 1415.0147301587301\n",
      "avg len of close predictions: 1438.5398956002982\n"
     ]
    }
   ],
   "source": [
    "df_wrong = results[\"misclassified\"]\n",
    "\n",
    "all_pos = df_wrong[df_wrong[\"true_label\"] == \"Positive\"]\n",
    "all_neg = df_wrong[df_wrong[\"true_label\"] == \"Negative\"]\n",
    "\n",
    "# Combine them\n",
    "all_combined = pd.concat([all_pos, all_neg])\n",
    "\n",
    "# Save to CSV\n",
    "all_combined.to_csv(\"all_misclassified_CLSTM_best_examples.csv\", index=False)\n",
    "average_length_test = IMDB_df['review'].str.len().mean()\n",
    "print(f\"avg len of test: {average_length_test}\")\n",
    "\n",
    "average_length_misclassified = all_combined['text'].str.len().mean()\n",
    "print(f\"avg len of misclassified: {average_length_misclassified}\")\n",
    "\n",
    "bad_predictions = all_combined[all_combined[\"pred_conf\"]>0.75]\n",
    "bad_preds_len = bad_predictions['text'].str.len().mean()\n",
    "print(f\"avg len of bad predictions: {bad_preds_len}\")\n",
    "\n",
    "close_predictions = all_combined[all_combined[\"pred_conf\"]<=0.75]\n",
    "close_preds_len =close_predictions['text'].str.len().mean()\n",
    "print(f\"avg len of close predictions: {close_preds_len}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641936cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e0317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca2eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d04851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
