{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90a8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dreamcore\\anaconda3\\envs\\fyp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, random,re,glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "# import torch, torch_directml as dml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel,get_linear_schedule_with_warmup\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "from carbontracker import parser as ct_parser\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f7cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ct_output_log(ct_dir, prefix:str | None = None, timeout_s: float = 3.0):\n",
    "    search_roots = [os.path.abspath(ct_dir), os.path.abspath(os.getcwd())]\n",
    "    deadline = time.time() + timeout_s\n",
    "    best = None\n",
    "    while time.time() <= deadline:\n",
    "        candidates = []\n",
    "        for root in search_roots:\n",
    "            if os.path.isdir(root):\n",
    "                candidates.extend(glob.glob(os.path.join(root, \"**\", \"*carbontracker_output.log\"), recursive=True))\n",
    "        candidates = sorted(set(candidates), key=os.path.getmtime, reverse=True)\n",
    "        if prefix:\n",
    "            pref = [p for p in candidates if prefix in os.path.basename(p)]\n",
    "            if pref:\n",
    "                return pref[0]\n",
    "        if candidates:\n",
    "            return candidates[0]\n",
    "        time.sleep(0.25)\n",
    "    return best\n",
    "\n",
    "def parse_carbontracker(ct_dir, prefix: str | None = None):\n",
    "    output_log = None\n",
    "    # Try official helper\n",
    "    try:\n",
    "        from carbontracker import parser as ct_parser\n",
    "        output_log, _ = ct_parser.get_most_recent_logs(log_dir=ct_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        output_log = find_ct_output_log(ct_dir, prefix=prefix)\n",
    "    if not output_log or not os.path.exists(output_log):\n",
    "        return None, None, None, None\n",
    "\n",
    "    with open(output_log, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        out_txt = f.read()\n",
    "    try:\n",
    "        from carbontracker import parser as ct_parser\n",
    "        actual, pred = ct_parser.get_consumption(out_txt)\n",
    "        cons = actual or pred\n",
    "        if cons:\n",
    "            e = cons.get(\"energy (kWh)\")\n",
    "            c = cons.get(\"co2eq (g)\")\n",
    "            d = cons.get(\"duration (s)\")\n",
    "            return (\n",
    "                float(e) if e is not None else None,\n",
    "                float(c) if c is not None else None,\n",
    "                float(d) if d is not None else None,\n",
    "                output_log,\n",
    "            )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #regex fallback\n",
    "    mE = re.search(r\"Energy:\\s*([0-9.eE+-]+)\\s*kWh\", out_txt)\n",
    "    mC = re.search(r\"CO2eq:\\s*([0-9.eE+-]+)\\s*g\", out_txt)\n",
    "    mT = re.search(r\"Time:\\s*([0-9:]+)\", out_txt)\n",
    "\n",
    "    energy_kwh = float(mE.group(1)) if mE else None\n",
    "    co2_g      = float(mC.group(1)) if mC else None\n",
    "    duration_s = None\n",
    "    if mT:\n",
    "        hh, mm, ss = (int(x) for x in mT.group(1).split(\":\"))\n",
    "        duration_s = 3600*hh + 60*mm + ss\n",
    "\n",
    "    return energy_kwh, co2_g, duration_s, output_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ef52f",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ae30b",
   "metadata": {},
   "source": [
    "data loading for YELP (train dataset) and IMDB (test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e4e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YELP\n",
    "\n",
    "yelp_neg= pd.read_csv(r\"C:\\Users\\Dreamcore\\Downloads\\yelp_2_and_below\")\n",
    "yelp_pos = pd.read_csv(r\"C:\\Users\\Dreamcore\\Downloads\\yelp_4_and_above\")\n",
    "\n",
    "#IMDB (test)\n",
    "imdb = pd.read_csv(r\"C:\\Users\\Dreamcore\\Downloads\\archive\\IMDB Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594cdca",
   "metadata": {},
   "source": [
    "setup of device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bf4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = dml.device(dml.default_device())\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1be375",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706030e",
   "metadata": {},
   "source": [
    "adding '1' for positive entries, '0' for negative entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c83db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_neg['sentiment'] = 'negative'\n",
    "yelp_pos['sentiment'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e967aea",
   "metadata": {},
   "source": [
    "keeping review(text) + sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a95a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['text','sentiment']\n",
    "yelp = pd.concat([yelp_neg[columns], yelp_pos[columns]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d636cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = imdb.rename(columns={'review':'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac4694",
   "metadata": {},
   "source": [
    "mapping positive to integer 1, and negative to integer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac17bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = {'negative': 0, 'positive': 1}\n",
    "for df in (yelp, imdb):\n",
    "    #normalize column names if needed\n",
    "    df['sentiment'] = df['sentiment'].str.lower().str.strip()\n",
    "    df['label'] = df['sentiment'].map(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60eed9",
   "metadata": {},
   "source": [
    "### 3. Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dd444",
   "metadata": {},
   "source": [
    "Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b27e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 20\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "yelp_train, yelp_val = train_test_split(\n",
    "    yelp, test_size=0.1, random_state=SEED, stratify=yelp['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51df62",
   "metadata": {},
   "source": [
    "text class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38501a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df['text'].astype(str).tolist()\n",
    "        self.labels = df['label'].astype(int).tolist()\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i): return self.texts[i], self.labels[i]\n",
    "\n",
    "def make_loader(df, tokenizer, shuffle):\n",
    "    def collate_fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        enc = tokenizer(list(texts), truncation=True, padding=True,return_tensors='pt')\n",
    "        enc['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        return enc\n",
    "    ds = TextDataset(df)\n",
    "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f0910",
   "metadata": {},
   "source": [
    "CNN head, shared among both BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d00720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCNN(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2, cnn_filters=256, kernel_size=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.conv = nn.Conv1d(in_channels=hidden, out_channels=cnn_filters,\n",
    "                              kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(cnn_filters, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        x = out.last_hidden_state\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = torch.max(x, dim=2).values\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb3182",
   "metadata": {},
   "source": [
    "training, test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer=None, scheduler=None, training=True):\n",
    "    model.train(training)\n",
    "    losses, preds_all, labels_all = [], [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids= batch['input_ids'].to(device)\n",
    "        attention_mask= batch['attention_mask'].to(device)\n",
    "        labels= batch['labels'].to(device).long()\n",
    "\n",
    "        output= model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss = output['loss']\n",
    "\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        preds_np  = torch.argmax(output['logits'], dim=1).detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "        preds_all.extend(preds_np)\n",
    "        labels_all.extend(labels_np)\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    f1  = f1_score(labels_all, preds_all, average='weighted')\n",
    "    return float(np.mean(losses)), acc, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    losses, preds_all, labels_all = [], [], []\n",
    "    for batch in loader:\n",
    "        output = model(input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    labels=batch['labels'].to(device))\n",
    "        losses.append(output['loss'].item())\n",
    "        preds = torch.argmax(output['logits'], dim=1).detach().cpu().numpy()\n",
    "        labels = batch['labels'].detach().cpu().numpy()\n",
    "        preds_all.extend(preds)\n",
    "        labels_all.extend(labels)\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    f1  = f1_score(labels_all, preds_all, average='weighted')\n",
    "    class_rep = classification_report(labels_all, preds_all, target_names=['negative','positive'])\n",
    "    return float(np.mean(losses)), acc, f1, class_rep\n",
    "\n",
    "def train_and_eval(model_name, epochs=10, lr=1e-5, patience=3,min_delta=0.01,plot_dir=\"plots\",tracker_dir=\"energy_logs\"):\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(tracker_dir,exist_ok=True)\n",
    "    safe_name = re.sub(r\"[^a-zA-Z0-9_.-]+\", \"_\", model_name)\n",
    "\n",
    "    tokenizer    = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    train_loader = make_loader(yelp_train, tokenizer, shuffle=True)\n",
    "    val_loader   = make_loader(yelp_val,   tokenizer, shuffle=False)\n",
    "    test_loader  = make_loader(imdb,tokenizer, shuffle=False)\n",
    "\n",
    "    model = TransformerCNN(model_name).to(device)\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(yelp_train) / BATCH_SIZE)\n",
    "    total_steps     = epochs * steps_per_epoch\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, int(0.05* total_steps),total_steps\n",
    "    )\n",
    "    \n",
    "    tracker = CarbonTracker(epochs=epochs,log_dir=tracker_dir,log_file_prefix=safe_name)\n",
    "    history = []\n",
    "    tr_losses, va_losses = [], []\n",
    "    tr_accs,   va_accs   = [], []\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    early_stopped = False\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, epochs + 1):\n",
    "        if tracker: tracker.epoch_start()\n",
    "\n",
    "        tr_loss, tr_acc, tr_f1 = run_epoch(model, train_loader, optimizer, scheduler, training=True)\n",
    "        va_loss, va_acc, va_f1 = run_epoch(model, val_loader,   optimizer=None, scheduler=None, training=False)\n",
    "\n",
    "        if tracker: tracker.epoch_end()\n",
    "\n",
    "        tr_losses.append(tr_loss); va_losses.append(va_loss)\n",
    "        tr_accs.append(tr_acc);   va_accs.append(va_acc)\n",
    "\n",
    "        history.append(dict(\n",
    "            epoch=ep,\n",
    "            train_loss=tr_loss, train_acc=tr_acc, train_f1=tr_f1,\n",
    "            val_loss=va_loss,   val_acc=va_acc,   val_f1=va_f1\n",
    "        ))\n",
    "\n",
    "        print(f\"[{model_name}] Ep {ep:02d}/{epochs} | \"\n",
    "              f\"train {tr_loss:.4f}/{tr_acc:.4f}/{tr_f1:.4f} | \"\n",
    "              f\"val {va_loss:.4f}/{va_acc:.4f}/{va_f1:.4f}\")\n",
    "\n",
    "        #early stopping\n",
    "        if va_loss + min_delta < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                early_stopped = True\n",
    "                print(f\"[EarlyStopping] No val_loss improvement for {patience} epoch(s). Stopping at epoch {ep}.\")\n",
    "                break\n",
    "\n",
    "    if tracker:\n",
    "        tracker.stop()\n",
    "    train_time_sec = time.time() - t0\n",
    "\n",
    "    #restore best state\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    #final\n",
    "    test_loss, test_acc, test_f1, class_rep = evaluate(model, test_loader)\n",
    "    print(f\"\\n[{model_name}] TEST | loss={test_loss:.4f} acc={test_acc:.4f} f1={test_f1:.4f}\")\n",
    "    print(class_rep)\n",
    "\n",
    "    #plots\n",
    "    history_df  = pd.DataFrame(history)\n",
    "    epochs_axis = range(1, len(tr_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs_axis, tr_losses, label=\"train loss\")\n",
    "    plt.plot(epochs_axis, va_losses, label=\"val loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Loss Curves — {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    loss_path = os.path.join(plot_dir, f\"{safe_name}_loss.png\")\n",
    "    plt.savefig(loss_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs_axis, tr_accs, label=\"train acc\")\n",
    "    plt.plot(epochs_axis, va_accs, label=\"val acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(f\"Accuracy Curves — {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    acc_path = os.path.join(plot_dir, f\"{safe_name}_acc.png\")\n",
    "    plt.savefig(acc_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    #tracker\n",
    "    energy_kwh = co2_g = duration_s = None\n",
    "    ct_devices = None\n",
    "    try:\n",
    "        output_log, standard_log = ct_parser.get_most_recent_logs(log_dir=tracker_dir)  # CHANGE\n",
    "        with open(output_log, \"r\", encoding=\"utf-8\") as f:\n",
    "            out_txt = f.read()                                                          # CHANGE\n",
    "        with open(standard_log, \"r\", encoding=\"utf-8\") as f:\n",
    "            std_txt = f.read()                                                          # CHANGE\n",
    "        actual, pred = ct_parser.get_consumption(out_txt)\n",
    "        cons = actual or pred\n",
    "        if cons is not None:                                                            # CHANGE\n",
    "            energy_kwh = cons.get(\"energy (kWh)\")\n",
    "            co2_g      = cons.get(\"co2eq (g)\")\n",
    "            duration_s = cons.get(\"duration (s)\")\n",
    "        ct_devices = ct_parser.get_devices(std_txt)                                     # CHANGE\n",
    "    except Exception as e:\n",
    "        print(\"[CarbonTracker] parse warning:\", e)                                      # CHANGE\n",
    "\n",
    "    # summary (kept clean — omit CT fields entirely if you prefer)\n",
    "    summary = {\n",
    "        \"model\": model_name,\n",
    "        \"early_stopped\": early_stopped,\n",
    "        \"epochs_ran\": len(tr_losses),\n",
    "        \"time_s_train\": round(train_time_sec, 2),\n",
    "        \"test_loss\": round(test_loss, 4),\n",
    "        \"test_acc\":  round(test_acc, 4),\n",
    "        \"test_f1\":   round(test_f1, 4),\n",
    "        \"loss_plot\": loss_path,\n",
    "        \"acc_plot\":  acc_path,\n",
    "        \"energy_kwh\": None if energy_kwh is None else round(float(energy_kwh), 4),\n",
    "        \"co2_g\": None if co2_g is None else round(float(co2_g), 2),\n",
    "        \"duration_s\": None if duration_s is None else int(float(duration_s)),\n",
    "        \"devices\": ct_devices\n",
    "    }\n",
    "    return model, history_df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d245dc",
   "metadata": {},
   "source": [
    "### 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT base\n",
    "bert_base = \"bert-base-uncased\"\n",
    "#BERT small\n",
    "bert_small = \"prajjwal1/bert-small\"\n",
    "\n",
    "model_bertsmall, hist_bertsmall_df, summary_bertsmall = train_and_eval(\n",
    "    bert_small, epochs=3, lr=2e-5, patience=2, min_delta=0.0,\n",
    "    plot_dir=\"plots\", tracker_dir=\"carbon_logs\"\n",
    ")\n",
    "print(summary_bertsmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8a6d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dreamcore\\anaconda3\\envs\\fyp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dreamcore\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: INFO - Detected CPU: 12th Gen Intel(R) Core(TM) i5-12400F\n",
      "CarbonTracker: WARNING - No matching TDP found for CPU: 12th Gen Intel(R) Core(TM) i5-12400F. Using average TDP of 35.61W at 50% utilization as fallback.\n",
      "CarbonTracker: WARNING - No API keys provided. Skipping intensity provider initialization.\n",
      "CarbonTracker: The following components were found: GPU with device(s) NVIDIA GeForce RTX 3060 Ti. CPU with device(s) 12th Gen Intel(R) Core(TM) i5-12400F.\n",
      "CarbonTracker: WARNING - No carbon intensity provider specified. Using average carbon intensity for SG: 498.74 gCO2eq/kWh.\n",
      "CarbonTracker: \n",
      "Predicted consumption for 3 epoch(s):\n",
      "\tTime:\t2:25:24\n",
      "\tEnergy:\t0.520018890368 kWh\n",
      "\tCO2eq:\t259.355167816494 g\n",
      "\tThis is equivalent to:\n",
      "\t2.428419174312 km travelled by car\n",
      "[bert-base-uncased] Ep 01/3 | train 0.2311/0.9031/0.9031 | val 0.0901/0.9740/0.9740\n",
      "[bert-base-uncased] Ep 02/3 | train 0.0713/0.9802/0.9802 | val 0.1191/0.9710/0.9710\n",
      "CarbonTracker: Average carbon intensity during training was 498.74 gCO2eq/kWh. \n",
      "CarbonTracker: \n",
      "Actual consumption for 3 epoch(s):\n",
      "\tTime:\t2:34:14\n",
      "\tEnergy:\t0.549267042533 kWh\n",
      "\tCO2eq:\t273.942444458942 g\n",
      "\tThis is equivalent to:\n",
      "\t2.565004161601 km travelled by car\n",
      "CarbonTracker: Finished monitoring.\n",
      "[bert-base-uncased] Ep 03/3 | train 0.0292/0.9938/0.9938 | val 0.1417/0.9700/0.9700\n",
      "[EarlyStopping] No val_loss improvement for 2 epoch(s). Stopping at epoch 3.\n",
      "\n",
      "[bert-base-uncased] TEST | loss=0.4050 acc=0.8485 f1=0.8466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86     25000\n",
      "    positive       0.95      0.74      0.83     25000\n",
      "\n",
      "    accuracy                           0.85     50000\n",
      "   macro avg       0.87      0.85      0.85     50000\n",
      "weighted avg       0.87      0.85      0.85     50000\n",
      "\n",
      "{'model': 'bert-base-uncased', 'early_stopped': True, 'epochs_ran': 3, 'time_s_train': 9254.12, 'test_loss': 0.405, 'test_acc': 0.8485, 'test_f1': 0.8466, 'loss_plot': 'plots\\\\bert-base-uncased_loss.png', 'acc_plot': 'plots\\\\bert-base-uncased_acc.png', 'energy_kwh': None, 'co2_g': None, 'duration_s': None, 'devices': {'gpu': ['NVIDIA GeForce RTX 3060 Ti'], 'cpu': ['12th Gen Intel(R) Core(TM) i5-12400F']}}\n"
     ]
    }
   ],
   "source": [
    "model_bertbase, hist_bertbase_df, summary_bertbase = train_and_eval(\n",
    "    bert_base, epochs=3, lr=2e-5, patience=2, min_delta=0.0,\n",
    "    plot_dir=\"plots\", tracker_dir=\"carbon_logs\"\n",
    ")\n",
    "\n",
    "print(summary_bertbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd04e18-0f35-4f60-a320-17ed1a4ba6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
