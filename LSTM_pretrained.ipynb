{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c05152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "df = pd.read_csv(r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\IMDB Dataset.csv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "print(df.head())\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86cf7f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500\n",
      "Validation size: 500\n",
      "Test size: 49000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train+val / test split\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.98, random_state=42, stratify=df[\"sentiment\"])\n",
    "\n",
    "# train / val split\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.5, random_state=42, stratify=train_val_df[\"sentiment\"])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89fc439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "input_path = r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\yelp_academic_dataset_review.json\"\n",
    "output_path = r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\yelp_sample.json\"\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, 50000):\n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "stars\n",
      "5    22220\n",
      "4    12721\n",
      "1     5379\n",
      "2     4003\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "1    34941\n",
      "0     9382\n",
      "Name: count, dtype: int64\n",
      "Index(['text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "yelp_df = pd.read_json(r\"C:\\Users\\ASUS\\Downloads\\SC4001 Neural Network project\\yelp_sample.json\",lines=True)\n",
    "print(len(yelp_df))\n",
    "# print(df.head())\n",
    "# print(yelp_df.columns)\n",
    "yelp_df = yelp_df[yelp_df[\"stars\"]!=3]\n",
    "yelp_df['sentiment'] = yelp_df['stars'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "print(yelp_df[\"stars\"].value_counts())\n",
    "print(yelp_df[\"sentiment\"].value_counts())\n",
    "\n",
    "yelp_df = yelp_df[[\"text\",\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f19dd20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6993885792929179, 0.8019972656482197)\n"
     ]
    }
   ],
   "source": [
    "yelp_test_texts = yelp_df[\"text\"]\n",
    "yelp_test_labels = yelp_df[\"sentiment\"]\n",
    "yelp_data = IMDBDataset(yelp_test_texts, yelp_test_labels)\n",
    "yelp_loader = DataLoader(yelp_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "print(evaluate(model,yelp_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b7e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267862d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterlen: 13861\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import copy\n",
    "\n",
    "train_texts = train_df[\"review\"]\n",
    "train_labels = train_df[\"sentiment\"]\n",
    "val_texts = val_df[\"review\"]\n",
    "val_labels = val_df[\"sentiment\"]\n",
    "test_texts = test_df[\"review\"]\n",
    "test_labels = test_df[\"sentiment\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# def tokenize(text, max_len=300):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "#     return text.split()[:max_len]\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(tokenize(text))\n",
    "print(f\"counterlen: {len(counter)}\")\n",
    "vocab_size = 20000\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(vocab_size))}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "\n",
    "def numericalize(tokens):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [torch.tensor(numericalize(tokenize(t))) for t in texts]\n",
    "        self.labels = torch.tensor(labels.tolist())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return texts_padded, torch.tensor(labels)\n",
    "\n",
    "train_data = IMDBDataset(train_texts, train_labels)\n",
    "val_data = IMDBDataset(val_texts, val_labels)\n",
    "test_data = IMDBDataset(test_texts, test_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0584e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(predictions.cpu().tolist())\n",
    "            labels_all.extend(labels.cpu().tolist())\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    f1 = f1_score(labels_all, preds)\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d04c5e",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ad98b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.8398, acc=0.5080, f1=0.6694\n",
      "New best accuracy: 0.5080 at epoch 1, saving model.\n",
      "Epoch 2: loss=0.7135, acc=0.5120, f1=0.0469\n",
      "New best accuracy: 0.5120 at epoch 2, saving model.\n",
      "Epoch 3: loss=0.7262, acc=0.5040, f1=0.6612\n",
      "Epoch 4: loss=0.7027, acc=0.5060, f1=0.6676\n",
      "Epoch 5: loss=0.7369, acc=0.5000, f1=0.6667\n",
      "Epoch 6: loss=0.7000, acc=0.5000, f1=0.6667\n",
      "Epoch 7: loss=0.6828, acc=0.4780, f1=0.3040\n",
      "Epoch 8: loss=0.7386, acc=0.5000, f1=0.0000\n",
      "Epoch 9: loss=0.6989, acc=0.5020, f1=0.0812\n",
      "Epoch 10: loss=0.6836, acc=0.5020, f1=0.1942\n",
      "Epoch 11: loss=0.7216, acc=0.4880, f1=0.6246\n",
      "Epoch 12: loss=0.6815, acc=0.4900, f1=0.6256\n",
      "Epoch 13: loss=0.6719, acc=0.4880, f1=0.6086\n",
      "Epoch 14: loss=0.6474, acc=0.5440, f1=0.2500\n",
      "New best accuracy: 0.5440 at epoch 14, saving model.\n",
      "Epoch 15: loss=0.6132, acc=0.4840, f1=0.5994\n",
      "Epoch 16: loss=0.4870, acc=0.5840, f1=0.6498\n",
      "New best accuracy: 0.5840 at epoch 16, saving model.\n",
      "Epoch 17: loss=0.3792, acc=0.5840, f1=0.4439\n",
      "Epoch 18: loss=0.3118, acc=0.6760, f1=0.6667\n",
      "New best accuracy: 0.6760 at epoch 18, saving model.\n",
      "Epoch 19: loss=0.2227, acc=0.6780, f1=0.6611\n",
      "New best accuracy: 0.6780 at epoch 19, saving model.\n",
      "Epoch 20: loss=0.1751, acc=0.5960, f1=0.6529\n",
      "Epoch 21: loss=0.1502, acc=0.6800, f1=0.6850\n",
      "New best accuracy: 0.6800 at epoch 21, saving model.\n",
      "Epoch 22: loss=0.1292, acc=0.6160, f1=0.6712\n",
      "Epoch 23: loss=0.1183, acc=0.6820, f1=0.6948\n",
      "New best accuracy: 0.6820 at epoch 23, saving model.\n",
      "Epoch 24: loss=0.1061, acc=0.5820, f1=0.6534\n",
      "Epoch 25: loss=0.1150, acc=0.6300, f1=0.5747\n",
      "Epoch 26: loss=0.4441, acc=0.5700, f1=0.4638\n",
      "Epoch 27: loss=0.3864, acc=0.5320, f1=0.5836\n",
      "Epoch 28: loss=0.1644, acc=0.6680, f1=0.6719\n",
      "Epoch 29: loss=0.0937, acc=0.6980, f1=0.6998\n",
      "New best accuracy: 0.6980 at epoch 29, saving model.\n",
      "Epoch 30: loss=0.0705, acc=0.6880, f1=0.7023\n",
      "Epoch 31: loss=0.0682, acc=0.6820, f1=0.6839\n",
      "Epoch 32: loss=0.0601, acc=0.6640, f1=0.6719\n",
      "Epoch 33: loss=0.0541, acc=0.6620, f1=0.6667\n",
      "Epoch 34: loss=0.0581, acc=0.6660, f1=0.6745\n",
      "Epoch 35: loss=0.0595, acc=0.6660, f1=0.6732\n",
      "Epoch 36: loss=0.0564, acc=0.6740, f1=0.6823\n",
      "Epoch 37: loss=0.0466, acc=0.6680, f1=0.6745\n",
      "Epoch 38: loss=0.0458, acc=0.6720, f1=0.6797\n",
      "Epoch 39: loss=0.0484, acc=0.6660, f1=0.6680\n",
      "Epoch 40: loss=0.0415, acc=0.6760, f1=0.6836\n",
      "Epoch 41: loss=0.0388, acc=0.6640, f1=0.6719\n",
      "Epoch 42: loss=0.0384, acc=0.6660, f1=0.6732\n",
      "Epoch 43: loss=0.0357, acc=0.6660, f1=0.6782\n",
      "Epoch 44: loss=0.0376, acc=0.6760, f1=0.6811\n",
      "Epoch 45: loss=0.0366, acc=0.6680, f1=0.6745\n",
      "Epoch 46: loss=0.0303, acc=0.6740, f1=0.6847\n",
      "Epoch 47: loss=0.0297, acc=0.6760, f1=0.6786\n",
      "Epoch 48: loss=0.0283, acc=0.6900, f1=0.6943\n",
      "Epoch 49: loss=0.0250, acc=0.6760, f1=0.6824\n",
      "Epoch 50: loss=0.0243, acc=0.6760, f1=0.6836\n"
     ]
    }
   ],
   "source": [
    "# class SimpleRNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, hidden_size, output_size, num_layers=1):\n",
    "#         super(SimpleRNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "#         # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         out, _ = self.rnn(x)\n",
    "#         out = self.fc(out[:, -1, :]) # last hidden state\n",
    "#         return out\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size,embed_dim, hidden_size, output_size, method=\"last\", num_layers=1, dropout=0):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # self.embedding_transform = nn.Linear(embed_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.method = method\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embeds = self.embedding_transform(x)\n",
    "        embeds = self.embedding(x)\n",
    "        out, _ = self.rnn(embeds)\n",
    "        if self.method == \"last\":\n",
    "            out = self.dropout(self.fc(out[:, -1, :]))\n",
    "        elif self.method == \"avg\":\n",
    "            out = self.dropout(self.fc(out.mean(dim=1)))\n",
    "        elif self.method == \"max\":\n",
    "            out, _ = out.max(dim=1)\n",
    "            out = self.dropout(self.fc(out))\n",
    "        return out\n",
    "\n",
    "model = SimpleRNN(vocab_size=len(vocab), embed_dim=100, hidden_size=128, output_size=2, method=\"avg\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, acc={acc:.4f}, f1={f1:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "830f54de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7297959183673469, 0.7170456488288597)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(vocab_size=len(vocab), embed_dim=100, hidden_size=128, output_size=2, method=\"avg\").to(device)\n",
    "# model.load_state_dict(torch.load(best_model_state))\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b14721",
   "metadata": {},
   "source": [
    "# BiGRU RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c2ceb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.9805, acc=0.5040, f1=0.6640\n",
      "New best accuracy: 0.5040 at epoch 1, saving model.\n",
      "Epoch 2: loss=0.7053, acc=0.5000, f1=0.0385\n",
      "Epoch 3: loss=0.6741, acc=0.5080, f1=0.6621\n",
      "New best accuracy: 0.5080 at epoch 3, saving model.\n",
      "Epoch 4: loss=0.4112, acc=0.5360, f1=0.2795\n",
      "New best accuracy: 0.5360 at epoch 4, saving model.\n",
      "Epoch 5: loss=0.1729, acc=0.5720, f1=0.6137\n",
      "New best accuracy: 0.5720 at epoch 5, saving model.\n",
      "Epoch 6: loss=0.0436, acc=0.5900, f1=0.6293\n",
      "New best accuracy: 0.5900 at epoch 6, saving model.\n",
      "Epoch 7: loss=0.0268, acc=0.5660, f1=0.6328\n",
      "Epoch 8: loss=0.0295, acc=0.5960, f1=0.6145\n",
      "New best accuracy: 0.5960 at epoch 8, saving model.\n",
      "Epoch 9: loss=0.0100, acc=0.5920, f1=0.5904\n",
      "Epoch 10: loss=0.0035, acc=0.6020, f1=0.5914\n",
      "New best accuracy: 0.6020 at epoch 10, saving model.\n",
      "Epoch 11: loss=0.0023, acc=0.5920, f1=0.5660\n",
      "Epoch 12: loss=0.0018, acc=0.5940, f1=0.5690\n",
      "Epoch 13: loss=0.0015, acc=0.5960, f1=0.5684\n",
      "Epoch 14: loss=0.0013, acc=0.5960, f1=0.5665\n",
      "Epoch 15: loss=0.0012, acc=0.5960, f1=0.5647\n",
      "Epoch 16: loss=0.0012, acc=0.5980, f1=0.5696\n",
      "Epoch 17: loss=0.0009, acc=0.6020, f1=0.5720\n",
      "Epoch 18: loss=0.0008, acc=0.6020, f1=0.5720\n",
      "Epoch 19: loss=0.0007, acc=0.6000, f1=0.5690\n",
      "Epoch 20: loss=0.0006, acc=0.5980, f1=0.5659\n",
      "Epoch 21: loss=0.0006, acc=0.6000, f1=0.5671\n",
      "Epoch 22: loss=0.0005, acc=0.6020, f1=0.5683\n",
      "Epoch 23: loss=0.0005, acc=0.6000, f1=0.5652\n",
      "Epoch 24: loss=0.0005, acc=0.6020, f1=0.5664\n",
      "Epoch 25: loss=0.0004, acc=0.6060, f1=0.5708\n",
      "New best accuracy: 0.6060 at epoch 25, saving model.\n",
      "Epoch 26: loss=0.0004, acc=0.6060, f1=0.5708\n",
      "Epoch 27: loss=0.0004, acc=0.6080, f1=0.5739\n",
      "New best accuracy: 0.6080 at epoch 27, saving model.\n",
      "Epoch 28: loss=0.0004, acc=0.6100, f1=0.5752\n",
      "New best accuracy: 0.6100 at epoch 28, saving model.\n",
      "Epoch 29: loss=0.0003, acc=0.6100, f1=0.5770\n",
      "Epoch 30: loss=0.0003, acc=0.6120, f1=0.5801\n",
      "New best accuracy: 0.6120 at epoch 30, saving model.\n",
      "Epoch 31: loss=0.0003, acc=0.6120, f1=0.5801\n",
      "Epoch 32: loss=0.0003, acc=0.6100, f1=0.5770\n",
      "Epoch 33: loss=0.0003, acc=0.6120, f1=0.5783\n",
      "Epoch 34: loss=0.0003, acc=0.6120, f1=0.5783\n",
      "Epoch 35: loss=0.0002, acc=0.6120, f1=0.5783\n",
      "Epoch 36: loss=0.0002, acc=0.6120, f1=0.5783\n",
      "Epoch 37: loss=0.0002, acc=0.6120, f1=0.5783\n",
      "Epoch 38: loss=0.0002, acc=0.6100, f1=0.5752\n",
      "Epoch 39: loss=0.0002, acc=0.6100, f1=0.5752\n",
      "Epoch 40: loss=0.0002, acc=0.6100, f1=0.5752\n",
      "Epoch 41: loss=0.0002, acc=0.6100, f1=0.5733\n",
      "Epoch 42: loss=0.0002, acc=0.6100, f1=0.5733\n",
      "Epoch 43: loss=0.0002, acc=0.6100, f1=0.5733\n",
      "Epoch 44: loss=0.0002, acc=0.6100, f1=0.5733\n",
      "Epoch 45: loss=0.0001, acc=0.6100, f1=0.5733\n",
      "Epoch 46: loss=0.0001, acc=0.6100, f1=0.5733\n",
      "Epoch 47: loss=0.0001, acc=0.6100, f1=0.5733\n",
      "Epoch 48: loss=0.0001, acc=0.6100, f1=0.5733\n",
      "Epoch 49: loss=0.0001, acc=0.6120, f1=0.5746\n",
      "Epoch 50: loss=0.0001, acc=0.6120, f1=0.5746\n"
     ]
    }
   ],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size, method=\"last\", num_layers=1, dropout=0):\n",
    "        super(BiGRU, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # BiGRU - note bidirectional=True\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Linear layer input size is 2*hidden_size due to bidirectional\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.method = method\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        out, hidden = self.gru(embeds)  # out: (batch, seq_len, 2*hidden_size)\n",
    "        \n",
    "        if self.method == \"last\":\n",
    "            out = self.dropout(self.fc(out[:, -1, :]))\n",
    "        elif self.method == \"avg\":\n",
    "            out = self.dropout(self.fc(out.mean(dim=1)))\n",
    "        elif self.method == \"max\":\n",
    "            out, _ = out.max(dim=1)\n",
    "            out = self.dropout(self.fc(out))\n",
    "            \n",
    "        return out\n",
    "    \n",
    "model = BiGRU(vocab_size=len(vocab), embed_dim=100, hidden_size=128, output_size=2, method=\"last\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, acc={acc:.4f}, f1={f1:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b7e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b060298",
   "metadata": {},
   "source": [
    "# Bi RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e288005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.7495, acc=0.5240, f1=0.5897\n",
      "New best accuracy: 0.5240 at epoch 1, saving model.\n",
      "Epoch 2: loss=0.6860, acc=0.5260, f1=0.6758\n",
      "New best accuracy: 0.5260 at epoch 2, saving model.\n",
      "Epoch 3: loss=0.6247, acc=0.5520, f1=0.6845\n",
      "New best accuracy: 0.5520 at epoch 3, saving model.\n",
      "Epoch 4: loss=0.3927, acc=0.6160, f1=0.7064\n",
      "New best accuracy: 0.6160 at epoch 4, saving model.\n",
      "Epoch 5: loss=0.1896, acc=0.7360, f1=0.7179\n",
      "New best accuracy: 0.7360 at epoch 5, saving model.\n",
      "Epoch 6: loss=0.0556, acc=0.6760, f1=0.5549\n",
      "Epoch 7: loss=0.0210, acc=0.6860, f1=0.5964\n",
      "Epoch 8: loss=0.0109, acc=0.6980, f1=0.6308\n",
      "Epoch 9: loss=0.0057, acc=0.6920, f1=0.6207\n",
      "Epoch 10: loss=0.0066, acc=0.6800, f1=0.5897\n",
      "Epoch 11: loss=0.0054, acc=0.6980, f1=0.6326\n",
      "Epoch 12: loss=0.0036, acc=0.7060, f1=0.6458\n",
      "Epoch 13: loss=0.0035, acc=0.7080, f1=0.6490\n",
      "Epoch 14: loss=0.0024, acc=0.7100, f1=0.6506\n",
      "Epoch 15: loss=0.0021, acc=0.7120, f1=0.6488\n",
      "Epoch 16: loss=0.0008, acc=0.7080, f1=0.6404\n",
      "Epoch 17: loss=0.0007, acc=0.7020, f1=0.6444\n",
      "Epoch 18: loss=0.0005, acc=0.7380, f1=0.7095\n",
      "New best accuracy: 0.7380 at epoch 18, saving model.\n",
      "Epoch 19: loss=0.0005, acc=0.7320, f1=0.7009\n",
      "Epoch 20: loss=0.0004, acc=0.7280, f1=0.6881\n",
      "Epoch 21: loss=0.0003, acc=0.7160, f1=0.6682\n",
      "Epoch 22: loss=0.0003, acc=0.7160, f1=0.6667\n",
      "Epoch 23: loss=0.0004, acc=0.7060, f1=0.6492\n",
      "Epoch 24: loss=0.0003, acc=0.7080, f1=0.6473\n",
      "Epoch 25: loss=0.0003, acc=0.7060, f1=0.6475\n",
      "Epoch 26: loss=0.0002, acc=0.7040, f1=0.6442\n",
      "Epoch 27: loss=0.0002, acc=0.7080, f1=0.6490\n",
      "Epoch 28: loss=0.0001, acc=0.7160, f1=0.6619\n",
      "Epoch 29: loss=0.0001, acc=0.7160, f1=0.6619\n",
      "Epoch 30: loss=0.0000, acc=0.7120, f1=0.6571\n",
      "Epoch 31: loss=0.0000, acc=0.7100, f1=0.6539\n",
      "Epoch 32: loss=0.0000, acc=0.7120, f1=0.6555\n",
      "Epoch 33: loss=0.0000, acc=0.7140, f1=0.6571\n",
      "Epoch 34: loss=0.0000, acc=0.7140, f1=0.6571\n",
      "Epoch 35: loss=0.0000, acc=0.7140, f1=0.6571\n",
      "Epoch 36: loss=0.0000, acc=0.7120, f1=0.6538\n",
      "Epoch 37: loss=0.0000, acc=0.7120, f1=0.6538\n",
      "Epoch 38: loss=0.0000, acc=0.7120, f1=0.6538\n",
      "Epoch 39: loss=0.0000, acc=0.7100, f1=0.6523\n",
      "Epoch 40: loss=0.0000, acc=0.7100, f1=0.6523\n",
      "Epoch 41: loss=0.0000, acc=0.7100, f1=0.6523\n",
      "Epoch 42: loss=0.0000, acc=0.7120, f1=0.6538\n",
      "Epoch 43: loss=0.0000, acc=0.7120, f1=0.6522\n",
      "Epoch 44: loss=0.0000, acc=0.7120, f1=0.6522\n",
      "Epoch 45: loss=0.0000, acc=0.7120, f1=0.6522\n",
      "Epoch 46: loss=0.0000, acc=0.7120, f1=0.6522\n",
      "Epoch 47: loss=0.0000, acc=0.7100, f1=0.6489\n",
      "Epoch 48: loss=0.0000, acc=0.7100, f1=0.6489\n",
      "Epoch 49: loss=0.0000, acc=0.7100, f1=0.6489\n",
      "Epoch 50: loss=0.0000, acc=0.7100, f1=0.6489\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size,embed_dim, hidden_size, output_size, method=\"last\", num_layers=1, dropout=0):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # self.embedding_transform = nn.Linear(embed_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # BiLSTM - note bidirectional=True\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Linear layer input size is 2*hidden_size due to bidirectional\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.method = method\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(embeds) \n",
    "        \n",
    "        if self.method == \"last\":\n",
    "            out = self.dropout(self.fc(out[:, -1, :]))\n",
    "        elif self.method == \"avg\":\n",
    "            out = self.dropout(self.fc(out.mean(dim=1)))\n",
    "        elif self.method == \"max\":\n",
    "            out, _ = out.max(dim=1)\n",
    "            out = self.dropout(self.fc(out))\n",
    "            \n",
    "        return out\n",
    "\n",
    "model = BiLSTM(vocab_size=len(vocab), embed_dim=100, hidden_size=128, output_size=2, method=\"avg\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, acc={acc:.4f}, f1={f1:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd9aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbaa94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407db768",
   "metadata": {},
   "source": [
    "# Regular LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeadee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.7001 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 2: Loss=0.6853 | Acc=0.5020 | F1=0.0460\n",
      "Epoch 3: Loss=0.6864 | Acc=0.5000 | F1=0.6640\n",
      "Epoch 4: Loss=0.6870 | Acc=0.5020 | F1=0.0311\n",
      "Epoch 5: Loss=0.6847 | Acc=0.5020 | F1=0.6631\n",
      "Epoch 6: Loss=0.6799 | Acc=0.4980 | F1=0.0079\n",
      "Epoch 7: Loss=0.6748 | Acc=0.4960 | F1=0.6585\n",
      "Epoch 8: Loss=0.6757 | Acc=0.5000 | F1=0.6622\n",
      "Epoch 9: Loss=0.6889 | Acc=0.5020 | F1=0.0235\n",
      "Epoch 10: Loss=0.6717 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 11: Loss=0.6782 | Acc=0.4960 | F1=0.0079\n",
      "Epoch 12: Loss=0.6720 | Acc=0.4960 | F1=0.0156\n",
      "Epoch 13: Loss=0.6728 | Acc=0.4960 | F1=0.6595\n",
      "Epoch 14: Loss=0.6714 | Acc=0.4960 | F1=0.6595\n",
      "Epoch 15: Loss=0.6765 | Acc=0.5000 | F1=0.0385\n",
      "Epoch 16: Loss=0.6781 | Acc=0.5000 | F1=0.6622\n",
      "Epoch 17: Loss=0.6745 | Acc=0.5020 | F1=0.0311\n",
      "Epoch 18: Loss=0.6790 | Acc=0.5000 | F1=0.6631\n",
      "Epoch 19: Loss=0.6749 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 20: Loss=0.6774 | Acc=0.5000 | F1=0.6622\n",
      "Epoch 21: Loss=0.6770 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 22: Loss=0.6716 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 23: Loss=0.6744 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 24: Loss=0.6751 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 25: Loss=0.6715 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 26: Loss=0.6747 | Acc=0.5000 | F1=0.6622\n",
      "Epoch 27: Loss=0.6721 | Acc=0.5020 | F1=0.0235\n",
      "Epoch 28: Loss=0.6723 | Acc=0.5020 | F1=0.0235\n",
      "Epoch 29: Loss=0.6720 | Acc=0.5020 | F1=0.6631\n",
      "Epoch 30: Loss=0.6748 | Acc=0.5000 | F1=0.6622\n",
      "Epoch 31: Loss=0.6740 | Acc=0.4940 | F1=0.0156\n",
      "Epoch 32: Loss=0.6714 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 33: Loss=0.6713 | Acc=0.4980 | F1=0.0157\n",
      "Epoch 34: Loss=0.6733 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 35: Loss=0.6746 | Acc=0.4980 | F1=0.6604\n",
      "Epoch 36: Loss=0.6723 | Acc=0.5000 | F1=0.0157\n",
      "Epoch 37: Loss=0.6725 | Acc=0.4980 | F1=0.6604\n",
      "Epoch 38: Loss=0.6718 | Acc=0.4960 | F1=0.0156\n",
      "Epoch 39: Loss=0.6728 | Acc=0.4940 | F1=0.6586\n",
      "Epoch 40: Loss=0.6686 | Acc=0.4960 | F1=0.0156\n",
      "Epoch 41: Loss=0.6749 | Acc=0.4940 | F1=0.0156\n",
      "Epoch 42: Loss=0.6763 | Acc=0.4960 | F1=0.6595\n",
      "Epoch 43: Loss=0.6760 | Acc=0.4940 | F1=0.0156\n",
      "Epoch 44: Loss=0.6742 | Acc=0.4980 | F1=0.6622\n",
      "Epoch 45: Loss=0.6724 | Acc=0.4980 | F1=0.0309\n",
      "Epoch 46: Loss=0.6758 | Acc=0.4960 | F1=0.6613\n",
      "Epoch 47: Loss=0.6718 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 48: Loss=0.6725 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 49: Loss=0.6740 | Acc=0.5000 | F1=0.6631\n",
      "Epoch 50: Loss=0.6704 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 51: Loss=0.6732 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 52: Loss=0.6720 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 53: Loss=0.6742 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 54: Loss=0.6722 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 55: Loss=0.6708 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 56: Loss=0.6701 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 57: Loss=0.6693 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 58: Loss=0.6721 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 59: Loss=0.6737 | Acc=0.4960 | F1=0.6613\n",
      "Epoch 60: Loss=0.6697 | Acc=0.5000 | F1=0.0310\n",
      "Epoch 61: Loss=0.6779 | Acc=0.4980 | F1=0.0309\n",
      "Epoch 62: Loss=0.6711 | Acc=0.4960 | F1=0.6613\n",
      "Epoch 63: Loss=0.6718 | Acc=0.4980 | F1=0.0309\n",
      "Epoch 64: Loss=0.6726 | Acc=0.4980 | F1=0.0309\n",
      "Epoch 65: Loss=0.6741 | Acc=0.4980 | F1=0.6622\n",
      "Epoch 66: Loss=0.6725 | Acc=0.5000 | F1=0.0310\n",
      "Epoch 67: Loss=0.6707 | Acc=0.5000 | F1=0.0310\n",
      "Epoch 68: Loss=0.6722 | Acc=0.5000 | F1=0.0310\n",
      "Epoch 69: Loss=0.6689 | Acc=0.5020 | F1=0.0386\n",
      "Epoch 70: Loss=0.6719 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 71: Loss=0.6752 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 72: Loss=0.6718 | Acc=0.5000 | F1=0.0385\n",
      "Epoch 73: Loss=0.6692 | Acc=0.5020 | F1=0.6658\n",
      "Epoch 74: Loss=0.6774 | Acc=0.4980 | F1=0.0309\n",
      "Epoch 75: Loss=0.6761 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 76: Loss=0.7522 | Acc=0.5000 | F1=0.6631\n",
      "Epoch 77: Loss=0.6749 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 78: Loss=0.6741 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 79: Loss=0.6756 | Acc=0.5020 | F1=0.6631\n",
      "Epoch 80: Loss=0.6735 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 81: Loss=0.6721 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 82: Loss=0.6706 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 83: Loss=0.6756 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 84: Loss=0.6719 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 85: Loss=0.6721 | Acc=0.4960 | F1=0.0233\n",
      "Epoch 86: Loss=0.6715 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 87: Loss=0.6709 | Acc=0.4960 | F1=0.6604\n",
      "Epoch 88: Loss=0.6732 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 89: Loss=0.6724 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 90: Loss=0.6706 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 91: Loss=0.6728 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 92: Loss=0.6732 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 93: Loss=0.6714 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 94: Loss=0.6704 | Acc=0.5000 | F1=0.6631\n",
      "Epoch 95: Loss=0.6698 | Acc=0.5000 | F1=0.6631\n",
      "Epoch 96: Loss=0.6735 | Acc=0.5000 | F1=0.0310\n",
      "Epoch 97: Loss=0.6711 | Acc=0.4980 | F1=0.0233\n",
      "Epoch 98: Loss=0.6716 | Acc=0.5020 | F1=0.0311\n",
      "Epoch 99: Loss=0.6704 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 100: Loss=0.6716 | Acc=0.5020 | F1=0.0311\n",
      "Epoch 101: Loss=0.6710 | Acc=0.5000 | F1=0.0234\n",
      "Epoch 102: Loss=0.6726 | Acc=0.5020 | F1=0.6640\n",
      "Epoch 103: Loss=0.6706 | Acc=0.5080 | F1=0.0611\n",
      "Epoch 104: Loss=0.6807 | Acc=0.5140 | F1=0.5653\n",
      "Epoch 105: Loss=0.7809 | Acc=0.4980 | F1=0.6613\n",
      "Epoch 106: Loss=0.6428 | Acc=0.5280 | F1=0.5280\n",
      "Epoch 107: Loss=0.5157 | Acc=0.5520 | F1=0.5573\n",
      "Epoch 108: Loss=0.4180 | Acc=0.5460 | F1=0.5558\n",
      "Epoch 109: Loss=0.2564 | Acc=0.5400 | F1=0.6075\n",
      "Epoch 110: Loss=0.1702 | Acc=0.5480 | F1=0.5950\n",
      "Epoch 111: Loss=0.0969 | Acc=0.5540 | F1=0.5619\n",
      "Epoch 112: Loss=0.0748 | Acc=0.5680 | F1=0.4977\n",
      "Epoch 113: Loss=0.1462 | Acc=0.5540 | F1=0.5184\n",
      "Epoch 114: Loss=0.1010 | Acc=0.5780 | F1=0.5772\n",
      "Epoch 115: Loss=0.0712 | Acc=0.6020 | F1=0.6151\n",
      "Epoch 116: Loss=0.0254 | Acc=0.5960 | F1=0.5809\n",
      "Epoch 117: Loss=0.0207 | Acc=0.5820 | F1=0.5673\n",
      "Epoch 118: Loss=0.0138 | Acc=0.5700 | F1=0.5147\n",
      "Epoch 119: Loss=0.0031 | Acc=0.5680 | F1=0.5091\n",
      "Epoch 120: Loss=0.0020 | Acc=0.5660 | F1=0.5079\n",
      "Epoch 121: Loss=0.0048 | Acc=0.5640 | F1=0.5023\n",
      "Epoch 122: Loss=0.0017 | Acc=0.5600 | F1=0.4931\n",
      "Epoch 123: Loss=0.0012 | Acc=0.5620 | F1=0.4942\n",
      "Epoch 124: Loss=0.0010 | Acc=0.5620 | F1=0.4942\n",
      "Epoch 125: Loss=0.0012 | Acc=0.5620 | F1=0.4942\n",
      "Epoch 126: Loss=0.0009 | Acc=0.5620 | F1=0.4942\n",
      "Epoch 127: Loss=0.0008 | Acc=0.5620 | F1=0.4942\n",
      "Epoch 128: Loss=0.0011 | Acc=0.5600 | F1=0.4931\n",
      "Epoch 129: Loss=0.0021 | Acc=0.5600 | F1=0.4931\n",
      "Epoch 130: Loss=0.0007 | Acc=0.5640 | F1=0.5000\n",
      "Epoch 131: Loss=0.0009 | Acc=0.5680 | F1=0.5068\n",
      "Epoch 132: Loss=0.0005 | Acc=0.5640 | F1=0.5045\n",
      "Epoch 133: Loss=0.0006 | Acc=0.5640 | F1=0.5068\n",
      "Epoch 134: Loss=0.0003 | Acc=0.5620 | F1=0.5011\n",
      "Epoch 135: Loss=0.0004 | Acc=0.5620 | F1=0.5011\n",
      "Epoch 136: Loss=0.0002 | Acc=0.5580 | F1=0.4989\n",
      "Epoch 137: Loss=0.0004 | Acc=0.5600 | F1=0.5023\n",
      "Epoch 138: Loss=0.0003 | Acc=0.5620 | F1=0.5056\n",
      "Epoch 139: Loss=0.0002 | Acc=0.5640 | F1=0.5112\n",
      "Epoch 140: Loss=0.0003 | Acc=0.5680 | F1=0.5179\n",
      "Epoch 141: Loss=0.0003 | Acc=0.5660 | F1=0.5145\n",
      "Epoch 142: Loss=0.0006 | Acc=0.5680 | F1=0.5179\n",
      "Epoch 143: Loss=0.0001 | Acc=0.5680 | F1=0.5179\n",
      "Epoch 144: Loss=0.0001 | Acc=0.5680 | F1=0.5179\n",
      "Epoch 145: Loss=0.0028 | Acc=0.5700 | F1=0.5254\n",
      "Epoch 146: Loss=0.0002 | Acc=0.5820 | F1=0.5386\n",
      "Epoch 147: Loss=0.0002 | Acc=0.5820 | F1=0.5386\n",
      "Epoch 148: Loss=0.0012 | Acc=0.5800 | F1=0.5354\n",
      "Epoch 149: Loss=0.0002 | Acc=0.5740 | F1=0.5277\n",
      "Epoch 150: Loss=0.0002 | Acc=0.5700 | F1=0.5233\n",
      "Epoch 151: Loss=0.0002 | Acc=0.5720 | F1=0.5265\n",
      "Epoch 152: Loss=0.0001 | Acc=0.5720 | F1=0.5244\n",
      "Epoch 153: Loss=0.0001 | Acc=0.5760 | F1=0.5289\n",
      "Epoch 154: Loss=0.0001 | Acc=0.5800 | F1=0.5333\n",
      "Epoch 155: Loss=0.0001 | Acc=0.5800 | F1=0.5333\n",
      "Epoch 156: Loss=0.0002 | Acc=0.5760 | F1=0.5310\n",
      "Epoch 157: Loss=0.0003 | Acc=0.5780 | F1=0.5363\n",
      "Epoch 158: Loss=0.0003 | Acc=0.5760 | F1=0.5371\n",
      "Epoch 159: Loss=0.0001 | Acc=0.5700 | F1=0.5336\n",
      "Epoch 160: Loss=0.0001 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 161: Loss=0.0002 | Acc=0.5720 | F1=0.5368\n",
      "Epoch 162: Loss=0.0001 | Acc=0.5740 | F1=0.5400\n",
      "Epoch 163: Loss=0.0001 | Acc=0.5740 | F1=0.5400\n",
      "Epoch 164: Loss=0.0000 | Acc=0.5740 | F1=0.5400\n",
      "Epoch 165: Loss=0.0001 | Acc=0.5740 | F1=0.5400\n",
      "Epoch 166: Loss=0.0001 | Acc=0.5720 | F1=0.5388\n",
      "Epoch 167: Loss=0.0002 | Acc=0.5720 | F1=0.5388\n",
      "Epoch 168: Loss=0.0004 | Acc=0.5720 | F1=0.5368\n",
      "Epoch 169: Loss=0.0001 | Acc=0.5720 | F1=0.5368\n",
      "Epoch 170: Loss=0.0009 | Acc=0.5680 | F1=0.5345\n",
      "Epoch 171: Loss=0.0022 | Acc=0.5740 | F1=0.5359\n",
      "Epoch 172: Loss=0.0001 | Acc=0.5760 | F1=0.5391\n",
      "Epoch 173: Loss=0.0004 | Acc=0.5700 | F1=0.5316\n",
      "Epoch 174: Loss=0.0000 | Acc=0.5680 | F1=0.5304\n",
      "Epoch 175: Loss=0.0001 | Acc=0.5700 | F1=0.5336\n",
      "Epoch 176: Loss=0.0001 | Acc=0.5700 | F1=0.5336\n",
      "Epoch 177: Loss=0.0000 | Acc=0.5680 | F1=0.5325\n",
      "Epoch 178: Loss=0.0001 | Acc=0.5680 | F1=0.5325\n",
      "Epoch 179: Loss=0.0001 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 180: Loss=0.0001 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 181: Loss=0.0001 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 182: Loss=0.0001 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 183: Loss=0.0002 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 184: Loss=0.0000 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 185: Loss=0.0002 | Acc=0.5700 | F1=0.5356\n",
      "Epoch 186: Loss=0.0000 | Acc=0.5680 | F1=0.5345\n",
      "Epoch 187: Loss=0.0000 | Acc=0.5680 | F1=0.5345\n",
      "Epoch 188: Loss=0.0001 | Acc=0.5680 | F1=0.5345\n",
      "Epoch 189: Loss=0.0003 | Acc=0.5740 | F1=0.5478\n",
      "Epoch 190: Loss=0.0001 | Acc=0.5720 | F1=0.5466\n",
      "Epoch 191: Loss=0.0000 | Acc=0.5720 | F1=0.5485\n",
      "Epoch 192: Loss=0.0000 | Acc=0.5700 | F1=0.5474\n",
      "Epoch 193: Loss=0.0002 | Acc=0.5740 | F1=0.5535\n",
      "Epoch 194: Loss=0.0000 | Acc=0.5740 | F1=0.5535\n",
      "Epoch 195: Loss=0.0000 | Acc=0.5760 | F1=0.5565\n",
      "Epoch 196: Loss=0.0001 | Acc=0.5760 | F1=0.5565\n",
      "Epoch 197: Loss=0.0001 | Acc=0.5780 | F1=0.5595\n",
      "Epoch 198: Loss=0.0001 | Acc=0.5800 | F1=0.5607\n",
      "Epoch 199: Loss=0.0000 | Acc=0.5820 | F1=0.5637\n",
      "Epoch 200: Loss=0.0001 | Acc=0.5800 | F1=0.5607\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "model = LSTMSentiment(vocab_size=len(vocab), embed_dim=100, hidden_dim=128, output_dim=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model, test_loader)\n",
    "    \n",
    "    # ðŸ”¹ Save metrics for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, acc={acc:.4f}, f1={f1:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ðŸ“‰ Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ðŸ“ˆ Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='o', color='orange')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62504c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56594c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\torchtext\\vocab.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name=\"6B\", dim=100)\n",
    "embedding_dim = glove.dim\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove.stoi:\n",
    "        embedding_matrix[idx] = glove[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embedding_dim) * 0.05  # random init for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training with Frozen GloVe embeddings ====\n",
      "Epoch 1: Loss=0.6946 | Acc=0.5004 | F1=0.6616\n",
      "Epoch 2: Loss=0.6931 | Acc=0.5012 | F1=0.6636\n",
      "Epoch 3: Loss=0.6948 | Acc=0.4988 | F1=0.6645\n",
      "Epoch 4: Loss=0.6927 | Acc=0.5028 | F1=0.6663\n",
      "Epoch 5: Loss=0.6902 | Acc=0.5016 | F1=0.6634\n",
      "Epoch 6: Loss=0.6866 | Acc=0.5048 | F1=0.0492\n",
      "Epoch 7: Loss=0.6879 | Acc=0.5064 | F1=0.0434\n",
      "Epoch 8: Loss=0.6943 | Acc=0.5028 | F1=0.6641\n",
      "Epoch 9: Loss=0.6903 | Acc=0.5056 | F1=0.0463\n",
      "Epoch 10: Loss=0.6863 | Acc=0.5056 | F1=0.0507\n",
      "Epoch 11: Loss=0.6893 | Acc=0.5032 | F1=0.0387\n",
      "Epoch 12: Loss=0.6872 | Acc=0.5016 | F1=0.6640\n",
      "Epoch 13: Loss=0.6842 | Acc=0.5032 | F1=0.0297\n",
      "Epoch 14: Loss=0.6834 | Acc=0.5016 | F1=0.6640\n",
      "Epoch 15: Loss=0.6798 | Acc=0.5020 | F1=0.0401\n",
      "Epoch 16: Loss=0.6860 | Acc=0.5024 | F1=0.6647\n",
      "Epoch 17: Loss=0.6825 | Acc=0.5040 | F1=0.6661\n",
      "Epoch 18: Loss=0.6809 | Acc=0.5016 | F1=0.6643\n",
      "Epoch 19: Loss=0.6789 | Acc=0.5028 | F1=0.0342\n",
      "Epoch 20: Loss=0.6805 | Acc=0.5040 | F1=0.0417\n",
      "Epoch 21: Loss=0.6787 | Acc=0.5044 | F1=0.0388\n",
      "Epoch 22: Loss=0.6775 | Acc=0.5008 | F1=0.0355\n",
      "Epoch 23: Loss=0.6804 | Acc=0.5060 | F1=0.0551\n",
      "Epoch 24: Loss=0.6778 | Acc=0.5008 | F1=0.6636\n",
      "Epoch 25: Loss=0.6769 | Acc=0.5024 | F1=0.6645\n",
      "Epoch 26: Loss=0.6735 | Acc=0.5040 | F1=0.0358\n",
      "Epoch 27: Loss=0.6790 | Acc=0.5048 | F1=0.6670\n",
      "Epoch 28: Loss=0.6843 | Acc=0.5032 | F1=0.6658\n",
      "Epoch 29: Loss=0.6802 | Acc=0.5020 | F1=0.6641\n",
      "Epoch 30: Loss=0.6751 | Acc=0.5056 | F1=0.0404\n",
      "Epoch 31: Loss=0.6836 | Acc=0.5040 | F1=0.6647\n",
      "Epoch 32: Loss=0.6764 | Acc=0.5060 | F1=0.0434\n",
      "Epoch 33: Loss=0.6809 | Acc=0.5060 | F1=0.0507\n",
      "Epoch 34: Loss=0.6788 | Acc=0.5064 | F1=0.0580\n",
      "Epoch 35: Loss=0.6750 | Acc=0.5040 | F1=0.6665\n",
      "Epoch 36: Loss=0.6872 | Acc=0.5028 | F1=0.6658\n",
      "Epoch 37: Loss=0.6776 | Acc=0.5048 | F1=0.0462\n",
      "Epoch 38: Loss=0.6738 | Acc=0.5056 | F1=0.0419\n",
      "Epoch 39: Loss=0.6740 | Acc=0.5036 | F1=0.6656\n",
      "Epoch 40: Loss=0.6724 | Acc=0.5056 | F1=0.0492\n",
      "Epoch 41: Loss=0.6741 | Acc=0.5068 | F1=0.0508\n",
      "Epoch 42: Loss=0.6756 | Acc=0.5068 | F1=0.0434\n",
      "Epoch 43: Loss=0.6974 | Acc=0.5028 | F1=0.6654\n",
      "Epoch 44: Loss=0.6737 | Acc=0.5052 | F1=0.0448\n",
      "Epoch 45: Loss=0.6772 | Acc=0.5056 | F1=0.0389\n",
      "Epoch 46: Loss=0.6781 | Acc=0.5020 | F1=0.6647\n",
      "Epoch 47: Loss=0.6961 | Acc=0.5020 | F1=0.6651\n",
      "Epoch 48: Loss=0.6823 | Acc=0.5048 | F1=0.0506\n",
      "Epoch 49: Loss=0.6817 | Acc=0.5056 | F1=0.0478\n",
      "Epoch 50: Loss=0.6730 | Acc=0.5032 | F1=0.0431\n",
      "\n",
      "==== Fine-tuning GloVe embeddings ====\n",
      "Epoch 1: Loss=0.6960 | Acc=0.5040 | F1=0.0358\n",
      "Epoch 2: Loss=0.6927 | Acc=0.5024 | F1=0.6634\n",
      "Epoch 3: Loss=0.6917 | Acc=0.5032 | F1=0.6643\n",
      "Epoch 4: Loss=0.6872 | Acc=0.5012 | F1=0.6631\n",
      "Epoch 5: Loss=0.6916 | Acc=0.5048 | F1=0.0298\n",
      "Epoch 6: Loss=0.6823 | Acc=0.5056 | F1=0.0419\n",
      "Epoch 7: Loss=0.6821 | Acc=0.5052 | F1=0.6656\n",
      "Epoch 8: Loss=0.6793 | Acc=0.5064 | F1=0.0329\n",
      "Epoch 9: Loss=0.6818 | Acc=0.5024 | F1=0.6661\n",
      "Epoch 10: Loss=0.6796 | Acc=0.5060 | F1=0.0522\n",
      "Epoch 11: Loss=0.6782 | Acc=0.5064 | F1=0.0464\n",
      "Epoch 12: Loss=0.6723 | Acc=0.5072 | F1=0.0464\n",
      "Epoch 13: Loss=0.6748 | Acc=0.5068 | F1=0.0360\n",
      "Epoch 14: Loss=0.6718 | Acc=0.5036 | F1=0.6661\n",
      "Epoch 15: Loss=0.6736 | Acc=0.5044 | F1=0.6649\n",
      "Epoch 16: Loss=0.6827 | Acc=0.5044 | F1=0.0535\n",
      "Epoch 17: Loss=0.6873 | Acc=0.5060 | F1=0.0463\n",
      "Epoch 18: Loss=0.6816 | Acc=0.5036 | F1=0.6663\n",
      "Epoch 19: Loss=0.6744 | Acc=0.5040 | F1=0.6659\n",
      "Epoch 20: Loss=0.6708 | Acc=0.5036 | F1=0.6656\n",
      "Epoch 21: Loss=0.6815 | Acc=0.5056 | F1=0.0463\n",
      "Epoch 22: Loss=0.6771 | Acc=0.5056 | F1=0.0507\n",
      "Epoch 23: Loss=0.6715 | Acc=0.5024 | F1=0.6656\n",
      "Epoch 24: Loss=0.6774 | Acc=0.5040 | F1=0.6647\n",
      "Epoch 25: Loss=0.6765 | Acc=0.5032 | F1=0.0312\n",
      "Epoch 26: Loss=0.6835 | Acc=0.5044 | F1=0.0328\n",
      "Epoch 27: Loss=0.6728 | Acc=0.5040 | F1=0.0447\n",
      "Epoch 28: Loss=0.6866 | Acc=0.5004 | F1=0.6645\n",
      "Epoch 29: Loss=0.6753 | Acc=0.5032 | F1=0.0387\n",
      "Epoch 30: Loss=0.6737 | Acc=0.5028 | F1=0.0387\n",
      "Epoch 31: Loss=0.6748 | Acc=0.5056 | F1=0.6643\n",
      "Epoch 32: Loss=0.6983 | Acc=0.5052 | F1=0.0359\n",
      "Epoch 33: Loss=0.6784 | Acc=0.5044 | F1=0.0343\n",
      "Epoch 34: Loss=0.6795 | Acc=0.5056 | F1=0.0344\n",
      "Epoch 35: Loss=0.6810 | Acc=0.5060 | F1=0.0536\n",
      "Epoch 36: Loss=0.6730 | Acc=0.5052 | F1=0.6652\n",
      "Epoch 37: Loss=0.6468 | Acc=0.7192 | F1=0.7547\n",
      "Epoch 38: Loss=0.4179 | Acc=0.7716 | F1=0.7728\n",
      "Epoch 39: Loss=0.2420 | Acc=0.7700 | F1=0.7857\n",
      "Epoch 40: Loss=0.1394 | Acc=0.7740 | F1=0.7755\n",
      "Epoch 41: Loss=0.0834 | Acc=0.7732 | F1=0.7666\n",
      "Epoch 42: Loss=0.0469 | Acc=0.7776 | F1=0.7771\n",
      "Epoch 43: Loss=0.0305 | Acc=0.7712 | F1=0.7621\n",
      "Epoch 44: Loss=0.0171 | Acc=0.7732 | F1=0.7660\n",
      "Epoch 45: Loss=0.0135 | Acc=0.7704 | F1=0.7582\n",
      "Epoch 46: Loss=0.0141 | Acc=0.7800 | F1=0.7733\n",
      "Epoch 47: Loss=0.0106 | Acc=0.7808 | F1=0.7767\n",
      "Epoch 48: Loss=0.0110 | Acc=0.7780 | F1=0.7685\n",
      "Epoch 49: Loss=0.0131 | Acc=0.7768 | F1=0.7663\n",
      "Epoch 50: Loss=0.0135 | Acc=0.7724 | F1=0.7584\n",
      "Epoch 51: Loss=0.0084 | Acc=0.7716 | F1=0.7573\n",
      "Epoch 52: Loss=0.0120 | Acc=0.7720 | F1=0.7549\n",
      "Epoch 53: Loss=0.0103 | Acc=0.7712 | F1=0.7549\n",
      "Epoch 54: Loss=0.0114 | Acc=0.7716 | F1=0.7527\n",
      "Epoch 55: Loss=0.0100 | Acc=0.7724 | F1=0.7567\n",
      "Epoch 56: Loss=0.0123 | Acc=0.7828 | F1=0.7794\n",
      "Epoch 57: Loss=0.0099 | Acc=0.7840 | F1=0.7756\n",
      "Epoch 58: Loss=0.0181 | Acc=0.7816 | F1=0.7900\n",
      "Epoch 59: Loss=0.0137 | Acc=0.7768 | F1=0.7648\n",
      "Epoch 60: Loss=0.0258 | Acc=0.7820 | F1=0.7812\n",
      "Epoch 61: Loss=0.0263 | Acc=0.7796 | F1=0.7848\n",
      "Epoch 62: Loss=0.0172 | Acc=0.7848 | F1=0.7793\n",
      "Epoch 63: Loss=0.0056 | Acc=0.7796 | F1=0.7781\n",
      "Epoch 64: Loss=0.0030 | Acc=0.7788 | F1=0.7773\n",
      "Epoch 65: Loss=0.0021 | Acc=0.7784 | F1=0.7822\n",
      "Epoch 66: Loss=0.0004 | Acc=0.7796 | F1=0.7821\n",
      "Epoch 67: Loss=0.0010 | Acc=0.7792 | F1=0.7830\n",
      "Epoch 68: Loss=0.0009 | Acc=0.7812 | F1=0.7851\n",
      "Epoch 69: Loss=0.0003 | Acc=0.7800 | F1=0.7831\n",
      "Epoch 70: Loss=0.0004 | Acc=0.7816 | F1=0.7855\n",
      "Epoch 71: Loss=0.0002 | Acc=0.7808 | F1=0.7824\n",
      "Epoch 72: Loss=0.0001 | Acc=0.7808 | F1=0.7815\n",
      "Epoch 73: Loss=0.0001 | Acc=0.7792 | F1=0.7796\n",
      "Epoch 74: Loss=0.0001 | Acc=0.7788 | F1=0.7796\n",
      "Epoch 75: Loss=0.0000 | Acc=0.7784 | F1=0.7789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LSTM_GLOVE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, embeddings, freeze_embed=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.embedding.weight.data.copy_(embeddings)\n",
    "        self.embedding.weight.requires_grad = not freeze_embed  # freeze or fine-tune\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "# def train_epoch(model, dataloader, optimizer, criterion):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for texts, labels in dataloader:\n",
    "#         texts, labels = texts.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(texts)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# def evaluate(model, dataloader):\n",
    "#     model.eval()\n",
    "#     preds, labels_all = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for texts, labels in dataloader:\n",
    "#             texts, labels = texts.to(device), labels.to(device)\n",
    "#             outputs = model(texts)\n",
    "#             predictions = torch.argmax(outputs, dim=1)\n",
    "#             preds.extend(predictions.cpu().tolist())\n",
    "#             labels_all.extend(labels.cpu().tolist())\n",
    "#     acc = accuracy_score(labels_all, preds)\n",
    "#     f1 = f1_score(labels_all, preds)\n",
    "#     return acc, f1\n",
    "\n",
    "\n",
    "model_frozen = LSTMSentiment(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,\n",
    "    embeddings=embedding_matrix,\n",
    "    freeze_embed=True\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_frozen.parameters(), lr=5e-3)\n",
    "\n",
    "print(\"==== Training with Frozen GloVe embeddings ====\")\n",
    "for epoch in range(50):\n",
    "    loss = train_epoch(model_frozen, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model_frozen, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss:.4f} | Acc={acc:.4f} | F1={f1:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. Phase 2: Fine-tune embeddings\n",
    "# =====================================================\n",
    "print(\"\\n==== Fine-tuning GloVe embeddings ====\")\n",
    "model_finetuned = LSTMSentiment(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,\n",
    "    embeddings=embedding_matrix,\n",
    "    freeze_embed=False\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_finetuned.parameters(), lr=5e-3)\n",
    "\n",
    "for epoch in range(75):\n",
    "    loss = train_epoch(model_finetuned, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model_finetuned, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss:.4f} | Acc={acc:.4f} | F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100d765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "sentiment\n",
      "1    1250\n",
      "0    1250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.unique())\n",
    "print(train_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610c0edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV rate: 12.67%\n"
     ]
    }
   ],
   "source": [
    "unk_count = sum(1 for w in vocab if w not in glove.stoi)\n",
    "print(f\"OOV rate: {unk_count/len(vocab):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62fe8ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.2704e-01,  4.8618e-01],\n",
      "        [-6.9359e-03, -1.8749e-05],\n",
      "        [-6.9359e-03, -1.8749e-05],\n",
      "        [-6.9359e-03, -1.8749e-05],\n",
      "        [-6.9359e-03, -1.8749e-05]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "texts, labels = next(iter(train_loader))\n",
    "outputs = model_finetuned(texts.to(device))\n",
    "print(outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc=0.7882 | F1=0.7871\n"
     ]
    }
   ],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(predictions.cpu().tolist())\n",
    "            labels_all.extend(labels.cpu().tolist())\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    f1 = f1_score(labels_all, preds)\n",
    "    return acc, f1\n",
    "\n",
    "test_texts_2 = test_df[\"review\"]\n",
    "test_labels_2 = test_df[\"sentiment\"]\n",
    "test_data_2 = IMDBDataset(test_texts_2, test_labels_2)\n",
    "test_loader_2 = DataLoader(test_data_2, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "acc, f1 = test(model_finetuned, test_loader_2)\n",
    "\n",
    "print(f\" Acc={acc:.4f} | F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, texts_list=None, label_names=None):\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    wrong_predictions = []  # store (text, true, pred)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (texts, labels) in enumerate(dataloader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            preds.extend(predictions.cpu().tolist())\n",
    "            labels_all.extend(labels.cpu().tolist())\n",
    "\n",
    "            # if you passed the original text list, we can recover them\n",
    "            if texts_list is not None:\n",
    "                batch_start = batch_idx * dataloader.batch_size\n",
    "                batch_end = batch_start + len(labels)\n",
    "                for i in range(len(labels)):\n",
    "                    if predictions[i] != labels[i]:\n",
    "                        wrong_predictions.append({\n",
    "                            \"text\": texts_list[batch_start + i],\n",
    "                            \"true_label\": (label_names[labels[i].item()] if label_names else labels[i].item()),\n",
    "                            \"pred_label\": (label_names[predictions[i].item()] if label_names else predictions[i].item())\n",
    "                        })\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    f1 = f1_score(labels_all, preds)\n",
    "\n",
    "    print(f\"Acc={acc:.4f} | F1={f1:.4f}\")\n",
    "    print(f\"Total wrong predictions: {len(wrong_predictions)}\")\n",
    "\n",
    "    # Print examples\n",
    "    for w in wrong_predictions[:5]:  # show first 5\n",
    "        print(\"â”€\" * 60)\n",
    "        print(f\"True: {w['true_label']} | Pred: {w['pred_label']}\")\n",
    "        print(f\"Text: {w['text']}\")  # limit text length for readability\n",
    "\n",
    "    return acc, f1, wrong_predictions\n",
    "\n",
    "test_texts_2 = test_df[\"review\"]\n",
    "test_labels_2 = test_df[\"sentiment\"]\n",
    "test_data_2 = IMDBDataset(test_texts_2, test_labels_2)\n",
    "test_loader_2 = DataLoader(test_data_2, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# acc, f1 = test(model_finetuned, test_loader_2)\n",
    "label_names = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "acc, f1, wrong_preds = test(\n",
    "    model_finetuned,\n",
    "    test_loader_2,\n",
    "    texts_list=test_df[\"review\"].tolist(),\n",
    "    label_names=label_names\n",
    ")\n",
    "print(f\" Acc={acc:.4f} | F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2756e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a97926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simpler LSTM with dynamic skip connection on top of nn.LSTM output.\n",
    "    Skips are computed at each timestep based on input + LSTM hidden output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, gate_type=\"scalar\", batch_first=True):\n",
    "        super().__init__()\n",
    "        assert gate_type in (\"scalar\", \"vector\"), \"gate_type must be 'scalar' or 'vector'\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gate_type = gate_type\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=batch_first)\n",
    "\n",
    "        # Gate takes concatenation of input and LSTM output at each step\n",
    "        gate_out_dim = 1 if gate_type == \"scalar\" else hidden_size\n",
    "        self.gate = nn.Linear(input_size + hidden_size, gate_out_dim)\n",
    "\n",
    "        # Optional: initialize gate bias so model starts with more updates than skips\n",
    "        nn.init.constant_(self.gate.bias, -2.0)\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, input_size)\n",
    "            hx: optional (h0, c0)\n",
    "        Returns:\n",
    "            outputs: (batch, seq_len, hidden_size)\n",
    "            (h_n, c_n): final states\n",
    "        \"\"\"\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, hx)  # (B, T, H)\n",
    "\n",
    "        # Compute skip gate for each timestep\n",
    "        # Concatenate input and LSTM output â†’ (B, T, input+hidden)\n",
    "        gate_in = torch.cat([x, lstm_out], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(gate_in))  # (B, T, 1 or H)\n",
    "\n",
    "        if self.gate_type == \"scalar\":\n",
    "            g = g.expand_as(lstm_out)\n",
    "\n",
    "        # Apply dynamic skip connection:\n",
    "        # Smoothly mix previous timestep output and current LSTM output\n",
    "        # Shift lstm_out by one timestep for \"previous\" context\n",
    "        prev_out = torch.zeros_like(lstm_out)\n",
    "        prev_out[:, 1:, :] = lstm_out[:, :-1, :]\n",
    "\n",
    "        outputs = g * prev_out + (1 - g) * lstm_out\n",
    "        return outputs, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4205e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, pad_idx, embeddings=None, freeze_embed=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        if embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(embeddings)\n",
    "            self.embedding.weight.requires_grad = not freeze_embed  # freeze or fine-tune\n",
    "        self.encoder = SkipLSTM(embed_dim, hidden_size, gate_type=\"scalar\", batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)                # (B, T, E)\n",
    "        outputs, _ = self.encoder(embedded)         # (B, T, H)\n",
    "        last_hidden = outputs[:, -1, :]             # take last timestep\n",
    "        logits = self.fc(last_hidden)               # (B, num_classes)\n",
    "        return logits\n",
    "    \n",
    "# hidden_size = 64\n",
    "# input_size= len(vocab)\n",
    "# model = LSTMClassifier(input_size, hidden_size, 2).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9fcfcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.6946, acc=0.5000, f1=0.3598\n",
      "New best accuracy: 0.5000 at epoch 1, saving model.\n",
      "Epoch 2: loss=0.6939, acc=0.5000, f1=0.3333\n",
      "Epoch 3: loss=0.6930, acc=0.4980, f1=0.3525\n",
      "Epoch 4: loss=0.6930, acc=0.5020, f1=0.3480\n",
      "New best accuracy: 0.5020 at epoch 4, saving model.\n",
      "Epoch 5: loss=0.6917, acc=0.5060, f1=0.3499\n",
      "New best accuracy: 0.5060 at epoch 5, saving model.\n",
      "Epoch 6: loss=0.6903, acc=0.5060, f1=0.3565\n",
      "Epoch 7: loss=0.6861, acc=0.5000, f1=0.3437\n",
      "Epoch 8: loss=0.6807, acc=0.5020, f1=0.3513\n",
      "Epoch 9: loss=0.6841, acc=0.5060, f1=0.3597\n",
      "Epoch 10: loss=0.6880, acc=0.5040, f1=0.3649\n",
      "Epoch 11: loss=0.6865, acc=0.5100, f1=0.3649\n",
      "New best accuracy: 0.5100 at epoch 11, saving model.\n",
      "Epoch 12: loss=0.6828, acc=0.5060, f1=0.3565\n",
      "Epoch 13: loss=0.6806, acc=0.5040, f1=0.3679\n",
      "Epoch 14: loss=0.7101, acc=0.4980, f1=0.3428\n",
      "Epoch 15: loss=0.6858, acc=0.5000, f1=0.3437\n",
      "Epoch 16: loss=0.6835, acc=0.5040, f1=0.3490\n",
      "Epoch 17: loss=0.6749, acc=0.5040, f1=0.3523\n",
      "Epoch 18: loss=0.6866, acc=0.5020, f1=0.3513\n",
      "Epoch 19: loss=0.6869, acc=0.5020, f1=0.3378\n",
      "Epoch 20: loss=0.6804, acc=0.5000, f1=0.3535\n",
      "Epoch 21: loss=0.6777, acc=0.5000, f1=0.3598\n",
      "Epoch 22: loss=0.6794, acc=0.5060, f1=0.3499\n",
      "Epoch 23: loss=0.6751, acc=0.5000, f1=0.3503\n",
      "Epoch 24: loss=0.6742, acc=0.4980, f1=0.3525\n",
      "Epoch 25: loss=0.6734, acc=0.5040, f1=0.3523\n",
      "Epoch 26: loss=0.6889, acc=0.4960, f1=0.3385\n",
      "Epoch 27: loss=0.6762, acc=0.5020, f1=0.3412\n",
      "Epoch 28: loss=0.6777, acc=0.5000, f1=0.3567\n",
      "Epoch 29: loss=0.6745, acc=0.5040, f1=0.3523\n",
      "Epoch 30: loss=0.6773, acc=0.5040, f1=0.3555\n",
      "Epoch 31: loss=0.6735, acc=0.5000, f1=0.3567\n",
      "Epoch 32: loss=0.6729, acc=0.5060, f1=0.3532\n",
      "Epoch 33: loss=0.6727, acc=0.5020, f1=0.3513\n",
      "Epoch 34: loss=0.6755, acc=0.5040, f1=0.3555\n",
      "Epoch 35: loss=0.6724, acc=0.5040, f1=0.3490\n",
      "Epoch 36: loss=0.6719, acc=0.5040, f1=0.3555\n",
      "Epoch 37: loss=0.6845, acc=0.5040, f1=0.3490\n",
      "Epoch 38: loss=0.6749, acc=0.5060, f1=0.3532\n",
      "Epoch 39: loss=0.6754, acc=0.4980, f1=0.3394\n",
      "Epoch 40: loss=0.6752, acc=0.5020, f1=0.3577\n",
      "Epoch 41: loss=0.6714, acc=0.5040, f1=0.3422\n",
      "Epoch 42: loss=0.6747, acc=0.5000, f1=0.3567\n",
      "Epoch 43: loss=0.6793, acc=0.5040, f1=0.3523\n",
      "Epoch 44: loss=0.6718, acc=0.5060, f1=0.3532\n",
      "Epoch 45: loss=0.6724, acc=0.5080, f1=0.3607\n",
      "Epoch 46: loss=0.6722, acc=0.5080, f1=0.3542\n",
      "Epoch 47: loss=0.6720, acc=0.5080, f1=0.3607\n",
      "Epoch 48: loss=0.6745, acc=0.5060, f1=0.3499\n",
      "Epoch 49: loss=0.6724, acc=0.5080, f1=0.3542\n",
      "Epoch 50: loss=0.6721, acc=0.5040, f1=0.3587\n",
      "Epoch 51: loss=0.6712, acc=0.5040, f1=0.3456\n",
      "Epoch 52: loss=0.6714, acc=0.5040, f1=0.3587\n",
      "Epoch 53: loss=0.6712, acc=0.5040, f1=0.3456\n",
      "Epoch 54: loss=0.6745, acc=0.5040, f1=0.3456\n",
      "Epoch 55: loss=0.6699, acc=0.5060, f1=0.3532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m best_model_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m200\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     acc, f1 = evaluate(model, test_loader)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, f1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m     10\u001b[39m outputs = model(texts)\n\u001b[32m     11\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m optimizer.step()\n\u001b[32m     15\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for texts, labels in dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(predictions.cpu().tolist())\n",
    "            labels_all.extend(labels.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds)\n",
    "    f1 = f1_score(labels_all, preds, average=\"macro\")\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Training setup\n",
    "# =====================================================\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "hidden_size = 192\n",
    "num_classes = 2  # binary sentiment classification\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embed_dim, hidden_size, num_classes, pad_idx,embeddings=embedding_matrix).to(device)\n",
    "# model = LSTMClassifier(vocab_size, embed_dim, hidden_size, num_classes, pad_idx,embeddings=None).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =====================================================\n",
    "# 4. Training loop\n",
    "# =====================================================\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "for epoch in range(200):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    acc, f1 = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, acc={acc:.4f}, f1={f1:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"New best accuracy: {best_acc:.4f} at epoch {epoch+1}, saving model.\")\n",
    "# save_path = \"./No_glove_1250.pth\"\n",
    "save_path = \"./glove_1250_skipLSTM.pth\"\n",
    "torch.save(best_model_state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_state\n",
    "# pre trained glove embed 0.6720 1250 samples 10e-3\n",
    "# pre trained glove embed 0.6336 1250 samples 10e-3\n",
    "# no pretrain embed 0.6112 1250 samples 10e-3\n",
    "# \n",
    "\n",
    "# pre trained glove embed 0.7120 0.7260 500 samples 10e-3 128 hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f8f2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./No_glove_1250.pth\"\n",
    "torch.save(best_model_state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2eb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bc2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "MAX_LEN = 300\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # truncate or pad each text\n",
    "    texts_truncated = [t[:MAX_LEN] for t in texts]\n",
    "    texts_padded = pad_sequence(texts_truncated, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    # If some sequences are shorter than MAX_LEN, pad to MAX_LEN\n",
    "    if texts_padded.shape[1] < MAX_LEN:\n",
    "        pad_size = MAX_LEN - texts_padded.shape[1]\n",
    "        padding = torch.full((texts_padded.shape[0], pad_size), vocab[\"<pad>\"], dtype=torch.long)\n",
    "        texts_padded = torch.cat([texts_padded, padding], dim=1)\n",
    "    return texts_padded, torch.tensor(labels)\n",
    "\n",
    "train_loader_2 = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader_2 = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86fc87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\SC4001\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=528.6711, acc=0.5027\n",
      "           val_loss=189.2079, val_acc=0.4907\n",
      "Epoch 2: loss=114.3804, acc=0.4927\n",
      "           val_loss=82.1652, val_acc=0.5090\n",
      "Epoch 3: loss=64.2405, acc=0.5336\n",
      "           val_loss=52.8107, val_acc=0.5450\n",
      "Epoch 4: loss=44.8637, acc=0.5622\n",
      "           val_loss=39.0375, val_acc=0.5716\n",
      "Epoch 5: loss=34.5627, acc=0.5890\n",
      "           val_loss=31.0410, val_acc=0.5980\n",
      "Epoch 6: loss=28.1773, acc=0.6105\n",
      "           val_loss=25.8254, val_acc=0.6172\n",
      "Epoch 7: loss=23.8361, acc=0.6262\n",
      "           val_loss=22.1560, val_acc=0.6318\n",
      "Epoch 8: loss=20.6980, acc=0.6382\n",
      "           val_loss=19.4398, val_acc=0.6419\n",
      "Epoch 9: loss=18.3261, acc=0.6480\n",
      "           val_loss=17.3502, val_acc=0.6522\n",
      "Epoch 10: loss=16.4703, acc=0.6575\n",
      "           val_loss=15.6876, val_acc=0.6600\n",
      "Epoch 11: loss=14.9737, acc=0.6637\n",
      "           val_loss=14.3353, val_acc=0.6663\n",
      "Epoch 12: loss=13.7463, acc=0.6698\n",
      "           val_loss=13.2147, val_acc=0.6717\n",
      "Epoch 13: loss=12.7203, acc=0.6748\n",
      "           val_loss=12.2716, val_acc=0.6767\n",
      "Epoch 14: loss=11.8513, acc=0.6796\n",
      "           val_loss=11.4733, val_acc=0.6813\n",
      "Epoch 15: loss=11.1156, acc=0.6838\n",
      "           val_loss=10.7893, val_acc=0.6853\n",
      "Epoch 16: loss=10.4776, acc=0.6883\n",
      "           val_loss=10.1921, val_acc=0.6892\n",
      "Epoch 17: loss=9.9183, acc=0.6914\n",
      "           val_loss=9.6658, val_acc=0.6926\n",
      "Epoch 18: loss=9.4232, acc=0.6949\n",
      "           val_loss=9.2022, val_acc=0.6957\n",
      "Epoch 19: loss=8.9888, acc=0.6975\n",
      "           val_loss=8.7895, val_acc=0.6980\n",
      "Epoch 20: loss=8.5981, acc=0.6994\n",
      "           val_loss=8.4221, val_acc=0.7000\n",
      "Epoch 21: loss=8.2505, acc=0.7014\n",
      "           val_loss=8.0894, val_acc=0.7022\n",
      "Epoch 22: loss=7.9329, acc=0.7035\n",
      "           val_loss=7.7871, val_acc=0.7041\n",
      "Epoch 23: loss=7.6432, acc=0.7055\n",
      "           val_loss=7.5112, val_acc=0.7059\n",
      "Epoch 24: loss=7.3810, acc=0.7066\n",
      "           val_loss=7.2641, val_acc=0.7065\n",
      "Epoch 25: loss=7.1492, acc=0.7072\n",
      "           val_loss=7.0488, val_acc=0.7071\n",
      "Epoch 26: loss=6.9463, acc=0.7079\n",
      "           val_loss=6.8535, val_acc=0.7078\n",
      "Epoch 27: loss=6.7582, acc=0.7085\n",
      "           val_loss=6.6684, val_acc=0.7088\n",
      "Epoch 28: loss=6.5782, acc=0.7098\n",
      "           val_loss=6.4921, val_acc=0.7102\n",
      "Epoch 29: loss=6.4068, acc=0.7110\n",
      "           val_loss=6.3259, val_acc=0.7117\n",
      "Epoch 30: loss=6.2458, acc=0.7127\n",
      "           val_loss=6.1700, val_acc=0.7132\n",
      "Epoch 31: loss=6.0947, acc=0.7141\n",
      "           val_loss=6.0290, val_acc=0.7142\n",
      "Epoch 32: loss=5.9607, acc=0.7147\n",
      "           val_loss=5.9009, val_acc=0.7151\n",
      "Epoch 33: loss=5.8372, acc=0.7159\n",
      "           val_loss=5.7829, val_acc=0.7162\n",
      "Epoch 34: loss=5.7255, acc=0.7170\n",
      "           val_loss=5.6735, val_acc=0.7168\n",
      "Epoch 35: loss=5.6182, acc=0.7173\n",
      "           val_loss=5.5690, val_acc=0.7171\n",
      "Epoch 36: loss=5.5162, acc=0.7177\n",
      "           val_loss=5.4718, val_acc=0.7177\n",
      "Epoch 37: loss=5.4247, acc=0.7185\n",
      "           val_loss=5.3841, val_acc=0.7186\n",
      "Epoch 38: loss=5.3395, acc=0.7194\n",
      "           val_loss=5.3029, val_acc=0.7195\n",
      "Epoch 39: loss=5.2621, acc=0.7201\n",
      "           val_loss=5.2298, val_acc=0.7201\n",
      "Epoch 40: loss=5.1948, acc=0.7207\n",
      "           val_loss=5.1624, val_acc=0.7207\n",
      "Epoch 41: loss=5.1282, acc=0.7212\n",
      "           val_loss=5.0963, val_acc=0.7211\n",
      "Epoch 42: loss=5.0620, acc=0.7214\n",
      "           val_loss=5.0276, val_acc=0.7213\n",
      "Epoch 43: loss=4.9894, acc=0.7216\n",
      "           val_loss=4.9537, val_acc=0.7214\n",
      "Epoch 44: loss=4.9158, acc=0.7217\n",
      "           val_loss=4.8835, val_acc=0.7216\n",
      "Epoch 45: loss=4.8496, acc=0.7218\n",
      "           val_loss=4.8208, val_acc=0.7218\n",
      "Epoch 46: loss=4.7900, acc=0.7222\n",
      "           val_loss=4.7631, val_acc=0.7221\n",
      "Epoch 47: loss=4.7344, acc=0.7225\n",
      "           val_loss=4.7079, val_acc=0.7225\n",
      "Epoch 48: loss=4.6796, acc=0.7227\n",
      "           val_loss=4.6524, val_acc=0.7226\n",
      "Epoch 49: loss=4.6235, acc=0.7231\n",
      "           val_loss=4.6003, val_acc=0.7231\n",
      "Epoch 50: loss=4.5750, acc=0.7234\n",
      "           val_loss=4.5541, val_acc=0.7234\n",
      "Epoch 51: loss=4.5315, acc=0.7237\n",
      "           val_loss=4.5129, val_acc=0.7235\n",
      "Epoch 52: loss=4.4927, acc=0.7237\n",
      "           val_loss=4.4720, val_acc=0.7235\n",
      "Epoch 53: loss=4.4496, acc=0.7236\n",
      "           val_loss=4.4332, val_acc=0.7235\n",
      "Epoch 54: loss=4.4141, acc=0.7237\n",
      "           val_loss=4.4029, val_acc=0.7235\n",
      "Epoch 55: loss=4.3898, acc=0.7237\n",
      "           val_loss=4.3809, val_acc=0.7236\n",
      "Epoch 56: loss=4.3717, acc=0.7238\n",
      "           val_loss=4.3636, val_acc=0.7237\n",
      "Epoch 57: loss=4.3523, acc=0.7239\n",
      "           val_loss=4.3394, val_acc=0.7237\n",
      "Epoch 58: loss=4.3241, acc=0.7239\n",
      "           val_loss=4.3119, val_acc=0.7238\n",
      "Epoch 59: loss=4.2982, acc=0.7239\n",
      "           val_loss=4.2852, val_acc=0.7236\n",
      "Epoch 60: loss=4.2704, acc=0.7237\n",
      "           val_loss=4.2581, val_acc=0.7236\n",
      "Epoch 61: loss=4.2440, acc=0.7237\n",
      "           val_loss=4.2339, val_acc=0.7235\n",
      "Epoch 62: loss=4.2237, acc=0.7237\n",
      "           val_loss=4.2112, val_acc=0.7235\n",
      "Epoch 63: loss=4.1981, acc=0.7237\n",
      "           val_loss=4.1878, val_acc=0.7236\n",
      "Epoch 64: loss=4.1761, acc=0.7238\n",
      "           val_loss=4.1653, val_acc=0.7237\n",
      "Epoch 65: loss=4.1523, acc=0.7239\n",
      "           val_loss=4.1390, val_acc=0.7239\n",
      "Epoch 66: loss=4.1250, acc=0.7241\n",
      "           val_loss=4.1129, val_acc=0.7240\n",
      "Epoch 67: loss=4.0997, acc=0.7242\n",
      "           val_loss=4.0887, val_acc=0.7241\n",
      "Epoch 68: loss=4.0761, acc=0.7243\n",
      "           val_loss=4.0679, val_acc=0.7241\n",
      "Epoch 69: loss=4.0579, acc=0.7242\n",
      "           val_loss=4.0504, val_acc=0.7241\n",
      "Epoch 70: loss=4.0412, acc=0.7243\n",
      "           val_loss=4.0368, val_acc=0.7241\n",
      "Epoch 71: loss=4.0303, acc=0.7242\n",
      "           val_loss=4.0227, val_acc=0.7241\n",
      "Epoch 72: loss=4.0135, acc=0.7242\n",
      "           val_loss=4.0057, val_acc=0.7240\n",
      "Epoch 73: loss=3.9964, acc=0.7241\n",
      "           val_loss=3.9899, val_acc=0.7240\n",
      "Epoch 74: loss=3.9812, acc=0.7242\n",
      "           val_loss=3.9759, val_acc=0.7241\n",
      "Epoch 75: loss=3.9675, acc=0.7244\n",
      "           val_loss=3.9622, val_acc=0.7244\n",
      "Epoch 76: loss=3.9539, acc=0.7247\n",
      "           val_loss=3.9492, val_acc=0.7247\n",
      "Epoch 77: loss=3.9414, acc=0.7249\n",
      "           val_loss=3.9370, val_acc=0.7249\n",
      "Epoch 78: loss=3.9299, acc=0.7252\n",
      "           val_loss=3.9260, val_acc=0.7251\n",
      "Epoch 79: loss=3.9195, acc=0.7254\n",
      "           val_loss=3.9172, val_acc=0.7253\n",
      "Epoch 80: loss=3.9127, acc=0.7255\n",
      "           val_loss=3.9111, val_acc=0.7255\n",
      "Epoch 81: loss=3.9071, acc=0.7256\n",
      "           val_loss=3.9066, val_acc=0.7255\n",
      "Epoch 82: loss=3.9033, acc=0.7257\n",
      "           val_loss=3.9013, val_acc=0.7256\n",
      "Epoch 83: loss=3.8968, acc=0.7259\n",
      "           val_loss=3.8902, val_acc=0.7258\n",
      "Epoch 84: loss=3.8818, acc=0.7260\n",
      "           val_loss=3.8753, val_acc=0.7260\n",
      "Epoch 85: loss=3.8663, acc=0.7262\n",
      "           val_loss=3.8596, val_acc=0.7261\n",
      "Epoch 86: loss=3.8514, acc=0.7263\n",
      "           val_loss=3.8455, val_acc=0.7262\n",
      "Epoch 87: loss=3.8374, acc=0.7263\n",
      "           val_loss=3.8330, val_acc=0.7263\n",
      "Epoch 88: loss=3.8267, acc=0.7265\n",
      "           val_loss=3.8223, val_acc=0.7264\n",
      "Epoch 89: loss=3.8169, acc=0.7266\n",
      "           val_loss=3.8134, val_acc=0.7266\n",
      "Epoch 90: loss=3.8087, acc=0.7268\n",
      "           val_loss=3.8072, val_acc=0.7268\n",
      "Epoch 91: loss=3.8035, acc=0.7270\n",
      "           val_loss=3.8012, val_acc=0.7269\n",
      "Epoch 92: loss=3.7973, acc=0.7271\n",
      "           val_loss=3.7958, val_acc=0.7270\n",
      "Epoch 93: loss=3.7928, acc=0.7271\n",
      "           val_loss=3.7902, val_acc=0.7270\n",
      "Epoch 94: loss=3.7862, acc=0.7272\n",
      "           val_loss=3.7837, val_acc=0.7271\n",
      "Epoch 95: loss=3.7800, acc=0.7272\n",
      "           val_loss=3.7790, val_acc=0.7271\n",
      "Epoch 96: loss=3.7767, acc=0.7273\n",
      "           val_loss=3.7768, val_acc=0.7272\n",
      "Epoch 97: loss=3.7758, acc=0.7274\n",
      "           val_loss=3.7757, val_acc=0.7274\n",
      "Epoch 98: loss=3.7753, acc=0.7275\n",
      "           val_loss=3.7748, val_acc=0.7276\n",
      "Epoch 99: loss=3.7739, acc=0.7278\n",
      "           val_loss=3.7731, val_acc=0.7278\n",
      "Epoch 100: loss=3.7720, acc=0.7280\n",
      "           val_loss=3.7711, val_acc=0.7280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "seq_len = 34\n",
    "embed_dim = 468 #300\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "train_loss_arr, test_loss_arr, train_acc_arr, test_acc_arr = [], [] , [], []\n",
    "model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, 1)),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Conv1D(filters=256, kernel_size=5, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# model = Sequential([\n",
    "#     # Input shape for Conv1D is (timesteps, features) = (seq_len, embed_dim)\n",
    "#     Conv1D(filters=50, kernel_size=3, strides=1, padding='same',\n",
    "#            activation='relu', input_shape=(983,)),#(seq_len, embed_dim)),\n",
    "#     Conv1D(filters=75, kernel_size=3, strides=1, padding='same',\n",
    "#            activation='relu'),\n",
    "#     MaxPool1D(pool_size=2),                 # seq_len -> ceil(seq_len/2)\n",
    "#     Dropout(0.25),\n",
    "\n",
    "#     Conv1D(filters=125, kernel_size=3, strides=1, padding='same',\n",
    "#            activation='relu'),\n",
    "#     MaxPool1D(pool_size=2),                 # -> ceil(previous/2)\n",
    "#     Dropout(0.25),\n",
    "\n",
    "#     Flatten(),                              # â†’ 1D vector\n",
    "#     Dense(500, activation='relu'),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(250, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='sparse_categorical_crossentropy',  # labels from loader are integer class IDs\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "# --- manual training loop using DataLoader batches ---\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    batches = 0\n",
    "    for xb, yb in train_loader_2:\n",
    "        # convert torch tensors to numpy for Keras\n",
    "        xb_np = xb.numpy()\n",
    "        yb_np = yb.numpy()\n",
    "\n",
    "        metrics = model.train_on_batch(xb_np, yb_np)\n",
    "        train_loss += metrics[0]\n",
    "        train_acc += metrics[1]\n",
    "        batches += 1\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss/batches:.4f}, acc={train_acc/batches:.4f}\")\n",
    "    train_acc_arr.append(train_acc)\n",
    "    train_loss_arr.append(train_loss/batches)\n",
    "    # simple validation pass\n",
    "    val_loss, val_acc, val_batches = 0.0, 0.0, 0\n",
    "    for xb, yb in test_loader_2:\n",
    "        xb_np = xb.numpy()\n",
    "        yb_np = yb.numpy()\n",
    "        metrics = model.test_on_batch(xb_np, yb_np)\n",
    "        val_loss += metrics[0]\n",
    "        val_acc += metrics[1]\n",
    "        val_batches += 1\n",
    "    print(f\"           val_loss={val_loss/val_batches:.4f}, val_acc={val_acc/val_batches:.4f}\")\n",
    "    test_acc_arr.append(train_acc)\n",
    "    test_loss_arr.append(train_loss/batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch 100 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d484dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041969c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
